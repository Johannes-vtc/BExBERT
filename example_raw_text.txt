Transcatheter aortic valve replacement (TAVR) for patients with symptomatic severe aortic stenosis at intermediate or greater surgical risk using balloon-expandable and self-expanding devices has gained US approval and widespread use in practice. This is a result of a series of randomized trials comparing these TAVR platforms with medical therapy and conventional surgery.1-6 Despite advances, existing TAVR devices have limitations, including the inability to implant without being able to retrieve or reposition after fully expanded, hemodynamic compromise during implantation and the requirement for rapid pacing during deployment. Valve malposition requiring a second TAVR valve has been associated with an increased procedural risk.7,8 The need for permanent pacemaker implantation is another important limitation of TAVR.9-12 Paravalvular leak (PVL) can occur. Compared with patients without leaks, there is a higher mortality associated with moderate or severe PVL.13 Although iterative improvements in TAVR devices and periprocedural technique have occurred, PVL has been reduced but not eliminated.5,6,13. Patients were screened by the case review committee (eTable 2 in Supplement 1) and randomized 2:1 between September 22, 2014, and December 24, 2015, at 55 centers in North America, Europe, and Australia. Quiz Ref IDEligible symptomatic patients had severe native aortic stenosis with a valve area of 1.0 cm2 or less (or an aortic valve area index ≤0.6 cm2/m2) and a mean pressure gradient of at least 40 mm Hg or a jet velocity of at least 4.0 m/s. Patients were required to have a Society of Thoracic Surgeons (STS) predicted risk of mortality of at least 8% or another indicator of high or extreme risk. Agreement by the local heart team (including an interventional cardiologist and a cardiac surgeon) regarding risk and suitability for TAVR was required. An aortic annulus size of 20 mm or larger and 27 mm or smaller (based on computed tomography) and eligibility for an available size of both valves were required. All patients were reviewed by the REPRISE III Case Review Committee to confirm eligibility. Additional inclusion and exclusion criteria are provided in eTables 3 and 4 in Supplement 1. A computerized pseudorandom number generator randomized patients stratified by center and high or extreme risk status. A 2:1 ratio was chosen to capture more information on the investigational device. Random permuted blocks (size 3 or 6, each with 50% chance, in a ratio of 2:1) were used to ensure approximate balance of treatment allocation within each stratum. Packaging and design of the MEV and SEV are different; therefore, the investigators performing the procedure were not blinded to the assigned treatment. The MEV consists of 3 bovine pericardial tissue valve leaflets and a braided nitinol frame with a polycarbonate-based urethane adaptive seal.15,16 Investigators completed comprehensive training on the MEV. Centers without previous experience implanting MEVs performed at least 2 roll-in procedures and received on-site proctorship during their initial implant procedures. The device was introduced via the femoral artery using conventional techniques. Balloon aortic valvuloplasty was followed by advancement of the MEV through the aorta and aortic arch. The valve was mechanically expanded and its position and function assessed. Repositioning (or retrieval) of the MEV could be performed if needed. Once a satisfactory position was achieved, the valve was locked and released. Postdeployment aortography of the ascending aorta was required. Mechanically expanded valve sizes included 23-mm, 25-mm, and 27-mm diameters. The SEV consists of a self-expanding nitinol frame and trileaflet porcine pericardial valve.3 The valve was introduced via the femoral artery per the instructions for use. Balloon aortic valvuloplasty was required. All centers were approved and proctor free for SEV implants before enrolling patients. Self-expanding valve sizes included 26-mm, 29-mm, and 31-mm diameters. All data were analyzed by independent core laboratories (blinded to treatment allocation), and a clinical events committee adjudicated all major clinical events. An independent data and safety monitoring board provided study oversight (eTable 2 in Supplement 1). Clinical follow-up occurred at discharge or 7 days after the procedure (whichever came first), then at 30 days, 6 months, and 1 year. Annual follow-up will occur through 5 years. Patients were treated with aspirin and a thienopyridine before and for at least 1 month following TAVR. Patients receiving long-term anticoagulation treatment could be treated with a single antiplatelet drug. The primary safety end point was the composite of all-cause mortality, stroke, life-threatening and major bleeding events, stage 2/3 acute kidney injury, and major vascular complications at 30 days. The primary effectiveness end point was the 1-year composite rate of all-cause mortality, disabling stroke, and moderate or greater PVL based on core laboratory assessment. The secondary end point was moderate or greater PVL at 1 year. Additional end points were based on the Valve Academic Research Consortium (VARC) end points and definitions.17,18 Additional exploratory analyses related to device performance, procedural success, and functional, neurological, and health status are described and shown in eTables 5, 10, and 11 in Supplement 1. Clinical procedural and device success were assessed at 30 days (eTable 5 in Supplement 1). Additional valve parameters were measured by transthoracic echocardiogram and assessed by an independent core laboratory. Functional status was evaluated by gait speed19 and New York Heart Association classification. Neurological status was determined by the National Institutes of Health Stroke Scale and the Modified Rankin Scale. The analysis of permanent pacemaker implantation, quality-of-life, and resource utilization data will be reported separately. Details of the statistical analyses are provided in the eAppendix in Supplement 1. Briefly, testing of primary and secondary end points was performed hierarchically to control for type I error. First, the 30-day primary safety and 1-year primary effectiveness end points were tested. If the null hypothesis for both end points was rejected to show noninferiority of the MEV to the SEV, the secondary end point was tested for superiority. If the null hypothesis for the secondary end point was rejected to show superiority of the MEV to the SEV, then the primary effectiveness end point was tested for superiority of the MEV to the SEV. The prespecified population for the primary noninferiority analyses was the implanted analysis set (Figure 1), which included consenting, enrolled patients treated with the assigned valve. Farrington-Manning tests were used to test noninferiority of the MEV vs the SEV. For the primary safety end point, an assumed composite event rate of 40% in each group and an absolute noninferiority margin of 10.5% required 912 patients (2:1 MEV:SEV) to provide 85% power (assuming 5% attrition). For the primary effectiveness end point, an expected composite event rate of 32% in the MEV and SEV groups and an absolute noninferiority margin of 9.5% required 912 patients to provide 80% power to detect noninferiority (assuming 10% attrition). For both primary end points, noninferiority of the MEV vs the SEV was concluded if P < .025, corresponding to a 1-sided upper 97.5% confidence interval on the difference in observed rates between groups being less than the noninferiority margin. Expected rates were derived from the CoreValve High Risk and CoreValve US trials, as the current trial was designed to enroll patients similar to the ones enrolled in those trials.3,20 Noninferiority margins were based on available data at the time of protocol development and were considered clinically reasonable. For both primary composite end points, if any individual component was missing, then the composite end point was set as missing. Missing data sensitivity analyses using the tipping point approach were performed for the primary end points by imputing missing data in both treatment groups with all possible combinations of failures to identify tipping points that might result in a change of statistical conclusion. For the secondary end point of moderate or greater PVL at 1 year, superiority was tested with a χ2 test in the full analysis data set. The expected rates were 1.1% with the MEV and 5.3% with the SEV.3,6 Superiority was concluded if P<.05 and the 2-sided upper 95% confidence interval for the difference between groups (MEV − SEV) was less than 0. Given enrollment of 912 patients and 25% attrition, there was 86% power to show superiority. If the secondary end point was met, superiority testing of the primary effectiveness end point was prespecified (eAppendix in Supplement 1). Continuous variables were estimated as mean with standard deviation and compared using the t test. Discrete variables were reported as counts with percentages and differences were assessed using χ2 or Fisher exact tests. Time-to-event analyses were performed using the Kaplan-Meier method and compared using the log-rank test. No adjustment for multiple tests was performed other than the hierarchical testing strategy used for the primary and secondary end points; analyses other than the primary and secondary analysis should be considered exploratory. Because the rate of PVL might be lower with the SEV-E, which was introduced during the course of the study, a post hoc analysis of the primary effectiveness end point, secondary end point, and PVL over time was conducted in patients treated with the SEV-E compared with the MEV. This was an underpowered post hoc analysis and should be considered hypothesis generating only. Statistical analyses were performed with SAS software, version 9.2 or later (SAS Institute Inc). A total of 1232 patients were screened by the case review committee; 912 patients were randomized. One-year clinical follow-up for the last patient occurred on March 8, 2017. The full analysis set included 607 patients randomized to receive the MEV and 305 randomized to receive the SEV (Figure 1). Among randomized patients, 30 MEV patients 8 SEV patients did not undergo the assigned procedure leading to a population with implants of 577 in the MEV group and 297 in the SEV group (Figure 1). The 1-year full analysis data set included the 96.7% of MEV patients and 97.4% of SEV patients who had either a VARC event or clinical follow-up after 335 days. Transthoracic echocardiogram assessment at 1 year occurred in 89.8% of MEV patients and 85.7% of SEV patients alive at 1 year. The 2 groups were well balanced for baseline clinical, procedural, and echocardiographic characteristics (Table 1). In the full analysis population, the mean age was 82.8 (SD, 7.3) years and 51% of patients were women. The mean STS risk was 6.7% in the MEV group and 6.9% in the SEV group; 23% of patients were considered at extreme risk. The first-generation MEV was used throughout the study while the second-generation SEV was introduced midway in the study, leading to use of the SEV in 153 (51.5%) and the SEV-E in 144 (48.5%) of 297 SEV patients. Procedure time was higher but x-ray contrast use was lower with the MEV (Table 1). Antiplatelet and anticoagulant medication use was similar between groups (eTable 6 in Supplement 1). Testing of primary and secondary end points was performed hierarchically. Quiz Ref IDThe 30-day primary safety end point (all-cause mortality, stroke, life-threatening or major bleeding, stage 2/3 acute kidney injury, and major vascular complications) occurred in 20.3% of patients in the MEV group and in 17.2% of patients in the SEV group (difference, 3.1%; 1-sided Farrington-Manning 97.5% CI, −∞ to 8.3%; P = .003 for noninferiority) (Table 2). The Kaplan-Meier time-to-event curves for the primary safety end point for the MEV and SEV were similar at 1 year (Figure 2). The 1-year primary effectiveness end point (composite of all-cause mortality, disabling stroke, and moderate or greater PVL) occurred in 15.4% in the MEV group and in 25.5% in the SEV group (difference, −10.1%; 1-sided Farrington-Manning 97.5% CI, −∞ to −4.41%; P < .001 for noninferiority) (Table 2). Both noninferiority tests were performed in the implanted analysis set; similar results for both primary end points were observed in other analysis sets including the full analysis set (primary safety end point: 19.0% with the MEV and 16.2% with the SEV; difference, 2.8%, 1-sided 97.5% CI, −∞ to 7.8%; P = .001 for noninferiority; primary effectiveness end point: 15.8% with the MEV and 26.0% with the SEV; difference, −10.2%; 1-sided 97.5% CI, −∞ to −4.5%; P < .001 for noninferiority) (eTables 7 and 8 in Supplement 1). Tipping-point analyses suggested that it would be highly unlikely that the conclusions of the noninferiority and superiority testing of the primary end points would have been different if missing data were available (eFigures 1 and 2 in Supplement 1). The 1-year secondary end point of moderate or severe PVL occurred in 0.9% in the MEV group and 6.9% in the SEV group (difference, −6.1%; 95% CI, −9.6% to −2.6%; P < .001 for superiority) (Table 2 and eFigure 3 in Supplement 1). Quiz Ref IDIn superiority testing in the full analysis set, the between-group difference for the primary effectiveness end point also was significant (difference, −10.2%;  95% CI, −16.3% to −4.0%; P < .001 for superiority) (Table 2). Additional echocardiographic findings are shown in Table 2 and eTable 8 in Supplement 1. Effective orifice area was larger and mean aortic valve gradient was smaller in patients receiving the SEV compared with the MEV (Table 3). Exploratory clinical outcomes at 30 days and 1 year are shown for the full analysis set in Table 3 (for the population receiving implants in eTable 9 in Supplement 1). There was no significant difference in the combined end point of all-cause mortality or disabling stroke (Table 3 and Figure 2). Disabling stroke as an individual end point occurred less frequently with the MEV. Quiz Ref IDThe need for a new permanent pacemaker within 1 year was more common with the MEV (Table 3), as was valve thrombosis. Repeat procedures for valve-related dysfunction, prosthetic valve malpositioning, and TAV-in-TAV deployment were less frequent in the MEV group. No differences in New York Heart Association class, National Institutes of Health Stroke Scale, or Modified Rankin Scale were observed (eTables 10 and 11 in Supplement 1). Valve thrombosis was uncommon. There were more valve thromboses, defined by VARC-2 criteria,17,18 with the MEV (1.5% vs 0%). None of these patients died or had stroke. The findings of this study are consistent with a recent investigation that suggested that the supra-annular SEV may have a lower rate of valve thrombosis (MEV, 14%; SEV, 6%; balloon-expandable valve, 14%).34 Approaches to prevention, detection, and treatment of valve thrombosis are currently being studied.34,35. Aortic valve area was increased and transvalvular pressure gradient was decreased with both devices. The improved mean gradient was stable for both devices up to 1 year. Valve area was larger with the SEV because the leaflets reside in a less constrained supra-annular location. Hemodynamics observed with the MEV in this study were similar to what is reported in commercially available balloon-expandable valves and similar to or better than reported for many surgical valves.3 The long-term effect of hemodynamic differences between the MEV and the SEV observed in this study is unknown; longer-term follow-up will help elucidate this. This study has several limitations. First, the noninferiority margins used were based on available data at the time of protocol development. The margins were considered clinically reasonable and were similar on a relative basis to margins used in other major TAVR trials. However, the observed primary end-point event rates were lower than the expected rates used to develop these margins, perhaps because of rapid improvements in the TAVR procedure over time. In assessing the safety profile of the MEV, it is therefore important to consider the confidence intervals around the differences in primary end-point rates in addition to the prespecified statistical testing. The Kaplan-Meier time-to-event curves for the primary safety end point for the MEV and the SEV are very similar, providing additional clinical evidence that supports the statistical conclusion of noninferiority. Second, composite end points inherently include events of differing severities; assessment of all components individually (Table 3) is necessary for a more complete understanding of harms and benefits. Third, the MEV was compared in part against an early-generation SEV TAVR prosthesis, which has been largely replaced by the newer-generation SEV-E, which may have less PVL. Fourth, this study does not provide comparative data between MEVs and balloon-expandable valves. Fifth, only transfemoral access was used in this study, and because the MEV arterial sheath size was larger, there were more instances of the MEV not being implanted (Figure 1). Sixth, balloon aortic valvuloplasty before dilatation was required per protocol for all patients, which is no longer routine clinical practice. Seventh, all patients met defined high-risk criteria based on STS risk assessment or other factors; however, the mean STS score was less than 8, similar to other contemporary high-risk trial populations.6,36 Eighth, long-term assessment of durability of all bioprosthetic TAVR devices remains a limitation in this field. A randomized, double-blind study was conducted in 78 study centers from 15 countries as previously described.16 The study protocol is available in Supplement 1. Newborn infants who had a first-degree relative with type 1 diabetes and defined human leukocyte antigen (HLA) genotypes were recruited between May 2002 and January 2007 and followed up until the youngest participant reached 10 years of age in February 2017. Randomization of the infants who met the inclusion criteria took place before birth or immediately after birth (Figure 1). Randomization was stratified by study center, with a block size of 4. Written informed consent was obtained from the family before enrollment. The study was approved by the ethics committees of all participating centers. Infants were randomly assigned weaning to either the intervention or control formulas, which were produced specifically for this study. Randomization was carried out in each strata within 4 blocks. The intervention formula was an extensively hydrolyzed casein-based formula, while the control formula was composed of 80% intact cow’s milk protein and 20% hydrolyzed milk protein and formulated so that the taste and smell would be indistinguishable from the intervention formula. Study formulas were prepared and coded with the use of 4 colors by Mead Johnson Nutritional and were blinded to all investigators except the data management unit. Newborn infants requiring supplemental feeding before randomization (eg, infants born at night or on weekends) received banked breast milk or Nutramigen, an extensively hydrolyzed casein-based formula. Breastfeeding was practiced at the discretion of the participating mothers, and maternal diets were unmodified. Breastfeeding was encouraged and exceeded national averages in both groups.17 The dietary intervention period lasted until the infant was at least 6 months of age and, if by that time the child had not received the study formula for at least 60 days, study formula feeding was continued until 60 days of study formula exposure was reached, but not beyond 8 months of age. Parents were asked not to feed the children any commercial or other baby foods containing bovine protein during the intervention period. Adherence to the protocol was monitored by means of regular family nutrition interviews (at the age of 0.5, 1, 2, 3, 4, 5, 6, 7, and 8 months) and by the analysis of cow’s milk antibodies in serum samples. ICAs were detected using indirect immunofluorescence. The other 3 autoantibodies were quantified with the use of specific radiobinding assays in the Scientific Laboratory, Children’s Hospital, University of Helsinki, Helsinki, Finland, with cutoff limits for positivity of 2.5 JDF units for ICAs, 2.80 relative units (RU) for IAA, 5.36 RU for GAD autoantibodies, and 0.77 RU for IA-2 autoantibodies.18 The disease sensitivity and specificity of the ICA assay were 100% and 98%, respectively, in the fourth round of the international workshops on standardization of the ICA assay. According to the Diabetes Autoantibody Standardization Program and the International Autoantibody Standardization Program workshop results in 2002-2016, the disease sensitivities of the IAA, GAD autoantibody, and IA-2 autoantibody radiobinding assays were 42% to 62%, 70% to 92%, and 62% to 80%, respectively. The corresponding disease specificities were 93% to 99%, 90% to 98%, and 93% to 100%, respectively. The primary end point was the diagnosis of diabetes according to World Health Organization criteria.19 According to those criteria, the diagnosis is based on (1) symptoms + a single random plasma glucose level of 200 mg/dL or greater (to convert to mmol/L, multiply by 0.0555) or (2) if no symptoms, the diagnosis requires a raised random plasma glucose reading of 200 mg/dL or greater on 2 occasions, a raised fasting plasma glucose reading of 126 mg/dL or greater, or a diabetic oral glucose tolerance test (OGTT, fasting venous plasma glucose ≥126 mg/dL and/or a 2-hour venous plasma glucose ≥200 mg/dL) on 2 occasions. OGTTs were performed by protocol on all study participants who had not been previously diagnosed at 6 and 10 years of age and at study end. Additional OGTTs were performed as clinically indicated. All diagnosed cases were centrally reviewed. The cumulative incidence of diabetes onset from the time of randomization within each group was estimated using a modified Kaplan-Meier diabetes-free survival function. The difference between groups in the cumulative incidence functions, and the associated hazard functions, was tested using the Mantel–log rank test on discrete time to type 1 diabetes (6-month intervals). The relative risk of diabetes onset between groups was estimated from the discrete Cox proportional hazard model.20 The proportionality assumption of the Cox proportional hazard model was tested. First, the Schoenfeld residuals were examined to determine whether there was an association with time. Second, the interaction of parameters of interest and time were included in the models and tested for significance. For treatment and the variables used in the adjusted models, the null hypothesis of proportionality failed to be rejected. The analyses were adjusted for HLA risk, duration of breastfeeding, duration of study formula consumption, sex, and region, while treating study center as a random effect. The critical value for the test statistic (P = .047) and confidence intervals in this primary analysis were adjusted for multiple looks, which took place during the trial and were based on the Lan and DeMets21 spending function. When comparing data between the 2 study groups, the t test was applied for normally distributed variables and the nonparametric Mann-Whitney U test for skewed variables. The effects of weaning to the casein hydrolysate vs conventional formula were tested using the intention-to-treat principle including all HLA-eligible participants who were randomized to a treatment group. Tests of significance reported herein were 2-tailed. Statistical analyses were performed using SAS version 9.4 (SAS Institute). No imputation for missing values was performed; rather, observations with relevant missing values were excluded from respective analyses. The analysis of diabetes risk was also performed according to treatment received (per-protocol analysis). Participants were included in this analysis if they had exposure to the study formula for 60 days or longer and were not exposed to nonallowed foods. This study was designed such that given a confidence level of 95%, an estimated cumulative incidence of diabetes of 7.6% by the age of 10 years in the control group and an expected dropout rate of 20% by 10 years and a frequency of 10% of exclusive breastfeeding (up to age of 6 months), the study would have 80% power to detect a 40% change in the end point. As a post hoc analysis, the hazard ratio of the treatment groups was also calculated after adjusting for the age at which multiple autoantibodies appeared as an exploratory analysis. Altogether, 2159 newborn infants (1021 female [47.3%]) with an eligible HLA genotype (41.9% of the genotyped infants) were randomized to the intervention study. Five hundred sixteen infants (23.9%) carried the high-risk HLA genotype; 953 (44.1%), moderate-risk genotypes; 668 (31.0%), mild-risk genotypes; and 22 (1.0%), the rare mild-risk genotype. The first-degree relative with type 1 diabetes was the mother in 1052 infants (48.8%), the father in 722 (33.4%), and a sibling in 308 (14.3%), and 77 participants (3.5%) had multiple affected relatives. The median follow-up time for the diagnosis of diabetes was 11.5 years (Q1-Q3, 10.2-12.8 years; mean, 11.0 years). Randomization resulted in 1081 infants in the casein hydrolysate group and 1078 in the control group. There were no differences in the demographics or the distribution of HLA genotypes between the 2 groups (Table 1). Eighty percent of infants in the casein hydrolysate group and 80.9% in the control group were exposed to the study formula during the intervention period. The mean (SD) ages of the infants at the time of study formula introduction were 2.0 (2.3) months in the hydrolysate group and 1.8 (2.2) months in the control group (difference, 0.2 months [95% CI, 0-0.42]). The mean (SD) duration of study-formula feeding was 10.2 (9.3) weeks in the casein hydrolysate group and 11.7 (9.7) weeks in the control group (difference, 1.5 weeks [95% CI, 0.7-2.3]; P < .001). As previously reported, the analysis of cow’s milk antibodies confirmed that the families adhered well to the dietary intervention, resulting in conspicuous differences in the antibody levels between the treatment groups.13. The median age at initial seroconversion was 1.6 years (Q1-Q3, 1.0-3.0 years) in the casein hydrolysate group among those who progressed to clinical diabetes, whereas it was 1.5 years (Q1-Q3, 1.0-3.0 years; P = .38) among the progressors in the control group. The mean duration from seroconversion to clinical diabetes was 4.1 years (median, 3.5 years [Q1-Q3, 1.4-6.6]) in the casein hydrolysate group and 3.9 years (median, 3.1 years [Q1-Q3, 1.1-6.2]) in the control group (difference, 0.2 years [95% CI, −0.8 to 1.1]; P = .76). The number of participants who were positive for each specific autoantibody during the preclinical period is shown in Table 1. Five children (5.5%) in the casein hydrolysate group and 6 (7.3%) in the control group had no detectable autoantibodies before the diagnosis of diabetes (P = .62). At diagnosis, the number of autoantibody-negative participants had dropped to 4 (4.4%) and 5 (6.1%), respectively (difference, 1.7% [95% CI, −6.4% to 10.4%]). During follow-up, diabetes developed in 91 children in the casein hydrolysate group (8.4%) and in 82 in the control group (7.6%) (difference, 0.8% [95% CI, −1.6% to 3.2%]; P = .47; Figure 2). The hazard ratio for type 1 diabetes adjusted for HLA risk group, duration of breastfeeding, duration of study formula consumption, sex, and region, while treating study center as a random effect, was 1.1 (95% CI, 0.8-1.5; P = .46). There was no significant difference in the median age at diagnosis between the 2 groups (6.0 years [Q1-Q3, 3.1-8.9] vs 5.8 years [Q1-Q3, 2.6-9.1]; P = .75; difference, 0.2 years [95% CI, −0.9 to 1.2]). About one-fourth of the cases in each group were diagnosed without clinical symptoms (Table 2). Five children (5.5%) in the casein hydrolysate group and 3 (3.7%) in the control group presented with diabetic ketoacidosis (difference, 1.8% [95% CI, −6.3% to 9.8%]; P = .57). Comparisons between the treatment groups within HLA risk groups, according to the relationship to the affected family member (father, mother, or sibling with diabetes), geographic region associated with the clinical site of enrollment, or sex were not statistically significant (Table 3). The frequency of any infection was 0.90 events/year in the hydrolysate group and 0.93 events/year in the control group. The corresponding frequencies of upper respiratory infections were 0.48 and 0.50, respectively. The rate of other adverse events was of the same magnitude in the 2 groups (eTable in Supplement 2). Similar linear growth and weight gain were observed in both groups. In this international randomized trial in children with an HLA genotype conferring increased risk for type 1 diabetes and an affected first-degree relative, weaning to a highly hydrolyzed formula during infancy did not reduce the incidence of type 1 diabetes compared with cow’s milk–based formula. This outcome is consistent with the report of this trial that showed no difference between the study groups in the appearance of islet autoantibodies,15 but is not consistent with data from the pilot study,22 which reported that weaning to an extensively hydrolyzed formula in infancy was associated with a decrease in the frequency of disease-associated autoantibodies by the age of 7.5 years. That study was conducted in 230 Finnish children, while the current trial included 2159 high-risk children from 15 different countries, most participants being from Canada, Finland, and the United States. The larger number of participants in this study provides substantially greater statistical power in a more heterogeneous study population compared with the pilot study and, therefore, provides a more definitive answer to whether weaning to an extensively hydrolyzed formula is protective of diabetes. Overall, 173 participants (8.0%) progressed to type 1 diabetes during the follow-up for 11.5 years. This is close to an expected rate of 7.5% by the age of 10 years in the control group, on which the sample size estimate was based. For unknown reasons, the rate of diabetes was higher, although not significantly so, among females compared with males in the casein hydrolysate group. About 49% of the participants had a mother affected by type 1 diabetes, while only around 35% of those who presented with clinical disease had an affected mother. This reflects the well-known fact that offspring of mothers with type 1 diabetes have a reduced disease risk compared with offspring of affected fathers.23,24. Additional strengths of the current trial include a very high retention rate of participants and dietary adherence. The fact that the study was performed in 15 countries on 3 continents also supports the generalizability of the results. This study was planned to have 2 end points, namely (1) positivity for 2 autoantibodies by the age of 6 years and (2) clinical diabetes by the age of 10 years. While the previous report of this study showed no benefit in terms of a reduction in seroconversion to autoantibody positivity,15 the follow-up of the trial participants to 10 to 14 years of age enabled the study to evaluate the possible effect of the treatment on progression from autoimmunity to diabetes. The casein-based formula used as the intervention modality in this study was highly hydrolyzed and did not contain intact proteins. Less than 0.3% of the peptides had a molecular weight exceeding 2000 Da. Accordingly, the formula should be free of intact bovine insulin, which is present in cow’s milk.26 Vaarala et al26 showed that infants fed a conventional cow’s milk–based formula before the age of 3 months developed a strong immune response to bovine insulin, which differs from human insulin by 3 amino acids. Infants developing early signs of β-cell autoimmunity lacked the capacity to mount oral tolerance to bovine insulin. It has been speculated that sustained bovine insulin immunity might contribute to prediabetes progression, as weaning to an insulin-free formula reduced the cumulative incidence of autoantibodies by more than half in young children at genetic risk for type 1 diabetes.27 The current data do not, however, support the bovine insulin hypothesis. An international survey suggests a need for improved management after hospital discharge. Of 1475 patients who survived hospitalization for sepsis, there was only low to moderate satisfaction with support services after they were discharged.5 In addition, rehospitalization after sepsis accounts for 12.2% of all US hospital readmissions and 14.5% of readmission costs.9 Therefore, improving medical care after sepsis hospitalizations may reduce health care utilization and costs. Although physical disability, cognitive impairment, and hospital readmission are common after sepsis, sepsis treatment guidelines provide no recommendations on posthospital management.10 This article reviews the epidemiology, pathophysiology, and clinical sequelae in the months following hospital discharge of patients treated for sepsis. Management strategies and directions for future research are also reviewed. A literature search of MEDLINE was conducted in PubMED through April 26, 2017, using search terms and synonyms for sepsis and survivors. Non-English language articles or those published before January 1, 2000, were excluded. Bibliographies of retrieved studies were searched for other relevant studies. Articles were reviewed for their contribution to current understanding of sepsis survivorship, with priority given to clinical trials, large longitudinal observational studies, and more recently published articles. Based on experimental and human volunteer models, sepsis was initially presumed to be an extreme, body-wide inflammatory response that led to alterations in microvascular flow, endothelial leak, and compromised parenchymal cell function, manifesting clinically as inadequate tissue perfusion and multisystem organ dysfunction. However, more recent evidence demonstrates that the pathophysiological response is more complex and variable17 (Figure 2). First, the initial host response includes activation of proinflammatory pathways and anti-inflammatory innate immune pathways, as well as alterations in adaptive immune pathways. Second, the characteristics of immune system changes vary and depend on both host and pathogen characteristics, as well as recent medical events (eg, surgery, other infection) and treatment (eg, timing of antibiotics).17 Third, the resolution of immune system changes in response to sepsis is complex and frequently prolonged. Many patients continue to have inflammatory changes, immune suppression, or both after sepsis.18. The reasons for these immune system changes are complex and include epigenetic20 and metabolic21 reprogramming of immune cells induced by the original septic insult and by on-going changes in the host environment, such as neuroendocrine22 or microbiome23 alterations. These processes continue despite successful eradication of the initial pathogen and increase a patient’s risk of secondary episodes of infection or sepsis. The combination of the initial septic insult and ongoing abnormalities in host control systems contributes to persistent organ dysfunction. The severity of immune suppression and organ dysfunction after sepsis treatment is influenced by a patient’s presepsis health and by characteristics of the infection (pathogen load, virulence), host response, and the quality of early sepsis treatment. Patients may also experience sequelae from iatrogenic complications24,25 and medication errors26-28 during and after hospitalization. Recovery from sepsis also varies (Figure 2). There are no validated tools to estimate a patient’s likelihood of complete recovery. However, several prognostic factors have been identified.  Patients with preexisting disability, frailty, or nursing home use are less likely to regain functional independence,29-34 while previously healthy patients have a higher chance of recovery. Importantly, the severity of cognitive impairments shortly after hospitalization do not predict well subsequent impairment.35. After hospitalization for sepsis, a patient’s ability to function independently frequently declines. Patients treated for sepsis typically develop 1 to 2 new limitations of activities of daily living (ADLs), such as inability to manage money, bathe, or toilet independently6 after hospital discharge (Table 1). Quiz Ref IDThe causes of functional decline are multifactorial. Patients often develop physical weakness following critical illness, which may be due to myopathy, neuropathy,36 cardiorespiratory impairments, cognitive impairment, or a combination of these conditions. The extent to which anxiety, depression, or PTSD are exacerbated by sepsis is unclear. In a study involving 439 Health and Retirement Study participants, the prevalence of clinically significant depressive symptoms was 28% before sepsis and 28% after sepsis. However, in a population-based Danish cohort, 9912 critically ill patients without prior psychological history were more likely than hospitalized controls to receive new psychoactive prescriptions (12.7% vs 5.0%; adjusted HR, 2.5; 95% CI, 2.19-2.74; P<.001) or new psychiatric diagnoses (0.5% vs 0.2%; adjusted HR, 3.4; 95% CI, 1.96-5.99; P<.001) in the 3 months after hospitalization.49 It is unclear whether depression, anxiety, or PTSD are exacerbated by sepsis, or merely more common among patients who develop sepsis. However, it is important to recognize and treat mental health impairments because they are associated with a more complicated clinical course.50,51. The only RCT80 that studied recovery after sepsis hospitalization randomized 291 patients to a multicomponent primary care management intervention vs usual care. The intervention included education for patients and clinicians about sepsis and its common sequelae; case management by nurses with ICU experience, focusing on proactive symptom monitoring; and decision support by physicians trained in both primary and critical care. The primary outcome was mental health—related quality of life at 6 months. However, 32 outcomes were measured, each at 6 and 12 months. The intervention group performed better on 5 of 64 outcomes (Short Musculoskeletal Function Assessment, Physical Function (XSMF-A [Extra Short Musculoskeletal Function Assessment regarding physical function] ) and Disability (XSMF-B) scales at 6 months, ADL limitations at 6 and 12 months, and Regensburg Insomnia Scale at 12 months), suggesting a potential effect on functional outcomes.80 However, given the number of outcomes measured, these positive findings must be considered exploratory.80. Because of the heterogeneity of potential problems after sepsis, clinicians may consider early referrals to multiple subspecialists and ancillary services. However, it is important to consider the experience of sepsis survivors within the cumulative complexity model framework,87 which conceptualizes patient experience as a balance between workload (the work of being a patient, including effort to understand, access, and use medical care) and capacity (the quality and availability of resources to facilitate being a patient). This framework acknowledges the challenges of adhering to medical care, and suggests that overly complex treatment plans have limitations. Patients with a recent sepsis hospitalization may experience several new barriers to carrying out treatment plans, such as new weakness, cognitive impairment, fatigue, lost income, or stressed caregivers. Clinicians should be aware of these challenges and should consider starting with 1 or 2 referrals to address the most significant symptoms, then place additional referrals over time. Many important questions about postsepsis morbidity remain unanswered. Researchers generally consider sepsis from the starting point of hospital admission. Although this may be appropriate for healthy patients, it may be inappropriate for patients whose health was declining prior to sepsis. Future research is needed to better characterize how presepsis health affects long-term outcomes after sepsis. As in-hospital sepsis mortality has decreased, an estimated 14 million patients survived hospitalization for sepsis in 2016. These patients often acquire new physical disability and cognitive impairment following sepsis and may experience further health deterioration after hospital discharge. Risk of subsequent infection, cardiovascular events, acute renal failure, and aspiration are increased after hospitalization for sepsis. Further research is needed to determine the optimal approach to caring for patients who have survived sepsis. In 1993, Ito et al proposed that small papillary thyroid microcarcinomas (PTMCs; size <1 cm) might have less malignant potential than previously thought. They initiated an observational surveillance trial4 selecting a cohort of Japanese patients with likely indolent PTMC and excluding those with high-risk features such as known lymph node or distant metastasis, tracheal invasion, or close proximity to the recurrent laryngeal nerve. The surveillance protocol included neck ultrasounds initially every 6 months for 2 years and then yearly. After 10 years of follow-up, only 3% to 4% of these patients with PTMC developed lymph node metastasis and 8% had documented nodule growth defined as an increase in any dimension of 3 mm or greater.4 Younger patients (aged <40 years) were more likely to have nodule growth. Rescue surgery in patients with disease progression was associated with outcomes equivalent to those expected had the lesion been removed when first diagnosed. These results were replicated in a similar study by Sugitani et al5 involving another cohort of Japanese patients. Based on these findings, active surveillance was included as a possible treatment option for patients with newly diagnosed PTMC in the most recent guidelines from the American Thyroid Association.3. Extending these observations to a US cohort, Tuttle and colleagues6 reported the short-term outcomes in the October 2017 issue of JAMA Otolaryngology–Head & Neck Surgery for 291 patients forgoing initial thyroidectomy for PTMC, which included 59 patients with larger tumors measuring 1.1 to 1.5 cm. With a short median follow-up of 25 months, no cases of de novo lymph node metastasis were found. There was a low (3%-4%) rate of nodule growth during the observation period, the likelihood of tumor growth was greater for younger patients (aged <40 years), and delayed thyroidectomy had excellent outcomes. Because there was interobserver and intraobserver variability of thyroid nodule size assessment by ultrasound, Tuttle et al6 stressed that surveillance images should be acquired and interpreted by an experienced sonographer. Treating small seemingly low-risk PTC by either active surveillance or surgery may thus appear to result in similar outcomes. Because patient-related factors such as younger age or history of familial thyroid cancer may increase the likelihood of cancer progression, surgery may be preferred when these characteristics are present. In addition, not all patients will be comfortable with nonoperative cancer treatment and may want surgery regardless of the low risk of their tumor. One study of 1174 patients with thyroid cancer assessed quality of life using the thyroid cancer–specific City of Hope quality of life instrument and found a reduction in quality of life within the first 5 years of treatment for their cancer.7 However, because nearly all patients had conventional aggressive treatment, it is not known if the decreased quality of life was related to treatment-related complications, adverse effects related to long-term suppression of the thyroid-stimulating hormone, or to cancer-specific concerns that guide surveillance such as recurrence or metastasis. To better inform shared decision making, future improvements in preoperative risk stratification will be essential. Ultrasound imaging provides excellent information on lateral nodal disease and gross extrathyroidal extension but does not assess histological features that are associated with higher risk of recurrence. A study using Surveillance, Epidemiology, and End Results data by Lim et al1 demonstrated a low but significant increase in mortality (1.1% per year), but a better understanding is still needed of what epidemiological and tumor-related factors are contributing to this increase. Several novel preoperative molecular tests are available that can identify common genetic mutations and rearrangements associated with thyroid cancers and can improve diagnostic sensitivity and specificity of fine needle aspiration biopsy. Moreover, the specific genetic profile of PTC can also be correlated with thyroid cancer subtype, histological features, and ultimately, short-term risk of recurrence.8,9 Molecular profiles of more aggressive cancers associated with mortality and recurrence include having the TERT gene or 1 or more detected mutations, and such cancers may not be ideal for active surveillance. The report by Tuttle et al6 did not stratify patients by available mutational status, and it may well be useful to compare the tumor genetic profile in the 2% of patients who were clinically assessed as inappropriate for observation with that of the 4% of patients who were assessed as ideal for surveillance. It is likely that accurate prediction of tumor biological behavior will eventually require a comprehensive risk assessment using clinical, imaging, cytological, and molecular variables. It is valuable and reassuring to know that most patients in the United States with short-term surveillance of small thyroid cancers did well. Perhaps the most important aspect of the study by Tuttle et al6 is in providing a roadmap for how to perform longitudinal studies of PTMC surveillance. There is little evidence to date regarding the relationship between tumor volume or diameter and prognosis because most patients with tumor enlargement still had excellent disease-related outcomes despite delayed surgery. However, the findings reported by Tuttle et al6 provide compelling data for using a volume increase of greater than 50% as a highly sensitive marker for disease progression. A 59-year-old black woman with a history of hypertension, hyperlipidemia, and type 2 diabetes presented with a 2-month history of a rapidly enlarging tender nodule on her right fifth digit. She denied fevers, joint pain, joint stiffness, inciting trauma, or recent outdoor activity. She had no history of immunosuppression. Results of a recent complete blood cell count and comprehensive metabolic profile were unremarkable. Physical examination showed a single 25-mm, firm, dome-shaped subcutaneous nodule with central ulceration on the lateral aspect of the right fifth digit (Figure 1). There was no erythema, edema, fluctuance, induration, or lymphangitis surrounding the lesion. No lymphadenopathy was present. The key to the correct diagnosis is the sudden onset of an isolated lesion without history of antecedent trauma, which is concerning for malignancy and warrants a biopsy. Physical examination findings of a nonfluctuant, noninflamed nodule are inconsistent with an abscess requiring incision and drainage. A touch preparation, which involves tissue sampling and staining for rapid identification of fungal organisms under microscopy, may be used if the suspicion for a fungal infection such as sporotrichosis is high. However, the patient lacked lymphadenopathy or other signs of infection, was not immunocompromised, and had no recent injury or outdoor exposure. Skin biopsy in this case would also rule out deep fungal, atypical mycobacterial, or viral infection as the etiology. The patient has no history or symptoms suggestive of osteoarthritis, and the presence of a solitary nodule not localized to the distal interphalangeal joint is unlikely to be a Heberden node. Thus, the patient would not benefit from anti-inflammatory therapy. MCC is seldom suspected prior to biopsy because of its nonspecific, often benign appearance. In this case, histopathology revealed a dermal tumor with nodular and trabecular aggregates of neoplastic cells having rounded finely granular nuclei and scant cytoplasm (Figure 2). Biomarkers distinguish MCC from metastatic small cell lung cancer, other neuroendocrine tumors, melanoma, and lymphoma.8 An appropriate immunohistochemical panel should include cytokeratin 20 (CK20) and thyroid transcription factor-1 (TTF-1).8 CK20 and most low-molecular-weight cytokeratin markers (pancytokeratins AE1/AE3) are typically positive in MCC, while TTF-1 is negative.8 In this patient, CK20 and pancytokeratin stains demonstrated a paranuclear pattern. Synaptophysin and chromogranin stains were positive, while CD45, CK7, S100, and TTF-1 stains were negative, which supported a diagnosis of MCC. Polymerase chain reaction and sequencing analysis of DNA obtained from tumor tissue and control tissue demonstrated MCPyV, with 995 copies/ng DNA in the tumor sample and only 38 viral copies/ng DNA in the control skin sample.6 Western blot analysis of the tumor sample showed viral large T antigen expressed in truncated form. Early diagnosis and treatment of MCC are paramount because of its aggressive nature. Management of MCC includes wide local excision of the primary tumor with sentinel lymph node biopsy. Preoperative or postoperative baseline imaging studies should also be performed for staging. Treatment of MCC located on a digit, as in this case, may involve amputation of the digit, as wide local excision may not allow preservation of digit function. Positive nodal disease can be managed with complete lymphadenectomy, radiation, or both,8 with the possible addition of avelumab, a fully human monoclonal IgG anti–PDL-1 antibody. Distant metastatic disease can be managed with avelumab or chemotherapy.8 Chemotherapy produces a good treatment response but does not prolong survival and should be decided on a case-by-case basis.8 Many patients with node-positive or metastatic MCC are enrolled in clinical trials. The only  treatment approved by the US Food and Drug Administration for metastatic MCC is avelumab. Avelumab has demonstrated durable response rates in disseminated disease,9 and a future trial will evaluate avelumab as an adjuvant therapy for nodal disease (clinicaltrials.gov NCT03271372). Other therapies being investigated include anti–PD-L1 monoclonal antibodies pembrolizumab and nivolumab, cytokine therapy, kinase inhibitors, somatostatin analogues, and MCPyV-specific adoptive T-cell therapies.8 MCC prognosis is variable and depends on stage of disease at diagnosis. Overall, local recurrence after excision occurs in about 40% of cases and is associated with a 5-year survival rate of 50%, while distant metastasis portends a 1-year survival rate of 44%.10. Wide local excision and concurrent axillary sentinel lymph node biopsy were performed. Negative margins were obtained with preservation of the digital extensor mechanism and no bony involvement, preventing the need for amputation. Three of 4 deep axillary sentinel nodes were positive for MCC. Postoperative positron emission tomography–computed tomography (PET-CT) scan demonstrated focal uptake within 1 right epitrochlear node, which was confirmed to be metastatic MCC by fine-needle aspiration. Complete lymphadenectomy of the epitrochlear and axillary nodes was planned pending insurance approval. In the interim, neoadjuvant avelumab therapy was initiated; after 4 cycles, PET-CT scan showed no signs of active disease. Lymphadenectomy was then deferred. The patient will undergo radiation therapy to her fifth digit, epitrochlear, and axillary nodes and will continue to receive avelumab for a total of 12 months of therapy. Dr Cohen:It’s turned out to be a difficult opponent. It's relatively common, frequently disabling, but very unpredictable. Each patient has different manifestations and a different course. Also, the disease appears to change over time. It begins primarily as an inflammatory process. That's the target for most of our currently approved medications. But then [it] evolves into a more degenerative disorder, and so currently the main unmet need is to try to treat that degenerative phase. Dr Hauser:In 85% or 90% of people, the illness begins in early to mid-adult life with attacks of neurological dysfunction, reflecting disconnection between sensory or motor pathways of the brain, producing symptoms such as visual blurring, double vision, loss of sensory and motor coordination, or bowel or bladder disturbances. Such relapses tend to appear approximately once every 9 to 12 months, followed by stabilization and often, early in the disease, partial or complete recovery. That form of the disease is called relapsing-remitting MS.But over time, the relapsing form transitions to a progressive march of worsening symptoms, usually motor disability, leading to cane, crutch, and wheelchair dependence. That form is called secondary progressive MS. And in the older literature, about 2% to 3% of people with relapsing MS would convert to secondary progressive MS annually. So after 15 years, nearly half of people with relapsing MS had developed progressive MS. However, in the age of therapy, there is incomplete but increasingly convincing data that the long-term course of MS has been favorably modified and that the transition from relapsing to progressive MS has been blunted, perhaps to about 1% a year.Ten to 15% of people with MS have progressive symptoms from [the] onset. That form is called primary progressive MS. The female predominance in relapsing MS is not true in primary progressive MS, and in fact males and females are approximately equally affected and [it] tends to begin about 10 years later than the relapsing phase. Dr Hauser:It began with the concept that to create an animal model that looked like the type of tissue changes our patients have, one needed [involvement of] not only T lymphocytes but also B lymphocytes. That then led to the preliminary testing of the anti-CD20 B cell therapy rituximab, a chimeric monoclonal antibody widely used against lymphoma and rheumatoid arthritis and some other autoimmune diseases that showed remarkable efficacy against relapsing forms of MS. This led to the development of the current new FDA-approved therapy, ocrelizumab, which in recently completed clinical trials demonstrated very robust efficacy against relapsing-remitting MS and for the first time efficacy against primary progressive MS.These studies published as the OPERA and ORATORIO trials in the New England Journal of Medicine were very exciting because of the effect size against relapsing-remitting MS. [There was] an almost 50% decrease in the clinical relapse rate [the primary end point] compared with high-dose interferon beta-1a therapy. The therapy was 95% more effective than high-dose interferon in preventing new areas of white matter inflammation. Compared with baseline MRI [magnetic resonance imaging] data, [it] resulted in a 99% decrease in such new areas. In the primary progressive MS trial, there was a statistically significant reduction of disability accumulation confirmed at 12 weeks with ocrelizumab compared with placebo. But the effect size was an only 24% relative reduction in disability progression. So extremely effective against new white matter plaques, extremely effective against new clinical relapses, but only partially effective against progressive disability accumulation.However, the door against progressive MS is now ajar. This was the first trial demonstrating a significant improvement in disease progression for any drug, for any MS therapy. Dr Hauser:Ocrelizumab is administered as an outpatient infusion every 6 months. The relapsing-remitting MS trials were 2 years in duration. Infusion reactions occurred in slightly more than a third of patients, and beyond that, the therapy was well tolerated. There was an imbalance in the number of malignancies, particularly breast cancers in women, seen with ocrelizumab, compared with interferon or placebo. We’re not certain if this was significant. It’s encouraging that the incidence of malignancy in the ocrelizumab group is similar to epidemiological expectations. And in open label extension trials the malignancy rate seems to be decreasing. Dr Cohen:One major unmet need in the MS field is treatment strategies that promote repair and specifically remyelination. Traditionally, we thought of using stem cells to accomplish that and there now have been a number of small studies of so-called mesenchymal stem cells that are probably naturally involved in mitigating inflammatory damage and promoting repair. However, one thing we've learned from those studies is that direct cell transfer has a number of practical challenges. In fact, we don't understand particularly well how to best grow the cells in the laboratory to improve their efficacy.Also, what we've learned is that these cells probably are of benefit not so much by directly replacing the cells of the central nervous system (CNS) but by producing a wide range of trophic or supportive factors that improve the intrinsic repair capabilities of the CNS. That's led to a very interesting line of research to use stem cell–based high-throughput screening to identify candidate medications that may promote remyelination and then take those through a series of more stringent assays to look at their ability to promote remyelination in animal models.And then finally, to test those in people. Using this approach, one could look at medications that are already approved for other indications. A number of laboratories, some in Cleveland and San Francisco, have identified potential candidates and one of those—clemastine, an antihistamine with antimuscarinic effects—has been tested in a phase 2 clinical trial and showed some promising results in improving optic nerve function. So I think we're at the cusp of not only being able to control the inflammatory aspects of the disease but to also be able to promote repair of the damaged nervous system. Dr Hauser:It's likely that the microbiome has a substantial effect on MS in 2 ways. The first is the molecular mimicry, where some cell component of a bacterium or virus resembles a protein or substance in the brain. The second is that our microbiota influence the regulatory tone of our immune system, and that could lead to differences in the trajectory of disease after it begins.We don't yet know what the likely microbial culprit or culprits are. The strongest epidemiologic link to multiple sclerosis remains a late onset infection with the Epstein-Barr virus (EBV), [perhaps] the occurrence of infectious mononucleosis, but we really don’t know if EBV is causally linked or is a surrogate for some other infectious agent. The sensor tracks ingestion of the pill along with a patch worn on the torso that contains another sensor. Inside the pill’s sensor are magnesium and cuprous chloride, which activate the patch’s sensor upon contact with gastric fluid. The patch then transmits ingestion data to a mobile app that enables patients to check on a smartphone that they’ve taken their medication. Patients also can authorize health professionals, family members, or others to access the data through a web-based portal. In a trial and expanded access protocols, 23 patients aged 5 months to 25 years received infusions of vestronidase alfa-vjbk every 2 weeks for up to 164 weeks. Efficacy was evaluated in 10 patients who could perform a 6-minute walk test. After 24 weeks, patients who received the drug could walk a mean of 18 meters, or about 59 feet, farther than those who received a placebo. Their symptoms improved or stabilized during 120 weeks of additional follow-up. Envision a society that fully connects the usually disparate worlds of health and housing. Clinicians would implement strategies that feature “housing as a vaccine” to prevent illness and disability. Professionals from both worlds would routinely link lodging with counseling, case management, and other services to ensure that supportive housing more robustly meets the needs of vulnerable people. Officials would blend funding streams from health departments and the US Department of Housing and Urban Development (HUD) to address health equity and affordability challenges for the estimated 38.9 million households (2015) that are spending more than 30% of income on housing. Steps toward realizing this vision have begun. More professionals and national organizations recognize housing as a key driver and major social determinant of health, as summarized by a 2013 Cochrane Collaboration review. A 2017 National Academy of Medicine (NAM) report and the Centers for Disease Control and Prevention’s “HI-5” (“Health Impact in Five Years”) program, among others, recommend cross-sector collaborations to address a long-standing equity issue: an estimated two-thirds of those receiving HUD affordable housing support are racial/ethnic minorities. Health organizations, cities, and states are also increasingly adopting a “Housing First” approach that “homes” people with substance use disorders and other chronic illnesses without first requiring them to reach sobriety or other milestones. The National Governors Association has encouraged innovative strategies in its 2016 bipartisan report Housing as Health Care. Health organizations have increasingly partnered with community groups and other service providers to help patients address housing insecurity. Medical-legal partnerships (MLPs), now represented by nearly 300 health organizations in 41 states, embed legal professionals in health organizations to bring supportive housing to vulnerable patients: children with asthma triggered by suboptimal housing conditions; low-income, chronically ill seniors seeking to prevent or delay entry into long-term care facilities; people who are homeless; and people with mental and physical disabilities, among others. Recent publications note the innovative potential for MLPs to address health disparities as well as the need for more long-term evaluation. In the past 2 decades, Health Leads has also connected patients to housing and other community-based resources as a standard part of quality care; one evaluation of cardiometabolic outcomes in primary care patients found that efforts to screen for and address basic unmet resource needs (regarding food, housing, medications) were associated with modest improvements in blood pressure and lipid levels. New efforts also encourage clinician activity as part of the patient encounter. For example, the Center for Medicare & Medicaid Innovation released a screening tool to probe patient needs in housing (as well as other major social domains) as part of its Accountable Health Communities model. Related efforts such as Children’s HealthWatch, launched by Boston Medical Center in 1998, encourage assessment of housing stability as part of patient visits. The Social Interventions Research & Evaluation Network (SIREN) at University of California, San Francisco, offers health professionals an evidence library on housing (and other interventions) to address social needs. The Supreme Court’s Olmstead v L. C. ruling (1999) that people with mental disabilities are entitled to services in the least restrictive setting prompted state and federal governments to begin to merge some Medicaid and housing funding. The 2010 Affordable Care Act (ACA) subsequently encouraged new payment and delivery approaches to address housing within health care settings, thus reaching a wider array of high-need, high-cost populations. Accountable care organizations have begun to address housing in Oregon, Utah, and Vermont, as have some Medicaid managed care organizations, such as Mercy Maricopa Integrated Care in Phoenix, Arizona. Additional ACA-related developments include the Community First Choice Option, which allows states to help Medicaid recipients at risk of institutionalization to access personal attendant services and supports in a home- and community-based setting. In an important 2015 announcement, the Centers for Medicare & Medicaid Services (CMS) clarified that Medicaid, while not covering rent, could fund certain services (for people who are homeless or with disabilities) regarding housing transition (from institutions to communities) as well as for maintenance of tenancy after housing is secured. Medicaid’s Innovation Accelerator Program provides technical assistance to states promoting community integration for beneficiaries. An upcoming 2018 NAM report will address the extent to which evidence-based interventions, including Housing First, can improve health for people who are homeless. Regarding the ACA’s nonprofit hospital requirements for community benefit strategies, Community Catalyst, a national advocacy organization, has served as a leading consumer voice in encouraging implementation. For example, a Boston Children’s Hospital community benefit pilot program addressed asthma-related health disparities for black and Hispanic children in low-income neighborhoods through broad interventions: case management, community health worker home visits for education and medication adherence, and remediation to improve air quality in homes and schools. Outcomes that included reduced asthma-related hospitalizations in the intervention group vs a comparison group informed a subsequent pilot bundled-payment program for pediatric patients at high risk of asthma in Massachusetts Medicaid.  . Despite the need for hospitals and health systems to accelerate investments in affordable housing, as recently advocated in a JAMA Viewpoint, looming federal actions on multiple fronts could discourage action. Congressional attempts to repeal the ACA and to deeply cut Medicaid funding leave states skittish about further innovation. The prospect of reduced health insurance coverage and increased hospital uncompensated care costs could prompt hospitals to scale back community-benefit investments. A proposed $3 billion cut in HUD’s budget for fiscal-year 2018 could weaken already underfunded affordable housing efforts, such as public housing and vouchers to reduce private unit rental costs. And some of the tax-reform options currently before Congress could potentially reduce funding for construction and rehabilitation of multifamily housing for low-income renters. Like other investigational glucose-responsive insulin systems, the strategy may allow patients with diabetes to stop tracking their blood glucose levels and injecting themselves with insulin. Traditional insulin replacement therapy can be painful and time consuming, and incorrect administration of insulin can lead to complications. Transplants of pancreatic cells may be a potential solution that removes the possibility of human error, but they are expensive, require donor cells and immunosuppressant drugs, and often fail due to the destruction of the transplanted cells. Closed-loop artificial pancreas systems that directly connect devices that track blood glucose levels and administer insulin are also being further developed; however, these approaches involve mechanical sensors and pumps, with catheters that must be replaced periodically and increase a patient’s risk of infection, inflammation, and scarring. The success of the AβC treatment in mice relies on specially designed insulin-filled vesicles. A rise in blood glucose levels leads to high glucose uptake and oxidation, generating a low pH within an individual AβC cell. This induces structural changes in synthetic peptides tethered to the insulin-loaded inner vesicles, ultimately causing them to fuse with the outer membrane of the AβC and release the insulin payload. When the glucose concentration declines to a normal range, glucose uptake decreases and the inner vesicle pH level increases, thus halting insulin release. Scientists are now working to optimize and test the AβCs in larger animals, and to eventually develop a skin patch delivery system for analysis in animals and humans. They indicated that transplanting the cells directly within an injectable gel or delivering them through transcutaneous microneedle patches may restore blood glucose balance while avoiding the use of immunosuppressive drugs that are required when live cells are transplanted. The researchers stressed, however, that considerable work is needed to optimize the artificial cell approach before human studies are attempted. Gu and his team are also working on a separate cell-free smart insulin skin patch that senses blood glucose levels and secretes insulin into the bloodstream as needed. Quiz Ref IDThe study population included adults (aged ≥18 years) on the deceased donor kidney transplantation waiting list for a first kidney transplantation between January 1, 1995, and December 31, 2014. On the OPTN adult kidney transplantation registration forms, which were completed by clinicians at the transplantation center, the race/ethnicity of all candidates included in this study was designated using 1 of the following fixed categories: white, black/African American, Hispanic/Latino, or Asian. This study was not sufficiently powered to examine temporal changes in live donor kidney transplantation disparities among candidates identified as American Indian or Alaska Native, Native Hawaiian or other Pacific Islander, or multiracial due to the small population sizes during some of the years. Five candidates (0.001%) did not have a racial/ethnic assignment and were excluded from the study. The primary study outcome of time to live donor kidney transplantation was determined by the transplant date documented on the OPTN forms by the transplantation center. Clinical and demographic characteristics were stratified by kidney transplantation candidate race/ethnicity and year of placement on the deceased donor kidney transplantation waiting list. Wilcoxon rank sum (for continuous variables) and χ2 tests (for categorical variables) were performed to compare distributions and assess statistical significance. Kaplan-Meier methods, which accounted for the competing risks of deceased donor kidney transplantation and mortality, were used to assess candidate racial/ethnic differences in the estimated cumulative incidence of live donor kidney transplantation at 2 years after appearing on the waiting list. In addition, among live donor kidney transplantation recipients, racial/ethnic differences in recipient and donor characteristics were assessed over time. Wilcoxon rank sum and χ2 tests were performed to assess statistical significance between racial/ethnic differences. All regression models were adjusted for biologically plausible confounders (age, sex, body mass index [calculated as weight in kilograms divided by height in meters squared], panel reactive antibody, and ABO blood type) measured at the time of appearing on the waiting list. Statistical interaction terms were used in regression models to formally test the statistical significance of temporal changes in racial/ethnic disparities in the receipt of live donor kidney transplantation. To evaluate whether differences in live donor kidney transplantation were related to differences in deceased donor kidney transplantation or death (ie, informative censoring), the main analyses were repeated using competing risk models according to the methods of Fine and Gray17 (estimating adjusted subhazard ratios). The analyses in which death and deceased donor kidney transplantation were treated as separate competing risk events were adjusted for the same variables described above. Complementary log-log plots and Schoenfeld residuals were examined to assess the proportional hazards assumption. The variance inflation factor and Eigen values were used to test for colinearity among the vector of explanatory variables. The standard SRTR risk adjustment approach was used for handling missing data. With the SRTR approach, missing variable levels were modeled separately from known variable levels in the regression models. The robustness of estimates was tested by comparing results from an alternate modeling approach to handle missing data (multiple imputation), and the inferences remained the same for the study outcomes of interest. The correction for false-discovery rate and the Bonferroni correction were used to account for multiple comparisons. Among 453 162 adult kidney transplantation candidates identified within the SRTR between 1995 and 2014 (mean [SD] age, 50.9 [13.1] years; 39% were women; 48% were white [n = 217 040]; 30%, black [n = 135 333]; 16%, Hispanic [n = 71 912]; and 6%, Asian [n = 28 877]), 59 516 (13.1%) received live donor kidney transplantation. Study participants were censored due to the following reasons: death (n = 30 653), deceased donor kidney transplantation (n = 86 008), waiting list removal due to reasons other than receipt of kidney transplantation or death (n = 50 748), or at 2 years after initial listing (n = 226 237). Of all live donor kidney transplantation recipients, 87.5% received live donor kidney transplantation within 2 years of placement on the waiting list and these outcomes were included in the study. After 2 years of placement on the waiting list, which is outside the study scope, 12.5% of patients received live donor kidney transplantations. Mean age, body mass index, and prevalence of end-stage kidney disease secondary to diabetes increased from 1995 to 2014. Black and Hispanic patients were younger and also less likely to have college degrees than Asian and white patients. The prevalence of private health insurance was highest among white and Asian patients, and the prevalence of Medicare as the primary coverage was highest among black and Hispanic patients. The prevalence of end-stage kidney disease due to hypertension or diabetes was highest among black and Hispanic patients, whereas the prevalence of end-stage kidney disease due to glomerular diseases or other causes was highest among white and Asian patients. Black and Hispanic patients spent the longest time receiving dialysis and had the highest prevalence of body mass index of 35 or greater (Table 1 and Table 2). In 1995-1999, there were 5472 live donor kidney transplantations within 2 years of placement on the waiting list among white patients, 1351 among black patients, 999 among Hispanic patients, and 316 among Asian patients. In 2000-2004, there were 9096 live donor kidney transplantations among white patients, 2445 among black patients, 1963 among Hispanic patients, and 605 among Asian patients. In 2005-2009, there were 12 293 live donor kidney transplantations among white patients, 2718 among black patients, 2628 among Hispanic patients, and 834 among Asian patients. In 2010-2014, there were 12 648 live donor kidney transplantations among white patients, 2412 among black patients, 2767 among Hispanic patients, and 969 among Asian patients (Table 3). This study has several strengths. The first is the ability to comprehensively analyze 2 decades of national data to assess temporal changes in live donor kidney transplantation by race/ethnicity. A second strength is the ability to examine a well-characterized US population of adult kidney transplantation candidates with comprehensive follow-up for transplantation, death, or waiting list removal. A third strength of the study is the ability to account for differences in many important characteristics that might confound or potentially mediate observed associations. The study also has a number of limitations. The first is that the study was limited by the variables available in the SRTR, including clinician-reported race/ethnicity and comorbidities, which has some potential for misclassification bias. The second limitation is the inability to further subcategorize the broad racial/ethnic categories available in the national registry (ie, Hispanic/Latino, Asian, black/African American, and white) to identify unique barriers to live donor kidney transplantation in more disadvantaged racial/ethnic subgroups. The third is the inability to assess the extent to which potential racial/ethnic differences in preferences about and willingness to pursue live donor kidney transplantation among kidney transplantation candidates may be related to our observed findings. The fourth limitation is that the study was unable to account for individual-level patient income. Even though zip-code level measures are important proxies,20 the future availability of individual-level income in national registries would allow researchers to better delineate the extent to which trends in individual-level income may have contributed to changes in racial/ethnic disparities in live donor kidney transplantation. The forces arrayed against scientific medicine are many and various. They range from the honest but deluded crank with an obsession through the various cults and ’pathies to the downright quacks and medical fakers. The American Medical Association as representative of scientific medicine in this country has, naturally enough, been the target for many of the verbal poison-gas attacks made by these different interests. In general, the Association and the profession have ignored such outbursts, for in many instances one of the obvious objects of the attackers has been to obtain, through a reply, a publicity they could never get through the avenues normally open to them. In this matter, as in many others, war brings about changed conditions. Vilifying scientific medicine in times of peace was a matter that affected chiefly only the physician, and he, knowing its source, ignored it. With the entry of our country into war, medicine, in common with other sciences, was called on to do its “bit” in successfully prosecuting the gigantic task the nation had undertaken. How well it has responded we will leave others to say. The facts are that the lives and health of the hundreds of thousands of young Americans who form the new National Army have been entrusted to the care of the representatives of scientific medicine. Source: Prescott HC, Angus DC. Medical management of patients who survive sepsis. JAMA. doi:10.1001/jama.2017.17687. To the Editor Dr Maret-Ouda and colleagues assessed the outcome of laparoscopic antireflux surgery among 2655 patients who underwent the operation in Sweden between 2005 and 2014.1 They defined failure as postoperative use of antireflux medications or need for secondary antireflux surgery. The overall failure rate was 17.7%, and female sex, older age, and comorbidities were risk factors. Hospital volume of laparoscopic antireflux surgery was not associated with risk of recurrent reflux. They concluded that the high rate of recurrent reflux diminishes some of the benefits of the operation. The study has significant limitations that raise questions about the validity of the findings and the soundness of the conclusions. First, it is correct that we had no information about the preoperative workup of each individual in this large cohort of patients. However, it is mandatory to conduct a careful preoperative assessment of each patient considered for antireflux surgery in Sweden, including symptom assessment, endoscopy, and 24-hour manometry and pH measurement. Only patients with objectively verified gastroesophageal reflux disease who have not benefited from medical treatment with proton pump inhibitor are considered for antireflux surgery. Second, we agree that antireflux surgery is often conducted in individuals with particularly severe symptoms or with incomplete relief of symptoms using medical therapy. Considering this, we also agree that the rate of reflux recurrence after antireflux surgery was indeed low in our study, even lower than in most previous studies on this topic, despite the complete follow-up of an unselected cohort. The third comment concerns our assessment of recurrence of reflux after antireflux surgery. Assessing the presence or absence of gastroesophageal reflux disease is sometimes difficult, but both endoscopy and 24-hour pH measurement have low sensitivity and specificity to assess this disease. Assessment of reflux symptoms also has a limited specificity, but it has a considerably higher sensitivity. Thus, current guidelines recommend that the diagnosis of gastroesophageal reflux disease is best established using typical reflux symptoms.1 Based on this, we believe that reflux symptoms and use of antireflux medication are still the best currently available means of defining reflux recurrence after antireflux surgery. Fourth, Patti and Schlottmann suggest that surgeon volume would be a better assessment than hospital volume when evaluating the role of annual antireflux surgery volume. We did not have data on the individual operating surgeon, and hospital volume is still a reliable measure of annual surgery volume. Additionally, surgeon volume and hospital volume usually correlate quite strongly, particularly in Sweden where each department of surgery that conducts laparoscopic antireflux surgery contains very few surgeons specialized in antireflux surgery. In Reply I agree with Dr Cardenas’ statement that self-regulation is a “core attribute of the learned professions.” No doubt it is one of the principal reasons why more than 800 000 licensed US physicians have elected to be certified by a board of the American Board of Medical Specialties.1 I also agree with him and with Dr Freeman that there is a level of disaffection with the certifying boards that stems in part from concerns related to the relevancy of MOC as well as board “processes, finances, and lack of transparency.” To that end, certifying boards are making concerted efforts to improve the MOC experience and to respond to these valid concerns.2 For example, the American Board of Internal Medicine (ABIM) recently reorganized its governance structure.3 This effort was undertaken in part to enhance program relevancy by increasing the number of practicing nonacademic physicians participating at all levels of governance. More than 70% of current ABIM governance members spend more than half their time in clinical care. Over the past 3 years, ABIM staff has changed examination blueprints in all disciplines based on critical review by thousands of physicians across the country.4 Earlier this year, in response to diplomate feedback, ABIM announced plans to roll out 2-year Knowledge Check-Ins taken at home or in the office as an option to the traditional MOC examination.5 To ensure full transparency, comprehensive ABIM financial information—including tax form 990 and the audited financial statement—is freely available online, along with a reader’s guide to help interested parties find the information of interest.6 Of note, ABIM carries a platinum rating for transparency from Guidestar, a designation attained by less than 0.1% of all nonprofit organizations. I believe these actions are indicative of the good faith efforts certifying boards are pursuing to address the specific concerns outlined by Cardenas and Freeman. Freeman suggests self-regulation is already weakened due to a misalignment between medical licensure and specialty competence. He is correct that state medical boards determine an individual’s legal ability to practice “medicine and surgery,” but physicians, through their specialty boards, define and curate the standards that define familiar disciplines (eg, ophthalmology or cardiology) and are widely used to create public confidence that physicians are actually trained to do what they do. In theory, without established specialty standards, any licensed physician would be free to perform a laser procedure or administer chemotherapy or perform a heart transplant. Such was the state of affairs at the beginning of the 20th century—at the very time when physicians opted to create specialty boards to establish the standards that defined the specialties practiced today. Laws designed to weaken board certification do not advance the medical profession. To the Editor The Viewpoint by Dr Moreno and colleagues1 understated the precedential value of the International Medical Tribunal's decision in the trial of Nazi doctors accused of war crimes that established the Nuremberg Code and the code’s influence on common law development of the legal duty of researchers to secure informed consent from their research participants. The refusal in 1987 of the US Supreme Court, in the case of an army sergeant who had secretly been dosed 4 times and claimed to have been injured in an lysergic acid diethylamide (LSD) experiment, to adopt or apply the code is noninformative because the majority on the Court held that soldiers may not sue the government or military leadership for monetary damages for injuries sustained while serving. My review of case law identified 19 published opinions from state and federal courts (applying federal law as well as the laws of Arizona, Florida, California, Illinois, Maryland, Massachusetts, Michigan, New York, and Pennsylvania) that recognize the duty of researchers to secure an informed consent from research participants. Four of those courts favorably recited the International Medical Tribunal decision or the code among other sources for establishing the duty.2-5 Other sources cited include the Declaration of Helsinki, other professional codes of ethics, federal regulations, the special nature of the participant-researcher relationship, and perhaps most importantly, concerns about nonconsensual invasions of bodily integrity. A systematic analysis of the code’s impact on US case law would be valuable. However, the evidence Merz provides does not undermine our argument. On the contrary, a close reading of the evidence seems to strengthen it. Although 4 of the 19 published opinions he reviewed from state and federal courts refer specifically to the code, they do so not to establish the “legal duty” of researchers to secure informed consent, but rather as an important “guideline” and “law of humanity” in the cases. The National Health Interview Survey (NHIS)5 is a nationally representative annual health survey in the United States. The NHIS was approved by the research ethics review board of the National Center for Health Statistics and US Office of Management and Budget. All respondents provided oral consent prior to participation. The University of Iowa institutional review board determined that the current study was exempt based on the use of deidentified data. The NHIS collects data on a broad range of health topics through in-person household interviews. For each interviewed family in the household, 1 sample child, if any, was randomly selected by a computer program. Information about the sample child was collected by interviewing an adult, usually a parent, who was knowledgeable about the child’s health. In NHIS 2014-2016, the total household response rate ranged from 67.9% to 73.8%, and the conditional response rate for the sample child component ranged from 91.2% to 92.3%. From 2014 to 2016, respondents were asked: “Has a doctor or health professional ever told you that [the sample child] had autism, Asperger’s disorder, pervasive developmental disorder, or autism spectrum disorder?” Responses for children and adolescents aged 3 to 17 years were included. Prevalence estimates were weighted using survey procedures in SAS (SAS Institute), version 9.4. The sample weights took into account unequal probabilities of selection and nonresponse. P values for overall differences across strata were calculated using the F test. Trends in prevalence were tested using a logistic regression model with sample weights, which included survey year as a continuous variable, and adjusted for age, sex, and race/ethnicity. A 2-sided P value less than .05 was considered statistically significant. Of all eligible participants aged 3 to 17 years in the NHIS 2014-2016, 28 (0.09%) had missing information on ASD diagnosis and were excluded. Among the included 30 502 US children and adolescents, 711 were reported to have been diagnosed as having ASD. The weighted prevalence of ASD was 2.47% (95% CI, 2.20%-2.73%). The prevalence was 3.63% (95% CI, 3.19%-4.08%) in boys and 1.25% (95% CI, 0.99%-1.51%) in girls; 1.82% (95% CI, 1.42%-2.22%) in Hispanic children and adolescents, 2.76% (95% CI, 2.39%-3.13%) in non-Hispanic white children and adolescents, and 2.49% (95% CI, 1.69%-3.29%) in non-Hispanic black children and adolescents (Table). Across the 3-year reporting period, the prevalence was 2.24% (95% CI, 1.89%-2.59%) in 2014, 2.41% (95% CI, 1.98%-2.84%) in 2015, and 2.76% (95% CI, 2.20%-3.31%) in 2016 (P for trend = .11) (Table). In a large, nationwide population-based study, the estimated ASD prevalence was 2.47% among US children and adolescents in 2014-2016, with no statistically significant increase over the 3 years. The observed prevalence was higher than estimates in previous years from the ADDM,2 although differences in study design and participant characteristics may partly explain the prevalence differences. For example, the NHIS was based on a nationally representative population, whereas the ADDM was conducted in selected sites. The NHIS was based on parent report of a physician diagnosis, whereas the ADDM was based on clinician review of education or health care evaluations. In the NHIS, the question about ASD changed in 2014,3 so the NHIS cannot be used to evaluate trends in ASD prevalence over a longer time. Another limitation is the ascertainment of ASD by the household respondents’ self-reports of physician diagnosis. The recent challenges to the Affordable Care Act (ACA), which has increased the number of individuals with health insurance in the United States but has had little effect on cost, has revived the debate about a single-payer health care system.1 Whether a single-payer system is the answer or not depends on what question is being asked and what form single payer will take. Single payer can take many forms, and many questions can be asked. This Viewpoint considers 3 problems of US health care: the uninsured, poor health outcomes (relative to other high-income countries), and high cost. In discussing cost, it will be critical to consider the form that a single-payer health care system might take. Regarding the uninsured, single payer in almost any form could achieve universal health care insurance coverage. But so could many less-comprehensive reforms. Despite the success of the ACA in increasing the number of individuals in the US who have health insurance, approximately 25 million US residents remain uninsured. Universal coverage requires (1) subsidies for individuals who are too poor or too sick to acquire insurance at actuarial correct premiums, and (2) compulsion (ie, a mandate) for everyone else to participate and implicitly contribute to the subsidies. No country achieves universal coverage without subsidies and compulsion. The United States could achieve universal coverage relatively promptly if it were willing to adopt these 2 principles. Public attitudes toward subsidies and compulsion have been assessed by analyzing answers to a Pew Research Center survey of the US population, which included almost 2000 adults.2 A modest majority favored subsidies only, and a modest majority favored compulsion only. There was not, however, a majority who favored subsidies and compulsion. A single-payer system would undoubtedly lower administrative expenses. US health care currently has a fragmented financing system that relies on employment-based health insurance, individual insurance, payroll taxes, income taxes, business taxes, state and local taxes, payments by patients, and the federal deficit. Costly programs are needed to raise revenue from those many sources. In addition, hospitals, physicians, and other entities and individuals that provide health care must employ armies of “back office” personnel to bill and collect for that care. In addition, a single-payer system would have the bargaining power needed to offset the monopoly power of drug and device manufacturers and hospitals and physicians. At present, prescription drug prices in the United States are double the prices in other Organisation for Economic Cooperation and Development (OECD) countries. The expensive artificial devices required for every hip and knee replacement carry a US price tag more than 3 times that of other countries. In addition, according to a British specialist in joint replacement, the fees of US physician specialists are double or triple those of their peers in other countries (Luke Jones, DPhil [Oxon], FRCS, oral communication, October 15, 2017). Recent mergers and acquisitions by hospitals and integration with physician groups often increase efficiency, but they also increase the monopoly power of the organizations that provide care. Single payer could offset this imbalance. Excessive expenditures for administration of the US health care system and monopoly prices for material and personnel inputs to that system account for a substantial portion of the higher cost of health care in the United States.6 More than half the difference between 18% of GDP and 12% of GDP, however, is attributable to a more expensive mix of services in the United States. The analogy is not perfect, but the biggest difference between health care in the United States and other high-income countries is similar to that between food expenditures at Whole Foods and at Wal-Mart. In the United States, medical care takes the form of greater use of specialists and subspecialists, greater use of technology such as magnetic resonance imaging (MRI) scans and mammograms, and a more expensive mix of drugs (Whole Foods). US medical care does not take the form of more basic care (Wal-Mart), such as visits to physician or days in acute care hospitals, which are frequently greater in peer countries. It is questionable whether this more expensive mix produces better health outcomes. It is not difficult to find examples in the United States, such as having more than double the number of MRI scans as in the average OECD country, but with small marginal benefits. It is difficult to find important examples in Canada, France, Germany, or Scandinavia where the more economical mix results in worse health outcomes. Harvey Fineberg, former President of the National Academy of Medicine, has warned, “[W]e cannot attain superior health results by continuing to outspend others on medical care.”4. The fragmented financing system is one of the principal explanations for the high cost of medical care in the United States. A careful consolidation of financing into some form of single-payer system is probably the only feasible solution. But single payer is easier said than done. To devise and operate 1 system for the US population (325 million) poses an enormous policy and managerial challenge. Canada, with only one-tenth the US population, found it desirable to have provincial health insurance plans rather than a national one. In the United States, that would mean 50 separate state health insurance plans. Some states might rise to this challenge; many would have difficulty; and some states would encounter major difficulties. For instance, for some states, an annual health insurance budget in the tens of billions of dollars may offer a target for lobbying, favoritism, bribery, and corruption that would probably be too difficult to resist. To have any chance of success in the United States, single payer would have to be simple, require a minimum of bureaucracy, be based on decentralized organizations to deliver care, and provide opportunity for individuals to choose among competing health plans. Choice is critically important for patient satisfaction and its role in competition. The provision of implicit subsidies to the poor and sick should not require any bureaucratic determination of income or health status. Universal insurance, paid for by a broad general tax that everyone pays in proportion to consumption of all goods and services, would be a progressive way to accomplish this.7. The insurance could allow each individual (or family) to choose membership in a local health plan that takes responsibility for the individual’s health and is reimbursed on a risk-adjusted per-capita payment. This system could provide the plans with incentives for efficiency and effectiveness and leave them free to organize production as they deem best. Plans could compete with each other for members on the basis of service and quality of care. Choice of plan could be open annually. Individuals who wanted to purchase more coverage than provided for in the universal plan would be free to do so with their own after-tax dollars. Congress would have control over the total health care bill by its setting of the tax rate. Some opposition to single payer reflects concern about its effects on health care innovation. Less innovation is not always harmful because elimination of very expensive interventions that have small incremental benefits would free resources for use with greater social value. A free society should allow individuals to acquire more than the universal plan with their own after-tax dollars; the key question concerns care that is collectively paid for by single payer or some other third-party insurance coverage. Rational policy would want to limit such care to that which is reasonably cost-effective. In conclusion, a single-payer system could easily provide for universal coverage, but so could less-comprehensive reforms, if the public would support subsidies and compulsion. Single payer might improve health outcomes by providing more equal access to medical care, but attention to the social determinants of health might be a more effective way to improve health. The strongest case for single payer is its potential to control the cost of care. The current fragmented system of financing care precludes such control. Perhaps because of Canada’s adjacency and close relationship with the United States, many policy makers and health care advocates in both nations still seem preoccupied with each other’s systems. Canadians of all political persuasions affirm the superiority of their health care systems by drawing comparisons with the costs and inequities of the US health care system. US opponents of single-payer reforms often demonize Canadian health care, even as US proponents extol their northern neighbor’s approach. This Viewpoint argues that the constant overemphasis on Canada is unhelpful to advancing the cause of universal and equitable access to health care for US citizens. Canada’s foundational health care legislation was also enacted in a different and simpler time. It is no accident that the 2 cornerstone Canadian laws cover 14 pages, contrasted with the more than 900 pages of the Affordable Care Act. When universal hospital insurance was enacted in 1957, all Canadian hospitals were nonprofit or public entities. Insurance cover was uneven, charity care was commonplace, and the federal legislation was supported unanimously across party lines. By 1965, as the federal government considered a national medical plan, matters were slightly more complex. About 120 private insurers covered 25% of the Canadian population, while plans sponsored or formally approved by organized medicine insured another 29%. Organized medicine and the insurance industry lobbied for extending coverage to all through means tests and subsidies. However, with 45% of the population uninsured or covered on limited “medical welfare” plans, and the physicians’ own nonprofit plans providing an obvious template, the logic of a single-payer system for medical services was compelling. There were only 2 nays out of 179 votes cast in the final parliamentary vote.3,4. The Canada of 5 or 6 decades ago bears limited resemblance to the United States today. On the one hand, financial barriers persist for tens of millions of US citizens with limited coverage, and polls of the general public and physicians are moving in favor of greater government intervention in health care financing. On the other hand, the absence of a political consensus on these issues is obvious; and a large number of powerful entities have a major financial stake in the status quo. As Reinhardt often cautioned, these heavily entrenched groups can exert a powerful influence on US lawmakers. Canadian Medicare remains admirable in many ways. The skills of those who work in Canadian health care are world class. Although seldom scaled, pockets of remarkable health care innovation can be found across the nation. Canada also boasts long-term success in cost containment, modest administrative expenses, good health care outcomes, and mitigation of the socioeconomic gradients in access that existed before universal coverage of medical and hospital services. These are important strengths. However, they are shared by health care systems in other Organisation for Economic Co-operation and Development (OECD) nations that have cost profiles similar to Canada, but have overall performance that is as good or better. The well-known 2014 and 2017 rankings by the Commonwealth Fund, for example, placed Canada 10th and then 9th out of 11 peer nations, slightly north of the United States, which has consistently come last. Those rankings reflect structural weaknesses in organization and finance that bedevil all the subnational Canadian plans, as reviewed extensively in a recent federal report.5 Policy analysts have repeatedly cited limited integration as the key flaw in systems that have a payment architecture largely mired in the 1970s. However, integration is limited by the absence of intermediaries, such as US integrated delivery systems or accountable care organizations. Medical associations, in particular, have representation rights to bargain directly with provincial governments, and, despite arguments that alignment of incentives would be in mutual interest, resist consolidation of medical budgets with other health care services. So difficult has it been to modernize Canadian Medicare that a 2013 scholarly monograph focused solely on the causes of the “Paradigm Freeze” in health care policy.6. More importantly, given the near-chaotic pluralism of US health care, obsessive reference to Canada as exemplar conflates ends and means. Universal coverage has been achieved for different baskets of services in OECD countries with or without the use of multiple insurance carriers or intermediary administrative entities. Arguably, no country actually has a single-payer health care system for all services. Instead, there are varying mixes of public and private funding for specific services. Canada’s public share (70%) is on the low end of the spectrum owing to the narrow scope of Canadian Medicare. Federal cost-sharing constrains provinces only to cover all necessary medical services and hospitalizations. Various other services are covered at provincial discretion, but the core coverage under US Medicaid is more generous than that available in any of Canada’s 13 health care systems. Canada’s current challenges in extending universal coverage to prescription drugs are illustrative of a broader dilemma facing US reformers. In Canada, spending on drugs now exceeds the costs of physician services, and is shared almost equally across provincial plans, private insurance, and out-of-pocket sources. Very credible analysts7,8 have argued that Canada could save billions through more effective procurement under a national pharmacare plan. But that rational step requires a shift from private to public financing—a difficult option when the federal government and several provinces face operating deficits and sizable debt burdens. There will be complex redistribution of gains and losses—not least between employees and employers, and intense federal-provincial jockeying as to who pays what share. Universal pharmacare should have been adopted in the 1960s or 1970s. Now, as with US health care, universal coverage is most likely to occur by a heavily regulated multipayer system with an overlay of consolidated procurement. Health care reform discourse in Canada and the United States would be vastly improved if more commentators, advocates, and lawmakers could overcome the cross-border obsessions and misperceptions that distort contemporary debates. Canada does offer important lessons for reform in the United States, not least in its relentless commitment to equitable access for some key services, its administrative efficiency, and its success in cost-containment. However, Canada’s health care arrangements are rooted in different values, facilitated by a different model of democratic governance, and reflect a different era, both in the conditions that fostered their creation and in an outmoded architecture that makes them a dubious exemplar for the United States. There is arguably much more for the United States to learn from the panoply of long-standing national experiments with universal coverage that can be found across the OECD. Reinhardt, for one, suggested that Germany, the Netherlands, and Switzerland merited particularly close examination.1. The nationwide implementation of electronic medical records (EMRs) resulted in many unanticipated consequences, even as these systems enabled most of a patient’s data to be gathered in one place and made those data readily accessible to clinicians caring for that patient. The redundancy of the notes, the burden of alerts, and the overflowing inbox has led to the “4000 keystroke a day” problem1 and has contributed to, and perhaps even accelerated, physician reports of symptoms of burnout. Even though the EMR may serve as an efficient administrative business and billing tool, and even as a powerful research warehouse for clinical data, most EMRs serve their front-line users quite poorly. The unanticipated consequences include the loss of important social rituals (between physicians and between physicians and nurses and other health care workers) around the chart rack and in the radiology suite, where all specialties converged to discuss patients. Similar concerns around artificial intelligence predictive models in health care have been discussed: clearly, in the 3-step process of selecting a dataset, creating an appropriate predictive model, and evaluating and refining the model, there is nothing more critical than the data. Bad data (such as from the EMR) can be amplified into worse models. For example, a model might classify patients with a history of asthma who present with pneumonia as having a lower risk of mortality than those with pneumonia alone,3 not registering the context that this is an artifact of clinicians admitting and treating such patients earlier and more aggressively. Since machine learning presents no human interface and cannot be interrogated, even if its predictions are extraordinarily accurate, some clinicians are likely to view the “black box” with suspicion. The missing piece in the dialectic around artificial intelligence and machine learning in health care is understanding the key step of separating prediction from action and recommendation. Such separation of prediction from action and recommendation requires a change in how clinicians think about using models developed using machine learning. In 2001, the statistician Breiman4 suggested the need to move away from the culture of assuming that models that are not causal and cannot explain the underlying process are useless. Instead, clinicians should seek a partnership in which the machine predicts (at a demonstrably higher accuracy), and the human explains and decides on action. The same sentiment was expressed by Califf and Rosati as early as 1981 in an editorial on predictive risk factors emerging from a computer database on exercise testing for coronary artery disease: “Proper interpretation and use of computerized data will depend as much on wise doctors as any other source of data in the past.”5. The 2 cultures—computer and the physician—must work together. For example, clinicians are biased toward optimistic prediction, often overestimating life expectancy by a factor of 5, while predictive models trained from vast amounts of data do better; using these well-calibrated probability estimates of an outcome, clinicians can then can act appropriately for patients at the highest risk.6 The lead time a predictive model can offer to allow for an alternative action matters a great deal. Well-calibrated levels of risk for each outcome, and the timely execution of an alternative action, are needed for a model to be useful. In short, a black-box model can lead physicians to good decisions but only if they keep human intelligence in the loop, bringing in the societal, clinical, and personal context. Additionally, the unique human brain and clinical training can generate new ideas, see new applications and uses of artificial intelligence and machine learning, and connect these technologies to the humanities and the social sciences in ways that current computers do not. To generate better evidence for pediatric populations, Congress provided an economic incentive. The pediatric exclusivity program, created in 1997 and renewed by the Best Pharmaceuticals for Children Act of 2002, authorizes the FDA to grant 6 months of market exclusivity in exchange for conducting pediatric studies requested by the FDA. The Pediatric Research Equity Act of 2003 allows the FDA to mandate pediatric studies for the same adult indication of newly approved drugs. The Best Pharmaceuticals for Children Act allows the FDA to request a broader range of pediatric studies for any new or already-marketed drug if it determines that information relating to pediatric use “may produce health benefits.”4. The FDA approved cinacalcet in 2004 for the treatment of secondary hyperparathyroidism in adult patients with chronic kidney disease. In 2007, Amgen asked the FDA to issue a Written Request to study cinacalcet for treatment of secondary hyperparathyroidism in pediatric patients with end-stage renal disease. The FDA and Amgen negotiated until agreeing on a Written Request encompassing 4 studies in 2010. Amgen completed 3 of the 4 studies, but for the other, study 3, the company failed to study the minimum number of patients for the minimum duration. The Written Request specified that study 3, an open-label safety study for younger pediatric patients, enroll 15 patients for 26 weeks, or for 12 weeks prior to a kidney transplant. Amgen struggled with recruitment, partly because of an FDA-issued clinical hold following the death of a participant in 2012, which suspended study 3 for 14 months. By 2015, Amgen had enrolled only 4 patients who completed study criteria. Amgen then asked the FDA to amend the Written Request, but the FDA refused. The agency had already approved several amendments between 2010 and 2015 to accommodate difficulties with recruitment in the other studies and was doubtful that decreasing the size of study 3 would accomplish the Written Request’s objective to evaluate the safety of cinacalcet for the relevant pediatric population. Moreover, Amgen’s interpretation of “fairly respond,” if adopted by courts, has implications for the FDA’s authority. Through the Written Request, the FDA specifies various trial parameters, which are not equally weighted but together can yield scientifically useful information. The agency then evaluates the submitted evidence to decide whether the studies meet the objectives of the Written Request. This is what Congress intended: “fairly” is a qualitative, not quantitative, concept for study design. The FDA admits that companies can obtain pediatric exclusivity even if studies do not meet every term, but the FDA uses its scientific expertise to distinguish inadequately designed studies from well-designed studies capable of generating meaningful evidence. Courts should give deference to the FDA’s current policy, which links “fairly respond” to the studies’ scientific value and allows the FDA to effectuate the purpose of the pediatric exclusivity program. Congress created pediatric exclusivity to address the urgent need for studies that can guide clinical decision making for pediatric patients. There is strong public interest in ensuring that, in exchange for the commercial advantage conferred by pediatric exclusivity, companies conduct studies that are likely to provide robust clinical evidence. This requires preserving the FDA’s scientific judgment and regulatory authority. “He was a great kid, my only son,” Jack says with a soft and shaky voice, as I pull out the stack of papers. On top, I see a newspaper obituary from two years ago with a photo of a smiling young man, David. I read the first line, “age 32, dies peacefully after an extremely brave battle with cancer.…” Jack goes on to tell me about David’s tragically short life, his many accomplishments, their favorite things to do as father and son and as a family. His eyes brighten as he shares these memories with me, but then begin to sink as he recounts the sudden weight loss, blood tests, x-rays, biopsies, and consultations. His gaze drifts toward the floor when he describes the chemotherapy, the hospitalizations, the pain—and then his voice trails off. When he looks up, I see his tears. I tell him how sorry I am for his loss. I think of my own wife and son. We say nothing for a long moment. Then Jack tells me that he has not been to a doctor in years. David’s death was his last encounter with the health care system. He says he needs to tell this story to his doctor before he can discuss his own health. Jack worries that this goes against tradition and will not be accepted. I imagine his grief receding just enough to free a fraction of self-preservation instinct and prompt him to make this appointment, to take this risk. I tell Jack I am glad he shared his story with me. Our time has run out, and I have barely scratched the surface of his medical assessment. I look at my template—the history of present illness and review of systems are blank. In fact, the only history recorded is what my medical assistant managed to document before I came into the room. I was not trained to provide clinical care this way, but the power of Jack’s story takes hold, and somehow it feels right. I am beginning to know Jack the person. He cannot put himself before his son. His medical needs will become clear in time. We make plans to follow up in 2 weeks. I say that I “take” the history, but is this really how it works? Jack reminds me that, in the best circumstances, I “receive” the history. I position myself best to earn this intimate and privileged information by listening, acknowledging, empathizing, noticing my patient’s expressions and gestures, and building a trusting relationship. Without this effort, I still get a history, but it is not the most meaningful, rich, and rewarding one that has the essential clues that I need to make diagnoses and truly understand and know my patient. Perhaps the revised adage should be, “Listen to your patient, be sure he senses your concern, curiosity, and interest, and then he will tell you what the diagnosis is.”. In this issue of JAMA, Purnell and colleagues1 reviewed national trends from 1995 to 2014 in the rates of live donor kidney transplantation according to racial/ethnic differences. The authors observed decreases in the cumulative incidence of live donor kidney transplantation for black and Hispanic patients, resulting in increasing disparities compared with white patients. These disparities are a topic of great importance. To accurately target solutions for the persistent disparities involving racial/ethnic minorities, it is incumbent to fully understand the multifaceted underlying issues related to live donor kidney transplantation. The study by Purnell and colleagues1 included 453 162 adult kidney transplantation candidates (mean age, 50.9 years; 39% were women; 48% were white; 30%, black; 16%, Hispanic; and 6%, Asian) of whom 59 516 (13.1%) ultimately received a live donor kidney transplantation. The authors found that the proportional incidence of live donor kidney transplantation among black and Hispanic patients was lower when comparing the rates in 2010-2014 vs 1995-1999. However, live donor kidney transplantation has not declined for these groups in terms of absolute numbers compared with the initial period studied. The greatest period of growth occurred from 1995 to 2009 when the volume of live donor kidney transplantation more than doubled for every racial/ethnic group. The largest increases in live donor kidney transplantation were among Hispanic and Asian recipients, which increased by more than 2.6 times during this period according to the crude numbers of live donor kidney transplantations provided in Table 3 in the article.1 For example, the number of live donor kidney transplantation increased from 999 among Hispanic patients in 1995-1999 to 2628 in 2005-2009. Changes in cumulative incidence of live donor kidney transplantation as reported by the authors must be considered within the context of the overall increase in the denominator that represents patients on the waiting list. This could be taken as some measure of success, suggesting that more black and Hispanic patients have achieved access to transplantation centers and the transplantation waiting list. Optimistically, greater numbers of patients on the waiting list should directly translate into greater numbers of live donor kidney transplantations. It is possible, however, that the increase in the numbers of patients on the waiting list represents greater access to the waiting list for black or Hispanic patients of lower socioeconomic status who are typically less likely to identify a suitable living donor. Substantial evidence has documented lower rates of knowledge regarding transplantation, referral, and access to the waiting list and, ultimately, lower rates of transplantation for both black and Hispanic patients. Similarly, an association between socioeconomic status and referral and access to transplantation has been repeatedly identified. The response to concerns about disparities in access have included local and national initiatives, including the Medicare Improvements for Patients and Provider Act in 2008, which aimed to improve transplantation-related education within dialysis centers and referrals to transplantation centers by establishing links to reimbursement. Differences in socioeconomic status are important to consider because it also has been repeatedly documented that patients with low socioeconomic status, regardless of race/ethnicity, historically have been less likely to identify a suitable living donor. Since 2014, the numbers of deceased donors nationally have increased (a trend unseen in the preceding decade) and correspond with the ongoing opioid epidemic. Based on United Network for Organ Sharing data as of November 24, 2017, the numbers of deceased donor kidney transplantation increased by 16% from 2014 to 2016. In these 2 years, the numbers of deceased donor kidney transplantation increased by 5% for whites, 23% for blacks, and 32% for Hispanics. In July 2017, the United Network for Organ Sharing announced a major “milestone” in the reduction of access disparities noting that the number of black and Hispanic patients undergoing deceased donor kidney transplantation was proportional to their representation on the waiting list; this improvement was credited to changes under the kidney allocation scheme adopted in 2014.2 The kidney allocation scheme provided waiting list time points according to the amount of time receiving dialysis prior to appearing on the waiting list, opportunities for allocation of A2 kidneys to B recipients (a larger proportion of black patients), and prioritization of highly sensitized patients (a greater proportion of black and Hispanic patients).2 However, these encouraging details of progress still mask the disparities in access to care and transplantation according to race/ethnicity. Despite improvements in the numbers of patients on the waiting list, inequalities remain with delays in referral and access. According to an analysis of the US Renal Data System,3 black patients had a 29% decreased likelihood of being on the waiting list after year 1 of receiving dialysis compared with white patients (after adjusting for age, sex, comorbidities, and socioeconomic factors) and continued to have significantly lower likelihoods of remaining on the waiting list until year 3. Downstream associations include a 71% decreased likelihood of transplantation after year 1 and a persistent 39% decreased likelihood of transplantation after year 4 for black patients. For Hispanic patients, there were no significant decreases in time on the waiting list, but they still had a 21% decreased likelihood of transplantation after 4 years. Numerous studies have documented an association between socioeconomic disparities in racial/ethnic groups such as education level and health insurance coverage in the timing and quality of medical care prior to the development of end-stage kidney disease. Delays in receipt of nephrology care have been associated with differences in knowledge about live donor kidney transplantation. Earlier work by Purnell et al4 indicated age- and sex-adjusted rates of live donor kidney transplantation were 80% lower for black patients and 60% lower for Hispanic patients compared with white patients. In this analysis, quality of health care accounted for the largest proportion of the disparity in live donor kidney transplantation rates for black patients. Specifically, predialysis nephrology care and dialysis quality metrics, including rates of moderate to severe anemia and prescription of an erythropoiesis-stimulating agent, were significant predictors of live donor kidney transplantation. Among Hispanic patients, health insurance coverage explained the greatest proportion of disparity. Health insurance type (Medicare or Medicaid vs private) has been established as a predictor of live donor kidney transplantation disparities and also has been shown to have a greater influence on live donor kidney transplantation rates among black compared with white transplantation recipients.5. Another potential limitation affecting access to live donor kidney transplantation for black and Hispanic patients involves higher rates of obesity, diabetes, hypertension, and the metabolic syndrome affecting the medical suitability of potential donors. In the analysis by Purnell et al,1 diabetes and hypertension were more prevalent as the origin of end-stage kidney disease in black and Hispanic patients. Black patients with a first-degree relative with end-stage kidney disease have a 9-fold increase in the odds of developing end-stage kidney disease.6 In addition to potential genetic factors, lower socioeconomic status seems to be associated with risk factor clustering in these communities, limiting identification of potentially suitable live donors. Lower socioeconomic status has been associated with increased incidence of chronic kidney disease, increased progression to end-stage kidney disease, inadequate dialysis treatment, decreased access to transplantation, and worse survival through its effects on dietary factors, access to care, education, health literacy, and rates of obesity.7 The incidence of end-stage kidney disease secondary to diabetes is increased in zip codes with limited access to primary health care.8 Worsening the association of this factor with live donor kidney transplantation rates, black patients are less likely to undergo an unrelated live donor kidney transplantation.9 Recent trends suggest this situation will only become more perilous given noted declines in live donor kidney transplantation are most pronounced among related donors and the increasing prevalence of diabetes and obesity in recent years.10. Perhaps the most obvious barrier to improvements in rates of live donor kidney transplantation relates to the financial disincentives faced by kidney donors. As noted previously, the annual volume of live donor kidney transplantation in the United States underwent a period of sustained growth starting in the mid-1990s. Then beginning in 2007, live donor kidney transplant numbers began to decline coinciding with the US financial and housing market crisis. From 2004 to 2011, live donor kidney transplantation declined 13% compared with 120% growth during the preceding decade.10 The largest declines were among male donors and adult donors younger than 50 years. One proposed explanation relates to concerns about time off from work. Black patients were affected at a higher rate.10 Potential donors with low and moderate incomes and greater concerns about employment security appear to be more influenced by global economic pressures. Recent evidence from the Kidney Donor Outcomes Cohort study11 showed that 20% of donors had more than $5000 in out-of-pocket costs and 53% had more than $1000 in out-of-pocket costs when accounting for direct costs such as travel and health care and indirect costs such as lost wages (excluding the value of paid leave benefits) and subtracting any financial support received. This analysis demonstrates the substantial gap between current realities encountered by kidney donors and the goals related to financial neutrality. Quiz Ref IDThe main eligibility criteria for centers for primary health care were an affiliation with the Remediar+Redes Program, location in a poor urban area, and employment of community health workers in addition to general practitioners and nurses. A total of 204 centers from Buenos Aires, Misiones, Tucuman, Corrientes, and Entre Ríos were screened for potential participation (study protocol is available in Supplement 1). Among centers that met the eligibility criteria, 18 were selected based on recommendations from the Remediar+Redes Program (Figure 1). Cluster randomization was stratified by geographic region and conducted at the data coordinating center at Tulane University. The randomization schedules were generated using PROC PLAN in SAS software (SAS Institute Inc). The main eligibility criteria for index participants were uncontrolled BP (systolic ≥140 mm Hg, diastolic ≥90 mm Hg, or both, measured on at least 2 separate screening visits), 21 years or older, uninsured and receiving primary care from the participating centers, spouses with or without hypertension, or adult family members (≥21 years) with hypertension living in the same household who were willing to participate in the study. The study nurses reviewed the clinic appointment schedules daily and identified all patients with hypertension. Two screening visits at least 1 day apart were conducted to assess patients’ eligibility. Eligible index participants, their spouses with or without hypertension, and adult family members with hypertension were recruited for the study between June 2013 and April 2015. To avoid selection bias, participants remained eligible for the study if they received antihypertensive treatment after the screening visits and their BP was less than 140/90 mm Hg at the baseline examination. The community health workers were trained to coach patients and their family members on lifestyle modification, home BP-monitoring, and medication adherence during a 2-day interactive training session followed by onsite field testing and certification. They were also trained to function as case managers for the patients and their families by coordinating intervention activities and facilitating patient care. They visited participants’ homes monthly for the first 6 months and every other month thereafter. The family-based intervention started with an initial 90-minute home visit at a time when all family members in the household were available to discuss general knowledge about hypertension prevention and treatment. During subsequent 60-minute monthly or bimonthly follow-up visits, the community health workers provided tailored counseling to participants and their families on lifestyle modification, home BP monitoring, and medication adherence skills. They reviewed specific strategies for lifestyle modification—such as weight loss, dietary sodium reduction, physical activity increase, alcohol moderation, and the DASH (Dietary Approaches to Stop Hypertension) diet use—with patients and their families. Patients were encouraged to adopt lifestyle modification strategies that were the most suitable for their individual needs. All patients with hypertension in the intervention group were given an automatic home BP monitor and log and were trained to record their BP weekly. Additionally, they were provided 7-day pill organizers and counseled on techniques for improving medication adherence. Home visits also focused on goal setting, problem solving, social support, and maintaining motivation during challenging situations. Community health workers helped patients schedule appointments with primary care physicians and delivered antihypertensive medications to patients’ homes if they did not have access to transportation. Trained and certified research nurses who did not engage in the intervention collected all study data at baseline and at 6, 12, and 18 months of follow-up in participants’ homes using standard questionnaires and measurement methods. Two visits between 1 and 14 days apart at baseline and at 18 months were conducted to obtain repeated BP measurements. Three BP measurements were obtained at each data collection visit, and the mean of all measurements at each time point was used for analysis. Blood pressure was measured according to a standard protocol recommended by the American Heart Association15 and was measured with participants in a seated position after 5 minutes of quiet rest. In addition, participants were required to avoid alcohol, cigarettes, coffee, tea, and exercise for at least 30 minutes before their BP measurement. An auto-BP cuff (Intellisense Digital BP Monitor; Omron HEM 907 XL, Omron Healthcare) was used, and 1 of 4 cuff sizes (pediatric, regular adult, large, or thigh) was chosen based on each participant’s arm circumference. Patient adherence to antihypertensive medication was quantified using the 8-item Morisky Medication Adherence Scale.16 Intensification of antihypertensive medications, including titration or addition of a new medication, was assessed by questionnaire and medical records. Intensification was used as an indicator of primary care physician adherence to the intervention program and related clinical guidelines. Adverse events, such as hypotension, syncope, and injurious falls, were queried during the nurse visits. The intention-to-treat principle was used for all analyses. Only patients with hypertension were included in the primary analysis, according to the study protocol, because we aimed to test the effect of the community health worker–led multicomponent intervention on BP control among patients with hypertension.9 A mixed-effects regression analysis, in which participants were nested in families, which were nested in centers, which were further nested in randomization groups, was used to estimate difference in the changes of BP from baseline to 6, 12, and 18 months, separately. In addition, the mean difference in the changes of BP during the intervention period were estimated. In these models, participants, families, and centers were assumed to be random effects, and the intervention was assumed to be a fixed effect. An autoregressive correlation structure was selected for these repeated measures. In addition, generalized estimating equations were used to compare baseline variables and the proportions of binary outcomes at 6, 12, and 18 months. Cluster effects were accounted for by assuming a compound-symmetry covariance structure, and standard errors were estimated using a robust variance estimator. In secondary analyses, important covariables were adjusted, and predefined subgroup analyses were conducted. In these analyses, pairwise deletion of missing data was used to preserve all information observed. Additionally, multiple imputation for missing data in the multivariable analyses was conducted using the Markov chain Monte Carlo method. PROC GLIMMIX and PROC GENMOD of SAS version 9.4 to obtain point estimates and standard errors and to test for differences between randomization groups. A 2-sided P value of <.01 was considered statistically significant because 5 main study outcomes were compared (additional explanation of the cost analysis is available in Supplement 2). The mean age of patients with hypertension was 55.8 years and 53.0% were women. In general, baseline characteristics of patients with hypertension were balanced between intervention and control groups (Table 1). However, the intervention group had a slightly higher proportion of individuals with self-reported major cardiovascular disease and hypercholesterolemia, as well as higher levels of mean systolic (151.7 vs 149.8 mm Hg) and diastolic (92.2 vs 90.1 mm Hg) BP than did the control group. Likewise, baseline characteristics of normotensive participants were balanced between intervention and control groups except for physical activity and diastolic BP, which were slightly lower in the intervention group than in the control group (eTable 1 in Supplement 2). During the 18-month intervention, community health workers completed 92.8% (8272 of 8916) of planned home-based interventions, and patients completed 84.2% (26342 of 31287) of anticipated home BP measurements. In addition, the eHealth platform sent out 91.2% of scheduled text-messages and 76.3% of participants reported receiving messages weekly. The proportion of high adherence to antihypertensive medication (Morisky score, 8) increased from 31.3% at baseline to 66.1% at 18 months in the intervention group and from 38.0% to 53.0% in the control group (Table 2). The difference in the proportion of high adherence to antihypertensive medication was 13.1% (95% CI, 7.0%-19.2%; P < .001) at 18 months. Proportions of medication intensification from baseline to 18 months were 57.6% in the intervention group and 42.8% in the control group (P < .001). The mean systolic BP was significantly reduced from 151.7 mm Hg (95% CI, 150.5-152.9 mm Hg) at baseline to 132.4 mm Hg (95% CI, 131.2-133.5 mm Hg) at 18 months in the intervention group and from 149.8 mm Hg (95% CI, 148.7-151.0 mm Hg) to 137.7 mm Hg (95% CI, 136.4-139.0 mm Hg) in the control group (Figure 2; eTable 2 in Supplement 2). Diastolic BP was reduced from 92.2 mm Hg (95% CI, 91.3-93.0 mm Hg) to 79.5 mm Hg (95% CI, 78.7-80.2 mm Hg) in the intervention group and from 90.1 mm Hg (95% CI, 89.1-91.1) to 83.7 mm Hg (95% CI, 82.9-84.6 mm Hg) in the control group. Difference in the reduction in systolic BP was 6.6 mm Hg (95% CI, 4.6-8.6 mm Hg; P < .001) and in diastolic BP was 5.4 mm Hg (95% CI, 4.0-6.8 mm Hg; P < .001) (Table 3). The intraclass correlation coefficients were 0.0768 and 0.0713 for changes in systolic and diastolic BP, respectively. Net reductions in systolic and diastolic BP were consistent by age, sex, cardiovascular risk (history of major cardiovascular disease, hypercholesterolemia, and diabetes), body mass index, and number of hypertensive family members in predefined subgroup analyses (Figure 3). The proportion of controlled hypertension increased from 17.0% at baseline to 72.9% at 18 months in the intervention group, and from 17.6% at baseline to 52.2% at 18 months in the control group. The difference in the increase in proportion of controlled hypertension was 20.6% (95% CI, 15.4%-25.9%; P < .001; Table 2). The intraclass correlation coefficient was 0.0415 for hypertension control. There were no significant differences in body weight or waist circumference changes between intervention and control groups (eTable 3 in Supplement 2). Mean intervention cost per patient was $114.6 (95% CI, $113.7-$115.6) or $6.36 per patient per month. There were no significant differences in mean health care costs per patient between groups: $62.2 (95% CI, $44.6-$79.7) in the intervention group and $67.6 (95% CI, $41.9-$93.3) in the control group. The total cost per patient over the 18-month follow-up was $178.6 (95% CI, $161.0-$196.1) in the intervention group and $67.6 (95% CI, $41.9-$93.3) in the control group. The mean adjusted total cost difference was $102.7 (95% CI, $61.0-$144.4), and the incremental cost-effectiveness ratio was $464.7 per additional percentage of patients achieving hypertension control at 18 months (95% CI, $335.2-$771.7). (For further explanation about the cost analysis, see Supplement 2.). Several strategies have been documented to improve BP control among patients with hypertension.6,7,26 In addition, multicomponent interventions targeting health care systems, physicians, and patients have been shown to be more effective.27,28 However, the effects of these intervention strategies have not been well studied in low-income settings. In a cluster randomized trial, Ogedegbe and colleagues29 reported that a multicomponent intervention—including patient education, home BP monitoring, lifestyle counseling, physician education, and BP audit and feedback—did not improve BP control compared with usual care in African-American patients with hypertension receiving care in low-resource primary care practices. The HCPIA trial also involved low-income patients who received health care from a resource-limited public primary care system in Argentina. The major differences between the 2 trials are that the intervention in the HCPIA trial was led by community health workers and conducted at patients’ homes. In another cluster randomized trial, Jafar and colleagues30 reported that community health worker–led home health education or general practitioner training alone did not reduce BP. However, the combination of home health education and practitioner training led to a significant 5.0-mm Hg reduction in systolic BP among patients with hypertension in Pakistan.30 These results support community health worker–led multicomponent interventions for BP control in low-income settings. In many low- and middle-income countries, community health workers are already employed within the public primary care system for infectious disease control and maternal and child health care. Training and engaging them in hypertension management may provide an effective, affordable, and sustainable approach for BP control in low- and middle-income countries. A significant BP reduction among patients from control centers was also observed. In the Remediar+Redes Program, only 11.6% of patients with hypertension had their BP controlled.32 In this study, 52.3% of patients achieved hypertension control at 18 months in the usual care group. Patients received repeated BP measurements every 6 months and were interviewed about behaviors related to antihypertensive medication adherence, which might have contributed to improvement in medication adherence and treatment intensification, and eventually, BP reduction. In addition, intervention contamination could have occurred and contributed to the findings observed in the control group. Furthermore, BP reduction in patients might be partially due to regression to the mean because participants with elevated BP were selected. This study has several limitations. It used a cluster randomized trial design because the multilevel and multicomponent interventions were implemented at the primary care center level. It was not practical to recruit all participants prior to randomization. Therefore, selection bias could have occurred. However, patients with hypertension and their family members were systematically recruited to avoid selection bias.33 Important covariables were also adjusted to limit potential confounding effects. Another limitation is that intervention contamination, if any occurred, might have diluted the observed effect. In addition, the incremental cost-effectiveness ratio for quality-adjusted life-years saved was not calculated because extensive assumptions were necessary for modeling, which was beyond the scope of this report. Therefore, the cost-effectiveness of this community health worker–led multicomponent intervention for BP control could not be directly compared with other interventions for various outcomes.23. This study was a single-center, double-blind, randomized clinical trial conducted to determine the efficacy of postoperative, prophylactic oral cephalexin plus metronidazole compared with placebo for 48 hours after cesarean delivery for the prevention of SSI among obese women who receive standard preoperative cefazolin prophylaxis. The study was approved by the University of Cincinnati Institutional Review Board. Written informed consent of study participants was obtained preoperatively or within 8 hours after cesarean delivery, followed by immediate randomization. Because the cesarean surgical approach was standardized for the study, the principal investigator set a predetermined study enrollment deadline to avoid potential changes in practice patterns that may occur with prolonged study duration. The complete trial protocol is available in Supplement 1. Cesarean surgical approach was standardized at the study institution prior to the start of this trial according to evidence-based practice at the time of study conception.16 All participants received chlorhexidine skin preparation, unless there was a documented allergy in which case povidone-iodine would be used. Standard sterile draping of participants was performed. Intravenous cefazolin (2 g) was administered prior to skin incision. A Pfannenstiel incision was the preferred skin incision. However, decisions regarding the type of skin incision were left to the discretion of primary surgeons and individualized to participants. The cesarean delivery was performed in standard accepted practice and the fascia was closed with a monofilament suture in a running-stitch fashion.17 The subcutaneous layer was reapproximated with 3-0 polyglactin in either running or interrupted sutures if the depth measured 2 cm or more. Skin incisions were closed using 4-0 polyglactin or poliglecaprone in a subcuticular fashion unless a primary surgeon believed skin staples were superior for an individual case. Deviations from the outlined standardized surgical protocol were recorded and analyzed. Surgical dressings were removed 24 to 36 hours postoperatively. Incisions were examined daily in the hospital by labor and delivery physician teams. Investigators, physicians, labor and postpartum staff, and participants remained blinded to treatment allocation through the entire duration of the study. All study participants received standard intravenous cefazolin (2 g) prior to surgical incision. After providing informed consent to participate in the trial, women were randomized to receive either a prophylactic 48-hour course of oral cephalexin, 500 mg, and oral metronidazole, 500 mg, or identical-appearing placebo, every 8 hours for a total of 6 doses of each antibiotic. The first dose of study medications was scheduled to be given 8 hours after intravenous cefazolin was administered. Study drug deviations were reported, including study drugs initiated more than 12 hours after administration of preoperative intravenous antibiotics, missed dose(s) of study drug, administration of study drugs more than 1 hour after the scheduled time of administration, and patient voluntary withdrawal from the study prior to the last scheduled dose. The primary study outcome was development of an SSI within 30 days of delivery. Surgical site infection was defined according to the National Healthcare Safety Network of the Centers for Disease Control and Prevention and comprised superficial incisional, deep incisional, or organ/space infections (see eTable 1 in Supplement 2 for outcome definitions).19 Prespecified secondary outcomes included any incisional morbidity, defined as any defect in the incisional integrity with or without the presence of an infection, including cellulitis, endometritis, and wound separation. Additional secondary outcomes included endometritis, cellulitis, fever of unknown etiology (any temperature greater than 38.3°C without an attributable source), and wound separation. Cellulitis was defined as an infection of the skin, underlying soft tissue, or both requiring antibiotics for treatment. Wound separation included any defect in the skin incision of at least 1 cm. Participants returned for postoperative evaluations at 2 and 6 weeks postpartum. Postpartum diagnoses of SSI were made by treating physicians and verified by chart review or discussion with diagnosing physicians by principal investigators or research registered nurses , who were unaware of the study group assignments. Follow-up examination visits included a detailed history and review of systems to screen for infection and medication adverse effects following delivery. Examination of patients’ wounds determined evidence of wound separation, erythema, induration, or signs of wound abscess. Participants’ vital signs were measured and a point-of-care urinalysis performed to assess for presence of urinary tract infection. Individuals who did not present for either the 2- or 6-week postpartum follow-up examination were called by a research registered nurse or investigator for a minimum telephone assessment, rescheduled for a follow-up visit, or both. Participants were questioned regarding pain at incision site, drainage from incision, fever symptoms, separation of incision, antibiotic administration after hospital discharge, and additional visits to any medical facility for incisional problems or potential medication-related concerns. Data pertinent to other potential risk factors for SSI were collected, including race/ethnicity, chronic hypertension, and diabetes. Maternal race/ethnicity was self-reported as an open-ended question on collection of the demographic information at the time of enrollment and confirmed upon data abstracted from the electronic medical record. Analysis of race/ethnicity was included because prior studies have demonstrated higher rates of SSI among black women.21 The data were reviewed and cross-checked for accuracy by C.R.W. prior to data analysis. Statistical analyses were performed by E.D., who was blinded to study group designation and not involved in study recruitment or data collection. The data continued to be blinded until all statistical analyses were complete. A sample size was calculated to reflect a clinically and statistically meaningful difference in SSI rates between all obese women who received postcesarean antibiotics to those who received placebo. Prior to the designing of the study, the rate of SSI was 20% in the population of obese women undergoing cesarean delivery at our institution. The following assumptions were considered in the power analysis: 50% relative SSI reduction, baseline SSI rate of 20%, 10% rate of postpartum loss to follow-up, 2-sided α=.05, 80% power, and 1:1 ratio of exposed to unexposed. A relative risk reduction of 50% was chosen given studies available during study design that had demonstrated similar reductions with antibiotic prophylaxis, a reduction that was deemed to be clinically meaningful, and one that could potentially outweigh risks of expanded antibiotic use.22,23 The total sample size to detect a clinically relevant and moderately large effect size in SSI reduction was 438 participants. The primary outcome was reported as the difference in proportion of SSI between the cephalexin-metronidazole group and the placebo group. To quantify the effect of antibiotics on the dichotomous outcomes of SSI and other wound morbidities, a log-binomial model, generalized linear model, with log link was used. The analysis was performed with an intention-to-treat principle, including participants lost to follow-up postpartum or with any study deviations. The number needed to treat to prevent 1 SSI was calculated. Approximately 5% of patients had missing follow-up information on the primary outcome of SSI. To account for the potential influence of missing outcome data on risk estimates, a multiple imputation sensitivity analysis was performed. Multiple imputation using multivariate normal distribution with 10 imputations was used, achieving 99% relative efficiency and ensuring in-range values. A regression analysis was conducted on each imputed data set, which combines the results using a weighted average approach that accounts for the variation among imputed data sets. Of the 404 participants randomized, 382 (94.6%) completed the trial, 357 (88.1%) completed the medication regimen without deviation, 12 (3.0%) voluntarily withdrew from the study before the first medication administration or prior to completion of the scheduled regimen, and 43 (10.6%) had 1 or more drug administration deviations. Of the medication administration deviations, 22 (51.2%) were attributable to participants missing the last dose of study drug secondary to hospital discharge. Twenty-one (5.2%) of 404 participants could be reached only by telephone and were not examined during follow-up. The cephalexin-metronidazole and placebo groups had similar rates of loss to follow-up (5.0% vs 5.9%, respectively; difference, 0.5%; 95% CI, −3.8% to 4.9%; P = .81). The overall rate of SSI, the primary outcome, was 10.9% (95% CI, 7.9%-14.0%) in the study population, occurring in 13 (6.4%) of 202 women who received a postoperative course of cephalexin-metronidazole vs 31 (15.4%) of 201 women in the placebo group (difference, 9.0%; 95% CI, 2.9%-15.0%; P = .01). The relative risk (RR) of SSI in the antibiotic group compared with the placebo group was 0.41 (95% CI, 0.22-0.77) (Table 2). Considering the influence of missing data, multiple imputation analysis of the primary outcome estimated an absolute between-group difference in rates of SSI with antibiotics vs placebo of 10.9% (95% CI, 6.1%-15.7%) and an RR of 0.36 (95% CI, 0.23-0.57), which were consistent with the original analysis. The number needed to treat to prevent 1 SSI in all obese women undergoing cesarean delivery was 12 (95% CI, 6.7-33.8). The rate of SSI was 19.8% among women with ROM and 6.9% among those with intact membranes [difference, 13.9%; 95% CI, 12.8%-26.9%; P < .001). Post hoc analysis revealed that among participants with ROM prior to cesarean delivery, SSI occurred in 6 (9.5%) of 60 in the cephalexin-metronidazole group and 19 (30.2%) of 58 in the placebo group (difference, 22.8%; 95% CI, 8.3%-37.2%; P = .008). Among participants with intact membranes, SSI occurred in 7 (5.0%) of 132 in the cephalexin-metronidazole group compared with 12 (8.7%) of 132 in the placebo group (difference, 3.8%; 95% CI, −2.5% to 10.1%; P = .47). Interaction testing was performed between study groups (cephalexin-metronidazole vs placebo) and by membrane status (intact vs ROM) (eTable 2 in Supplement 2). The rate of SSI was highest in those with ROM who received placebo (30.2%) and lowest in those with intact membranes who received antibiotics (5.0%), but the test for interaction did not show statistical significance at P = .30. Cephalexin and metronidazole have high oral bioavailability and pharmacoeconomic advantages and are generally well tolerated, supporting the drug combination as a choice for postpartum prophylactic coverage.29 A 48-hour time course for prophylaxis was chosen considering the biological stages of wound healing. After the initial 1- to 2-day hemostatic and inflammatory phases of healing, the proliferation period occurs and includes epithelization, angiogenesis, and granulation tissue formation. This period is critical for the proper deposition of fibroblasts, basement membrane organization, and collagen formation.30 Because regulation of this inflammatory phase is important for the remodeling periods of healing, we chose a 48-hour course of prophylactic antibiotics to provide coverage while reepithelialization occurs. This study was the first to our knowledge to evaluate the use of a prophylactic postcesarean course of cephalexin-metronidazole for prevention of SSI in an obese population. Cellulitis was the most frequent incisional morbidity observed in the study and had improved rates of SSI among participants who received postpartum antibiotics. The secondary outcomes were not powered to determine a statistically significant difference between participants who received cephalexin-metronidazole vs those who received placebo. Therefore, this study cannot conclude that postpartum cephalexin-metronidazole reduces the other individual secondary outcomes. Rupture of membranes was associated with SSI. However, the tests for interaction between the intact membranes and ROM subgroups and postpartum cephalexin-metronidazole were not statistically different and should not be interpreted as showing a difference in significance or effect size among the subgroups with and without ROM. The use of additional broad-spectrum antibiotics has been shown to reduce SSI in women in all BMI categories undergoing cesarean delivery. Tita et al5 demonstrated a decrease (RR, 0.51; 95% CI, 0.38-0.68; P < .001) in postcesarean SSI by expanding the preoperative antimicrobial spectrum with azithromycin among women in labor and those with ROM but not specifically among obese women. These study results were published in the final year of the current study enrollment period, and azithromycin was chosen to increase coverage of Mycoplasma isolates. Although this study targeted prepregnancy obesity and the C/SOAP Consortium study focused on women in labor as the high-risk cohort,5 there is considerable population overlap, and both studies demonstrated similar and significant reductions in SSI. This study has several strengths. First, the study used a randomized, blinded design; included representation of a diverse, high-risk patient population; and included a relatively large cohort of obese participants. Second, the primary outcome of interest is a significant health care–associated infection that is common and preventable. Although prior studies have demonstrated undergoing labor prior to delivery significantly increased postoperative infections after cesarean deliveries among all BMI categories, these studies were not focused to the obese population nor stratified by membrane status prior to delivery.3,21 Third, the magnitude of the reduction of SSI with postcesarean delivery related to antibiotic prophylaxis in this population (RR, 0.41; 95% CI, 0.22-0.77) is both statistically significant and clinically meaningful. This study also has several limitations. First, the trial was performed at a single site with a high prevalence of obesity, which may not be generalizable to all obstetric practices. Second, women diagnosed as having chorioamnionitis were excluded from study enrollment because they represent a separate risk population. The current study design was unable to assess efficacy among this population. Third, the analyses did not account for multiple comparisons and subgroup interactions, so all secondary outcomes and findings based on stratification of intact or ruptured membranes should be considered exploratory and hypothesis generating. Fourth, this study had insufficient power to determine a significant reduction in SSI in the subgroup of women with intact membranes. The majority of women with intact membranes had scheduled cesarean deliveries and did not have contractions resulting in cervical change, and only 7% of women in this study went into labor with intact membranes prior to cesarean delivery. Hence, the efficacy of prophylactic postpartum antibiotics in this subgroup cannot be determined from the current study, but the findings provide support to promote further study of this subset of participants. Fifth, this trial included a very specific population of women with prepregnancy obesity undergoing cesarean delivery, and the findings should not be extrapolated to other populations or other surgical procedures. Sixth, enrollment was terminated before target recruitment was met (targeted: n=438; actual: n=404) secondary to a predetermined stop date; however, a lower rate of attrition was experienced (10% projected; 5% actual). Seventh, adverse events were monitored per institutional review board protocol, but data regarding more specific minor adverse effects were not collected unless reported by participants. In this issue of JAMA, He and colleagues1 report the results of the Hypertension Control Program in Argentina, a community health worker–led, home-based intervention that aimed to lower blood pressure among 1432 low-income adults with uncontrolled hypertension in Argentina. Compared with usual care, the intervention lowered systolic blood pressure by 6.6 mm Hg (95% CI, 4.6-8.6 mm Hg) and diastolic blood pressure by 5.4 mm Hg (95% CI, 4.0-6.8 mm Hg). This program led to a remarkable 21% absolute difference in the proportion of individuals with controlled blood pressure, defined as a systolic and diastolic blood pressure less than 140/90 mm Hg (73% in the intervention group vs 52% in the usual care group). After 18 months, the mean-adjusted total cost related to the intervention and to health care was $103 (95% CI, $61-$144) higher per participant in the intervention group than in the control group ($178.6 vs $67.6 in total costs, respectively), which was approximately 5% of Argentina’s annual per capita health spending of $1322 in 2015.2. The study was designed and conducted well but has some limitations. First, the intervention group received a higher “dose” of treatment, or engagement, from the health system than the usual care group, which likely drives the observed blood pressure differences. Whether this specific complex intervention—and which component—compared with any intervention that increases engagement with the health system leads to lower blood pressure might be debated. For example, 4 previous trials including 1770 participants demonstrated that home blood pressure monitoring via telemedicine was associated with reduction in systolic blood pressure by 4.3 mm Hg (95% CI, 3.4-5.3 mm Hg) at 9 months compared with usual care.8 Text messaging has been demonstrated to lower systolic blood pressure by 2.2 mm Hg (95% CI, 0.04-4.4 mm Hg) at 12 months in 1 trial of 1256 participants in South Africa. Other interventions might either be more expensive (eg, nurse-led medication therapeutic management9) or be difficult to sustain (eg, unpaid peer support), but some degree of peer support through community or other lay health workers seems to be the key ingredient to reach the “hardly reached.”10 Furthermore, the usual care group, which included a moderate level of baseline medication adherence and a high level of blood pressure control at 18 months, was likely engaged with the health system more than typical low-income patients with hypertension in Argentina through their participation in the trial. The direction and magnitude of the effect of this intervention reported by He et al are similar to behavioral strategies (eg, healthful eating pattern, physical activity12) or individual drugs13 that have been demonstrated to lower blood pressure. However, the nature of this community health worker–led, home-based intervention lies closer to the health-system level, rather than solely to the individual level, which can also potentially strengthen the health system. For example, training among participants, community health workers, and primary care physicians who were randomized to the intervention group helps to improve service readiness for hypertension treatment and control. This training directly addresses patient- and clinician-level knowledge gaps in hypertension treatment and control.6 Furthermore, the community health worker–led, home-based intervention helps to create demand for essential medicines because patients who can benefit from blood pressure–lowering drugs will seek them. The combination of a cost-effective “vertical” program that focuses on a priority condition (eg, blood pressure control) that also strengthens the “horizontal” health system lies at the heart of the diagonal approach.14. Cesarean delivery is one of the most commonly performed surgical procedures. In 2015, 1.27 million infants born in the United States, representing 32% of all US births, were born by cesarean delivery.1 Cesarean delivery is the most important risk factor for infection in the postpartum period. Surveillance for postcesarean surgical site infection (SSI) conducted in the United States between 2006 and 2008 demonstrated an overall SSI rate of approximately 2%,2 but infection rates of up to 20% have been observed in some clinical studies.3 Several factors have been associated with the development of postcesarean SSI, including emergency surgery, onset of labor prior to delivery, membrane rupture prior to surgery, surgical wound class, procedure duration, and obesity.2,4 Among these, obesity is one of the most commonly encountered risk factors. In 2014, 24.8% of women who gave birth in the United States were obese (body mass index >29.9).5 Thus, an estimated 315 000 infants are born to obese women by cesarean delivery in the United States each year. In this issue of JAMA, Valent and colleagues8 report the results of a randomized clinical trial of oral antimicrobial prophylaxis during the first 48 hours after surgery for prevention of postcesarean SSI among obese women. In this single-center study, obese women (prepregnancy body mass index ≥30) admitted to the labor unit for delivery via cesarean method were enrolled, stratified by predelivery membrane status, and randomized to receive either oral cephalexin (500 mg) plus metronidazole (500 mg) (n = 202) or placebo (n = 201) every 8 hours for a total of 6 doses after delivery. All patients received a single 2-g intravenous dose of cefazolin prior to surgical incision. The women were evaluated at 2 and 6 weeks after surgery for the primary outcome of any superficial incisional, deep incisional, or organ/space SSI within 30 days of surgery. Second, is this strategy appropriate for all obese women or is there a more specific subset of women in whom it should be considered? The authors concluded that 12 women need to be treated to prevent 1 SSI. In other words, 11 of 12 women would not benefit from the intervention. While this number needed to treat is certainly less than that for many commonly accepted health interventions, it begs the question as to whether further specification of the target population is possible. Although the study was not powered or designed to determine subpopulations within which the strategy is most appropriate and effective, the post hoc analyses provide some interesting and potentially relevant data. In a subgroup analysis of women with ruptured membranes, assignment to the intervention group of the study was associated with a significantly lower SSI rate (9.5% vs 30.2%; P = .008). Among women with intact membranes, there was no significant difference in SSI rates between study groups (5% vs 8.7%). Further study is needed to determine if membrane status or other factors, such as indication for cesarean delivery, prior cesarean delivery, and the presence or absence of labor, should be included in the decision to implement postoperative antimicrobial prophylaxis. Similarly, study of this strategy among nonobese women with ruptured membranes may also be of value. Fourth, are there risks for the infants of the treated women? The authors acknowledge the possibility of adverse long-term childhood or neonatal outcomes related to maternal receipt of cephalexin-metronidazole, an important outcome that was not evaluated in this study, but propose that the benefits of the intervention outweigh the potential risks. Although maternal use of cephalexin is generally thought to be safe for breastfeeding infants, the World Health Organization and other groups recommend that women avoid breastfeeding during treatment with metronidazole. Thus, many infants may be precluded from the benefits of breastfeeding in the first days of life. In addition, there is a growing body of literature suggesting that antimicrobial exposures during early infancy may have an important role in long-term risks and outcomes, such as obesity, due to alterations in the human microbiome.13-15 Given that cesarean delivery alters the establishment of the infant microbiome, the potential for additional adverse effects among the infants of treated women complicates the assessment of risks vs benefits and further highlights the importance of identifying women who are most likely to benefit from the intervention to minimize unnecessary collateral damage. The clinical trial by Valent et al is an important effort for identifying an effective intervention for the prevention of postcesarean SSI among obese women. The results of this study suggest that a brief course of postoperative antimicrobial prophylaxis may significantly reduce the incidence of SSI among women with a prepregnancy body mass index of 30 or higher. Additional studies are needed to confirm these findings and to a determine if there are specific subpopulations within this group to whom the benefit is limited to avoid unnecessary antibiotic exposure and its associated risks among those unlikely to benefit. Furthermore, despite the apparent effectiveness of the intervention, the rate of SSI observed among women assigned to the postoperative antimicrobial prophylaxis group of the study was higher than that typically observed among nonobese women.2,3 This finding is important because it highlights that this strategy is not a panacea, that obese women remain at high risk of infectious complications, and that additional strategies to mitigate this risk are needed. When patients with advanced cancer near the end of their life, it is important for physicians, nurses, and other health care personnel to respect and dignify the dying process of the patient. This requires a shift in focus from medical intervention to personalization of care to meet the subjective needs of patients and families, including coordination of care, symptom management, communication, and education; emotional and spiritual support; and support of patients’ social relationships and decision making.1 Pain, dyspnea, and hyperactive (ie, agitated) delirium are often experienced by dying patients and witnessed by caregivers. In a study, for example, 51% of 236 patients in palliative care units toward the end of their life had the distressing symptoms of hyperactive delirium.2 Limited evidence is available to guide the clinician in managing these symptoms. Although administration of psychoactive medications can sedate patients and reduce outward symptoms of hyperactive delirium, these drugs do so at the cost of precious time that dying patients and families have to communicate with each other, and they also have important adverse effects. In this issue of JAMA, the study by Hui et al3 provides critical evidence to help guide management of patients with hyperactive delirium in the last few days of life, a very difficult population to study. The investigators enrolled 90 patients with advanced malignancies and delirium who were admitted to an acute palliative care unit, and had at least 1 episode of hyperactive delirium within the previous 24 hours. All patients were then treated with an increased baseline regimen of scheduled haloperidol and were randomized in a blinded manner to receive additional haloperidol + lorazepam or haloperidol + placebo, if they had a recurrence of their hyperactive delirium. Of the 90 patients enrolled, 58 patients were randomized and received study medications for hyperactive delirium (delirium with a mean Richmond Agitation-Sedation Scale [RASS] score of 1.6; denoting apprehensive movement, frequent nonpurposeful movement, or both). Patients receiving haloperidol + lorazepam had a greater reduction of their RASS score over 8 hours than the haloperidol + placebo group (−4 in the haloperidol + lorazepam group vs −2.3 in the haloperidol + placebo group), and most of this decrease in the RASS score was observed in the first 30 minutes. Patients in the haloperidol + lorazepam group had fewer subsequent hyperactive delirium episodes (28% in the haloperidol + lorazepam group vs 76% in the haloperidol + placebo group) and less need for rescue antipsychotic medications. Caregivers and bedside nurses perceived that these patients appeared more comfortable. Although, on the surface, this study may seem to provide evidence to support treating every dying patient who demonstrates hyperactive delirium with a combination of haloperidol + lorazepam, clinicians taking care of these patients need to consider some important caveats. The combination of haloperidol + lorazepam did not treat delirium, but rather masked hyperactive delirium symptoms by sedating the patients and, more likely, converting those patients to hypoactive (ie, apathy, inattention, or lethargy) delirium. This may explain why patients in the haloperidol + lorazepam group had greater severity of delirium (evidenced by Memorial Delirium Assessment Scale [MDAS] scores ≥2 points higher) than the haloperidol + placebo group, despite fewer episodes of hyperactivity. However, the precise effects experienced by patients with hypoactive delirium are unknown because of the limitations of the MDAS in palliative care patients with low Karnofsky Performance Scale Index scores.4. Caregivers and nurses did “perceive” that the patients were more comfortable, yet given the distressing descriptions from some patients of what it is like to experience hypoactive delirium,5 these findings may reveal more about the desire to treat the distress experienced by caregivers and the health care team than actually being a patient-centered intervention. In the study by Hui et al, the mean RASS score among patients treated with haloperidol + lorazepam was −2 or −3 (minimally responsive to verbal stimulus) compared with 0 to −1 (awake and alert, or drowsy) in the haloperidol + placebo group; thus, fewer patients in the haloperidol + lorazepam group would have been able to communicate with their families during their dying hours, which, according to many dying patients, is an unfavorable outcome. Moreover, even though more patients in the haloperidol + placebo group had a RASS score of 1 (hyperactivity) or more in the ensuing 8 hours, the mean RASS score was 0 to −1 in this group, suggesting that these hyperactivity episodes were perhaps mild restlessness (RASS score, 1) and not dangerous or distressing agitation that would have put the patient at risk for causing injury to self or others. Furthermore, among the 32 of the 90 enrolled patients who were not randomized, 27 (84%) never had another hyperactive delirium episode, suggesting that, for many patients, these episodes were mild, self-limiting, or amenable to nonpharmacological (environmental or biopsychosocial) interventions or treatment of the underlying cause. Thus, the priorities of care are to ensure that the dying person’s physical, medical, psychological, and spiritual needs are met, that medication needs are considered, that the family is supported, and that the views of team members with respect to decision making and intervention are considered. If hyperactive delirium persists after these important initial care steps, psychoactive medications remain a consideration. The decision to use such powerful medications should be individualized for the patient using the lowest-possible doses of targeted antipsychotic medications or benzodiazepines rather than a one-size-fits-all protocol. In this fast-paced world of medicine, the challenge for clinicians is to show patience and balance in seeking to reduce distressing symptoms, such as those involving severe pain or hyperactive delirium. Ultimately, as the study by Hui et al3 sought to demonstrate, it is essential for clinicians to focus on the humanness of medicine; to keep dying patients comfortable and as awake as they and their families would like them to be so they can make the last few hours or days of life meaningful; and to make reasonable efforts not to cloud their sensorium unless essential to alleviate patient pain or other severe symptoms. Major recommendations  (1) Outside of situations when patients represent an imminent threat to themselves or others, antipsychotic medications should be used in patients with dementia for the treatment of agitation or psychosis only when symptoms are severe, are dangerous, or cause significant distress to the patient (B recommendation). (2) The clinical response to nonpharmacologic interventions should be reviewed prior to nonemergency use of an antipsychotic medication (C recommendation). (3) Pharmacologic treatment should be initiated at a low dose and titrated up to the minimum effective dose as tolerated (B recommendation). (4) If there is no clinically significant response after a 4-week trial of an adequate dose of an antipsychotic drug, the medication should be tapered and withdrawn (B recommendation). (5) In patients who show adequate response to antipsychotic drug treatment, an attempt to taper and withdraw the drug should be made within 4 months of initiation unless a patient experienced a recurrence of symptoms with prior tapering attempts (C recommendation). (6) In patients whose antipsychotic medication is being tapered, assessment of symptoms should occur at least monthly during the taper and for at least 4 months after medication discontinuation to identify signs of recurrence and initiate a risk-benefit reassessment of treatment (C recommendation). (7) In the absence of delirium, if nonemergency antipsychotic medication treatment is indicated, haloperidol should not be used as a first-line agent (B recommendation). This guideline was developed by the APA.4 The guideline writing group consisted of a chair, a vice chair, and 8 psychiatrists with research and clinical expertise. All members of the guideline writing committee were required to disclose conflicts of interest with the intent of meeting the Institute of Medicine standards. Disclosures were made before appointment, before and during the guideline development, and on publication. No other actions were taken. Experts from other disciplines were added as needed and the final group consisted of 11 members. The manuscript was reviewed by the Alzheimer Association with input also provided by members of the APA, patient and family advocacy groups, and the general public. Reviewers were asked to disclose relevant conflicts of interest (Table). The guideline writing group conducted literature searches in January 2013 and January 2015. The searches yielded 45 randomized clinical trials and 52 observational studies that met criteria for inclusion. Included studies were limited to those that included the second-generation antipsychotics (SGAs) olanzapine, risperidone, quetiapine, and aripiprazole. There is no relevant information in the literature on asenapine, brexpiprazole, cariprazine, clozapine, iloperidone, lurasidone, paliperidone, or ziprasidone. The strength of the evidence was evaluated for risk of bias, consistency of findings across studies, directness of the effect on a specific health outcome, and precision of the estimate of effect using the Agency for Healthcare Research and Quality 2014 recommendations.5 The recommendations’ strengths were rated by the guideline writing group. Fifteen recommendations were made in the guideline, 7 of which are included herein. No recommendations received an A rating. Five recommendations, 4 of which are discussed herein, received a B rating. Three C-rating recommendations are also included in this review. The best evidence for use of SGAs is for agitation. This evidence is primarily from studies using risperidone. The evidence for use of SGAs in psychosis suggests low utility (low strength of evidence for a very small effect). The evidence for risperidone is substantially better than for the rest of the SGAs for this indication (moderate strength of evidence for a small effect). As a point of reference, the standardized mean differences in the studies considered ranged from −0.11 to 0.38, with most estimates in the 0.3 to 0.4 range. The evidence of efficacy for SGAs in the management of overall behavioral/psychological symptoms also suggests low utility (high strength of evidence for a very small effect). For this indication, the evidence for aripiprazole is substantially better than for other SGAs, with moderate strength of evidence for a small effect. Adverse effects of first-generation antipsychotics (FGAs), including increased mortality vs SGAs, led to the recommendation that haloperidol not be a first-line medication for patients with dementia. Haloperidol is specifically mentioned in the guidelines rather than the class of FGAs because most studies used haloperidol as the FGA. There is no reason to believe that other FGAs would be safer. First-generation antipsychotics might be considered for short-term use in individuals with delirium given haloperidol’s rapid onset and its availability in intravenous and intramuscular preparations. This guideline references the trials of nonpharmacologic and pharmacologic interventions for treatment of agitation and psychosis in patients with dementia. Further study of these interventions will clarify the appropriate means of treating these clinical issues. Considering the small effect sizes of these interventions, new therapies need to be developed for these indications. Ultimately, the most effective therapy would be interventions to reduce the development of dementia. A 4-year-old boy presented with a pink nodule on his left cheek. The lesion started as a hyperpigmented macule 3 years ago and since then had been gradually increasing in size and developing into a pink nodule. The lesion was pruritic at times and sometimes bled with minor trauma. The boy was otherwise healthy, with an unremarkable medical history and with no history of trauma or exposure to radiation. On physical examination, there was a 1-cm well-circumscribed, symmetrical, pinkish, dome-shaped nodule with central erosions on the left cheek (Figure 1). Hair, nails, and mucosae were of normal appearance, and the remainder of the examination was unremarkable. The key to the correct diagnosis is the early onset of a pink, symmetrical, solitary, smooth, and dome-shaped nodule on the cheek of a young boy. The differential diagnosis includes Spitz nevus, pyogenic granuloma, sporotrichosis, and amelanotic malignant melanoma. A biopsy for fungal culture is needed to diagnose sporotrichosis, but sporotrichosis typically has a granulomatous appearance and erosions or ulcerations with purulent drainage can be seen on the surface. Cryotherapy and pulsed dye laser therapy can be used in pyogenic granulomas, but pyogenic granulomas predominantly occur in the second decade of life, mostly in young adult females.1 Amelanotic malignant melanomas rarely occur in young children and are usually ulcerated and asymmetrical. There are different types of Spitz nevi, which cannot be completely differentiated clinically; therefore, a skin biopsy for hematoxylin-eosin stain is needed to confirm the diagnosis. Spitz nevi are uncommon melanocytic neoplasms with an overall estimated incidence ranging from 1.4 to 7 per 100 000 persons per year.2 The lesions usually occur before age 20 years, with some presenting at birth.3 Most Spitz nevi present as asymptomatic, symmetrical, solitary, smooth, firm, and dome-shaped nodules, with pinkish, red, or black coloration depending on the melanin content. They are usually less than 2 cm in diameter, although there is often a period of more rapid growth following initial slow growth. They are most commonly found on the lower extremities and head.4 They are highly vascular and can bleed with slight trauma. In this case, the asymmetry of the lesion shown in Figure 1 is a result of the punch biopsy; the lesion was symmetrical prior to biopsy. Histologically, Spitz nevi are usually composed of spindle melanocytes or mixed spindle and epithelioid melanocytes extending from the epidermis into the reticular dermis in an inverted-wedge configuration. Unlike the common Spitz nevus, an epithelioid Spitz nevus is nearly exclusively composed of large epithelioid cells with abundant cytoplasm. Some nevus cells are multinucleated. Melanin is absent or symmetrically distributed and is usually absent in the deepest part.5 Since pleomorphism and mitotic activity can be present, Spitz nevi are sometimes misdiagnosed as other malignant tumors. Histologic features that indicate a Spitz nevus rather than melanoma include nevus cell maturation in deeper parts, minimal or absent pagetoid spread, absence or scarcity of melanin deep in the lesion, Kamino bodies, low nuclear to cytoplasmic ratio, and restriction of mitotic activity. Although pleomorphism and mitotic activity may present in Spitz nevi, expression of Ki-67 by the tumor cells is usually less than 1%. Dermoscopy (direct microscopic examination of the surface and architecture of pigmented skin lesions) reveals a starburst appearance and globular pattern.5,6 Clinically, unlike melanomas, Spitz nevi affect young children and are small, symmetric, and lack surface ulceration. Histopathological examination of the lesion revealed many large, nonpigmented and pleomorphic epithelioid cells in the dermis with large nuclei and abundant cytoplasm (Figure 2, top and bottom left), consistent with epithelioid Spitz nevus. The majority of epithelioid cells were S100 and melan-A positive (Figure 2, bottom right). Ki-67 expression was less than 1%. Because of the location on the face and history of bleeding with trauma, the nodule was completely excised. At 1 year of follow-up, no recurrence of the lesion has been detected. A 53-year-old woman presented for a prescription refill of hydrocodone/acetaminophen 10 mg/325 mg. She had chronic low back pain and partial paralysis from a thoracic spinal cord infarction, secondary to aortic dissection from prior cocaine use. Taking 2 to 3 tablets of hydrocodone/acetaminophen daily improved her back pain from 5 to 2 on a 10-point scale. She reported no recent illicit substance or drug use and stated her last dose of hydrocodone was that day. The patient had not achieved pain control with prior nonopioid pharmacologic pain management, including duloxetine and gabapentin. Although past cocaine use was a risk factor for opioid misuse, a trial of hydrocodone was initiated, after discussion of risks and benefits, with a plan for careful monitoring. The state prescription drug monitoring program showed no other prescribers of controlled substances. A urine immunoassay drug screen was ordered to evaluate for medication misuse and illicit use (Table). CDC guidelines note that a urine drug screening may be clinically useful to determine whether patients are taking prescribed opioids, other substances with risk of misuse, or both.3 This patient’s negative opiate screen may raise suspicion for nonadherence or diversion (giving or selling the medication to others). However, because the opiate screen used has limited cross-reactivity and a high cutoff (2000 ng/mL) for detecting hydrocodone,10 the negative result may be false. False-negative results may also occur when metabolite concentration is low. A test that improves overall diagnostic accuracy by increasing the sensitivity and specificity for hydrocodone and its metabolites would be necessary to confirm its absence. In contrast, the false-positive rate for a cocaine urine drug screening is near zero. The patient restated that she had not used cocaine and was taking hydrocodone daily. The physician informed the patient that given the likelihood of cocaine use, she would no longer prescribe hydrocodone but wanted to continue caring for the patient. Naproxen and topical lidocaine were recommended, and the patient was referred to a pain management program. The patient canceled her next appointment. The state prescription drug monitoring program revealed new hydrocodone prescriptions from another clinician. A third of US adults have hypertension, a major risk factor for heart disease, which is the leading cause of death in the country. Additionally, more than a quarter of the population has higher than normal blood pressure (BP), or prehypertension. With stats like that, one might assume checking BP would be at the top of the list of medical student proficiencies. Yet a recent report suggests otherwise. Only 1 out of 159 medical students correctly performed all 11 elements in a BP check challenge with simulated patients, and the average number of steps performed properly was an abysmal 4.1. Some medical schools have already implemented clinical competency examinations that students must pass prior to graduation, according to Alison J. Whelan, MD, chief medical education officer at the Association of American Medical Colleges (AAMC). Other schools have “intern boot camps,” an intensive clinical skills refresher course in the spring of the final year of medical school. “While not every ‘boot camp’ includes blood pressure measurement, the results of this study suggest that could be a worthwhile, simple addition to such programs,” Whelan said in an email. According to a statement from AbbVie Inc, which developed the drug with Pharmacyclics LLC and Janssen Biotech Inc, 67% of the patients responded to the drug. Among them, 21% had complete responses and 45% had partial responses. Some 48% of patients had sustained responses lasting for at least 20 weeks. Lead investigator David Miklos, MD, PhD, a specialist in blood and marrow transplantation at Stanford University Medical Center, said in a statement that ibrutinib’s approval “represents a major advance and provides physicians with a new option for adults with steroid-refractory [chronic] GVHD.”. A spatial mixed model with a Poisson link function and adjustment for age, sex, and race was fit for each period to calculate and map annualized county-specific risk-standardized rates for carotid endarterectomy and carotid artery stenting. The US counties are shaded from green (lowest rate) to red (highest rate) according to the carotid revascularization rate per 100 000 beneficiary-years. The scales are specific to the procedure and period, and they reflect 6 equal quantiles based on the data. The carotid endarterectomy rates ranged from 77 to 735 per 100 000 beneficiary-years for 1999-2000 and from 41 to 351 per 100 000 beneficiary-years for 2013-2014. The carotid artery stenting rates ranged from 11 to 263 per 100 000 beneficiary-years in 1999-2000 and from 15 to 209 per 100 000 beneficiary-years for 2013-2014. Areas shaded white indicate insufficient data that precluded rate calculations (not calculable). Data from Puerto Rico were used to estimate the national county-level carotid revascularization rates, but they are not included in the maps. For Puerto Rico, the median risk-standardized annual procedure rate for carotid endarterectomy was 195 (interquartile range [IQR], 170-240) per 100 000 beneficiary-years in 1999-2000 and 121 (IQR, 110-132) per 100 000 beneficiary-years in 2013-2014 vs 38 (IQR, 33-49) per 100 000 beneficiary-years in 1999-2000 and 42 (IQR, 40-46) per 100 000 beneficiary-years in 2013-2014 for carotid artery stenting. A Poisson link function and county-specific random intercepts were used to model the number of carotid endarterectomy and carotid artery stenting procedures as a function of patients’ age, sex, and race and accounting for geographic differences between counties. A spherical covariate structure was included in the models to account for spatial autocorrelation. County-level geographic differences were considered because factors such as lifestyle, access to care, and local care practices vary across counties and may affect receipt of procedures. Data were combined for 1999-2000 and 2013-2014 to increase the sample sizes at the county level. Using these models, the annualized risk-standardized procedure rates for each county for the 2 periods were mapped and counties were shaded with a gradient from green to red (lowest rates to highest). To further assess geographic variation over time, the weighted Pearson correlation coefficient was calculated for county-specific procedure rates in 1999-2000 and 2013-2014. The Mantel-Haenszel χ2 test was used to assess temporal changes in patient characteristics and observed outcomes. All models were adjusted for patient demographics, comorbidities, and symptomatic status and included an interval time variable corresponding to each study year (1999 [time = 0] through 2014 [time = 15]) to represent the annual change in outcome. Odds ratios were used for annual change in in-hospital and 30-day mortality. Hazard ratios were used for 30-day ischemic stroke or death, 30-day ischemic stroke, myocardial infarction, or death, and 1-year ischemic stroke. Estimates for the time variable were transformed to reflect the percentage annual reduction in outcome by subtracting the odds ratio or hazard ratio from the null value of 1.0. To permit complete follow-up, we restricted the 1-year analyses to discharges through 2013 and the 30-day analyses to discharges through November 30, 2014. Patient characteristics and outcomes are reported in 2-year intervals. Analyses were conducted using SAS version 9.4 (SAS Institute Inc). Statistical tests used a 2-sided α of .05. Corrections were not made for multiplicity of outcomes to address the potential for type I error; therefore, the analyses should be considered exploratory. There were 937 111 unique patients who underwent carotid endarterectomy (mean age, 75.8 years; 43% women) and 231 077 unique patients who underwent carotid artery stenting (mean age, 75.4 years; 49% women) during the 16-year study. The number of patients who underwent carotid endarterectomy ranged from 81 306 in 1999 to 36 325 in 2014. The number of patients who underwent carotid artery stenting ranged from 10 416 in 1999 to 22 865 in 2006 to 10 208 in 2014 (Table 1, Table 2, and Figure 1). For both procedures, there was an increase in the proportion of symptomatic patients over time (P < .001; Tables 1-2). For carotid artery stenting, the percentage of symptomatic patients varied from 14.4% in 1999-2000 to 25.9% in 2013-2014. The prevalence of several comorbidities and surgical risk factors (eg, hypertension, kidney failure, depression, and diabetes) increased from 1999 to 2014 (all P < .001; Tables 1-2). The national carotid endarterectomy rate decreased from 298 (95% CI, 297-300) per 100 000 beneficiary-years in 1999-2000 to 128 (95% CI, 127-129) per 100 000 beneficiary-years in 2013-2014 (P < .001; Table 1 and Figure 1). Although rates varied across demographic subgroups, patterns were consistent over time (eTable 3 and eFigure 2 in the Supplement). Carotid endarterectomy was performed in 2527 hospitals in 1999 and in 1914 in 2014. The annual volume per hospital ranged from a median of 18 (interquartile range [IQR], 6-45) to 14 (IQR, 5-29) procedures over the 16-year study. Procedure rates varied across the United States. In 1999-2000, the annualized county-specific carotid endarterectomy rates adjusted for age, sex, and race ranged from 77 to 735 per 100 000 beneficiary-years (Figure 2A). Southern and central US regions had the highest adjusted rates. Regional variation persisted in 2013-2014, with rates from 41 to 351 per 100 000 beneficiary-years. The weighted Pearson correlation between rates in 1999-2000 and 2013-2014 was 0.44 (P < .001), and there was no change in rank over time for 52% of counties in the lowest 2 rate quantiles in 1999-2000 and 53% of counties in the highest 2 rate quantiles. National carotid artery stenting rates increased between 1999-2000 and 2005-2006 from 40 to 75 per 100 000 beneficiary-years (P < .001) but later decreased annually to 38 per 100 000 beneficiary-years in 2013-2014 (P < .001; Table 2 and Figure 1). There was variation among demographic subgroups, but temporal patterns were generally consistent (eTable 4 and eFigure 2 in the Supplement). Carotid artery stenting was performed in 1738 hospitals in 1999, 1821 in 2006, and 1608 in 2014. There was a median of 4 (IQR, 1-8) carotid artery stenting procedures performed per hospital in 1999, 7 (IQR, 3-19) in 2006, and 4 (IQR, 2-11) in 2014. Annualized county-specific rates for carotid artery stenting adjusted for age, sex, and race varied geographically from 11 to 263 per 100 000 beneficiary-years in 1999-2000 to 15 to 209 per 100 000 beneficiary-years in 2013-2014 (Figure 2B). Rates in 1999-2000 were highest in the southern and central regions, with additional peaks in California and in parts of the southwest. In 2013-2014, rates remained highest for the southern and central regions. The weighted Pearson correlation between rates in 1999-2000 and 2013-2014 was 0.20 (P < .001), and there was no change in rank over time for 43% of counties in the lowest 2 quantiles in 1999-2000 and 42% of counties in the highest 2 quantiles. Thirty-day outcomes after carotid endarterectomy improved during the study years. The composite of ischemic stroke or death decreased from 4.4% in 1999-2000 to 3.1% in 2013-2014 (absolute decrease, 1.4% [95% CI, 1.2%-1.5%]) and all-cause mortality decreased from 1.6% to 1.1% (absolute decrease, 0.5% [95% CI, 0.4%-0.6%]; Table 1). These improvements were significant in adjusted analyses, with annual reductions of 2.90% (95% CI, 2.63%-3.18%) for 30-day ischemic stroke or death and 2.97% (95% CI, 2.56%-3.37%) for 30-day all-cause mortality (Figure 3A and eTable 5 in the Supplement). Patterns were similar across prespecified demographic subgroups, with the exception of all-cause mortality for those with race/ethnicity of other. For carotid artery stenting, outcome patterns were more complex (Table 2). Rates of 30-day ischemic stroke or death decreased from 7.0% to 4.8% between 1999-2000 and 2005-2006 and then increased to 7.0% by 2013-2014. The absolute decrease in the rates of 30-day ischemic stroke or death between 1999-2000 and 2013-2014 (−0.1%; 95% CI, −0.5% to 0.4%) was not statistically significant. Similar results were observed for in-hospital mortality (absolute decrease, 0.5%; 95% CI, 0.2% to 0.8%), 30-day all-cause mortality (absolute decrease, −0.1%; 95% CI, −0.5% to 0.3%), and 30-day ischemic stroke, myocardial infarction, or death (absolute decrease, −0.1%; 95% CI, −0.6% to 0.4%). One-year ischemic stroke decreased during the study years from 8.1% to 6.3% (absolute decrease, 1.6%; 95% CI, 1.2%-2.1%). Between 1999-2000 and 2013-2014, the median hospital length of stay for the index hospitalization decreased from 2 days to 1 day among patients who underwent carotid endarterectomy (Table 1). Among patients who underwent carotid artery stenting, the median length of stay decreased from 2 days to 1 day between 1999-2000 and 2011-2012 but then returned to 2 days by 2013-2014 (Table 2). The inflation-adjusted median Medicare payment for the index hospitalization decreased during the study years from $8278 to $6779 for carotid endarterectomy (Table 1) but increased from $12 963 to $14 796 for carotid artery stenting (Table 2). For both procedures, the rates of discharge to home decreased, whereas discharge to home care and skilled nursing or intermediate care facilities increased (Tables 1-2). Results stratified by symptomatic status appear in eTables 6-7 in the Supplement. Quiz Ref IDIn this nationwide 16-year study of fee-for-service Medicare beneficiaries, there was a reduction in the performance of carotid revascularization and improvement in postprocedure outcomes. Carotid endarterectomy rates decreased annually from 1999 to 2014, whereas carotid artery stenting rates increased from 1999 to 2006 and then decreased from 2007 to 2014. Despite lower hospital procedure volume over time, there were annual improvements in mortality and ischemic stroke outcomes for both procedures after accounting for demographic characteristics, comorbidities, and symptomatic status. Reductions in adverse outcomes were observed for both sexes and within most age and race subgroups. Quiz Ref IDBecause of advances in medical therapy, physicians may less frequently recommend revascularization for patients with asymptomatic carotid disease. Asymptomatic patients represent the majority of those undergoing revascularization. If their stroke risk is low, the perceived need to intervene by referring physicians could have been affected. Recent observational studies suggested an annual stroke risk of 1% or less for asymptomatic patients,26,27 which is about half the risk rate reported in clinical trials initiated in the 1980s and 1990s.4,6. Changes in surgical reimbursement policies over time may have also contributed to the reported patterns.23,28 As carotid endarterectomy rates declined from 1999 to 2014, CMS reimbursement also decreased. Performance of carotid artery stenting increased from 1999 to 2006 and then decreased annually through 2014. Prior to 2005, the CMS did not reimburse carotid artery stenting when patients were treated outside approved clinical trials and postapproval studies.23 In March 2005, the CMS determined that carotid artery stenting coverage could be provided for symptomatic patients with 70% or greater stenosis who were at high risk to undergo carotid endarterectomy.28 Thereafter, the CMS has not reimbursed the costs of carotid artery stenting for symptomatic patients meeting the conventional risk profile nor asymptomatic patients with conventional or high risk treated outside an approved clinical trial or postapproval study. These restrictions for asymptomatic patients may have played a role in the decreasing proportion of asymptomatic patients undergoing carotid artery stenting, and the higher proportion of symptomatic patients could have attenuated outcome improvements. Annual improvements in carotid endarterectomy and carotid artery stenting outcomes within demographic subgroups suggest improved safety for older individuals (aged ≥65 years). The adjusted annual percentage reductions in 30-day outcomes after carotid endarterectomy were similar across age groups. Annual reductions were comparable across sexes and race/ethnicity for both procedures, with the exception of black patients who underwent carotid artery stenting and had no significant annual reduction in the 30-day composite outcomes that included stroke. These findings extend prior research that compared overall outcomes by demographic characteristics16,17,33,34 by assessing whether improvements were seen within subgroups over time. The mechanisms mediating improved outcomes following revascularization are not clear. Criteria for case selection may have changed, favoring procedures for individuals at lower risk for adverse outcomes (ie, those who are younger, asymptomatic, and have a lower comorbidity burden). However, there was an overall increase in the proportion of patients with symptomatic disease, hypertension, diabetes, and kidney failure. Higher case volume has been associated with improved postprocedural outcomes,38,39 but there was a decline in the average number of procedures performed at hospitals during the study period. This study has several limitations. First, analyses of Medicare data are subject to coding errors, changes in coding patterns over time, and lack of detailed clinical information on the procedure, anatomical details (eg, stroke etiology, ipsilateral vs contralateral location), intraprocedural practices, chronology of events during the hospitalization, and patient preferences. Second, available comorbidity data may not adequately characterize the medical complexity of patients; however, the performance of models using administrative databases was comparable with that of models using medical records for short-term outcomes in prior work.18 Third, compared with other patients who underwent carotid revascularization and met CMS criteria, patients who underwent carotid artery stenting while enrolled in industry-sponsored registries during the study may have had more extensive postprocedure assessment of outcomes. Fourth, outpatient and observation stay revascularization procedures, which may have increased during the latter study years, could not be assessed. Fifth, analyses were limited to fee-for-service beneficiaries aged 65 years or older, and results may not generalize to those with Medicare Advantage or to younger patients. However, fee-for-service Medicare provides the largest national database with rates of hospitalization and postdischarge outcomes. Sixth, our analyses included a large number of comparisons, and corrections were not made for multiplicity to address the potential for type I error; accordingly, the analyses should be considered exploratory. One oft-cited study, from Communities That Care (CTC), tailored evidence-based interventions to families and school-aged youth, including training to reduce risk factors, such as delinquency, and boost protective factors, such as decision-making skills for problem solving and resisting peer pressure. Involving more than 4000 youth (in grade 5 at the beginning of the study) in 7 states, this study matched 12 pairs of small towns, randomly assigning one from each pair to the intervention. By grade 12, those in intervention towns were 18% more likely to avoid delinquent behavior and 31% more likely to abstain from gateway drug use (alcohol, cigarettes, or marijuana) than their counterparts in matched control towns. Such studies not only provide the foundation for addiction prevention recommendations by the White House Office of National Drug Control Policy National Drug Control Strategy (2016), the Office of the Surgeon General, and the National Institute on Drug Abuse but also inform the ongoing work of the President’s Commission on Combating Drug Addiction and the Opioid Crisis, established in March 2017. Meanwhile, organizations are encouraging prevention strategies in communities nationwide. The Center for Substance Abuse Prevention of the Substance Abuse and Mental Health Services Administration (SAMHSA) supports implementing effective prevention practices, focusing on their fidelity and sustainability. The Community Anti-Drug Coalitions of America (CADCA) promotes drug-free communities via youth education and training. The National Association of State Alcohol and Substance Abuse Directors fosters prevention and treatment through every state. Yet, to date, such prevention practices have currently reached only about 10% of youth. Developing stronger prevention measures also requires recognizing specific challenges of opioids. The 33 000 opioid-related deaths in 2015 arose from misuse of illegal drugs (including heroin or illicitly manufactured synthetic opioids such as fentanyl), legal prescription drugs, or multiple additional substances. The average age of initiation was about 25 years, and median age of overdose death ranged from 35 years to 44 years. Of those misusing heroin, 75% to 83% reported starting with a prescription drug. Of those misusing prescription opioid painkillers, 41% to 57% obtained them from friends or relatives. In 2015, 91.8 million (37.8%) US adults reported using prescription opioids, 11.5 million (4.7%) misused them, and 1.9 million (0.8%) had use disorders. Misuse and use disorders were most common among those who were uninsured or unemployed, were low-income individuals, or had behavioral health problems. These data highlight the urgent need to address addiction broadly—and opioids in particular—and to reduce access to both illegal and legal drugs. Prevention can encompass a continuum of activities. This ranges from educational efforts that frame the crisis as a medical issue (as opposed to solely a criminal justice issue) to multifaceted policies and practices that can complement treatment. Across the country, multisector coalitions (including, for example, patients, families, educators, health leaders, law enforcement officials, and policy makers) are tailoring opioid-specific interventions to their own communities. Online toolkits from CADCA and from SAMHSA document case studies of some of these efforts in areas hardest hit by the crisis. Public Education. Major national momentum to teach lay bystanders to reverse overdoses with naloxone has now reached 50 states and the District of Columbia, aided in 40 by Good Samaritan laws providing liability protections. A systematic review of 19 studies showed increased knowledge and administration rates, as well as some early evidence of decreased deaths in Massachusetts and in North Carolina. Such education can also raise public understanding of harm reduction through syringe service programs and catalyze collaborations with criminal justice officials committed to expanding their role beyond traditional law enforcement. Community-Based Medication Disposal Programs. Collection sites and “drop boxes” for unused prescription opioids have arisen in sites including hospitals, fire departments, and pharmacies. For example, Walgreens' safe medication disposal kiosks populate over 500 stores across 43 states, and the Drug Enforcement Agency's National Prescription Drug Take-Back Days involve over 5000 collection sites. Further evaluation awaits. The new data also showed that last year an additional 6.6 million infants who received their first dose of the DTP vaccine did not complete the full 3-dose immunization series, called DTP3. Coverage rates for DTP3 shots were below 50% in 8 countries: Central African Republic, Chad, Equatorial Guinea, Nigeria, Somalia, South Sudan, Syrian Arab Republic, and Ukraine. Millions of children remain undervaccinated in countries that are especially vulnerable to outbreaks due to poverty and conflict. There was a significant decrease in the percentage of survey respondents with disabilities for whom health care was available (69% vs 44%) and whose experience of health care coverage was positive (78% vs 45%) between 2005 and 2013. The time it took people with disabilities to reach a health care center was longer and the connectivity by paved roads was worse in 2013 than 2005, which also negatively affected perceptions of health services. These findings conflict with reports that Afghanistan is making progress in ensuring access to health care for all Afghans as the result of initiation of a Basic Package of Health Services initiated in 2002. But there’s an aspect I believe she has not adequately emphasized. Having been immersed in poetry since childhood I, like many creatively driven people, have had to write. It isn’t a business of harnessing an unruly inner flame. It is not captured in Jamison’s idea that Lowell “did much of his healing through his writing.”1(p174) Nor does she clinch it with how “art also serves the writer who is ill” and gives “meaning and moment to awful things.”1(p194) It feels rather like a current that must find ground. Writing is, I believe, a way to hold some sense of self and world together when there may be no other way. When someone says, “I have to write,” I believe it. I imagine many writers, composers, painters, and others staving off psychic disorganization by intensive involvement in the creative process. When I look at a van Gogh painting it seems that by the brushstroke he is feverishly, barely holding his world together. When I listen to heartbreaking or fervent music—Beethoven, Schoenberg, or Stravinsky—I sense the overwhelming intensity of aliveness being rendered coherent, meaningful, bearable, note by note in the composer’s activity. It is not uncommon these days for clinicians to prescribe antidepressants and leave the personal meanings of presenting distress unattended. The reigning assumption is that depression and anxiety are meaningless. Suffering of all sorts is implicitly considered useless. Our culture seems not to know what to do with invisible suffering—how it might be transduced into fuller aliveness. The arts constitute such a modality. Making a poem shapes meanings to say what one would otherwise harbor silently. It is a weaving of selfhood. Without this and the other arts, we would all be more mutely alone. Robert Lowell, as Jamison makes clear, has contributed to our coherence. He offers work that holds us in historical space, meaning-space, where we can integrate the chaos assaulting our mortal lives to the music of his language. Had fate not allowed him his measure of madness, he may not have had to write. If we can welcome some despair—bipolar or melancholic, social or psychological—we can grow as a culture. If time is money, and we make no space for our patients’ distress—if we shove pharmaceuticals down mania’s throat and keep it there—we silence a vibrant part of our humanity. Might we practice a more attuned engagement that takes time, does not submit to corporate urgencies, and incurs some risks but keeps close watch, in the interests of human meaning?. In the search for new or little appreciated sources of palatable foods to meet the shortage that the war has brought to the entire civilized world, some consideration has already been given to fish. From a physiologic or nutritional standpoint, this type of animal food commends itself on account of its comparative richness in precisely those nutrients—proteins and fats—which at present command the highest prices. The fact that no special feeding or nurture is required to produce the common fish of our markets puts them into striking contrast, from an economic standpoint, to the various types of meat that are obtained as the result of the prolonged feeding of animals originally valued at a liberal price, through a period of growth and the process of finishing for the market. The cod, the mackerel and the shad, for example, require no management comparable to the methods of animal production in farm practice. The essential features of expense lie in the capture and marketing of the food. The food purchasers of this country are losing valuable opportunities to extend the variety of edible products in this period of stress through an ignorance of the advantages or possibilities offered by some of the little used fishes. The prejudices engendered through ignorance or tradition in turn limit the satisfactory development of the fish-producing and fish-distributing industries. Even inland the opportunities are not so limited as is commonly supposed, so that a more liberal use of fish is not necessarily restricted to the regions bordering on the coast line of this country.2 As instances of such neglected nutrient products Ward mentions the burbot, a fresh-water fish belonging to the cod family, which is sufficiently palatable to be regarded as a delicacy in continental Europe. The bowfin is a Mississippi basin fish that yields a smoked and salted product of satisfactory flavor. The carp, which is caught far inland, likewise has acquired an undeserved prejudice. Among ocean fish the tilefish and the sable-fish are destined to come into popular favor ; while the rapidly increased demand for the grayfish, a new candidate for recognition in this country, though long used along the Mediterranean, suggests something in the nature of psychologic popularity promoted by the familiarity with whitefish and bluefish. If butter substitutes can become popular despite the prejudice that was long created and maintained against them, if other cereals can win their way as competitors of wheat for popular favor in human nutrition, surely widespread reminders of the value of fish can and ought to create a demand for these wholesome products even in cases of little recognized merit. Strange names have not militated against many other novelties that have become household words. As Ward says, most families plan for fish once a week or less frequently; if they do not find just what they want available, they are apt to pass by, and meat is substituted. Thus for one cause or another the average fish consumption in the interior portion of the country falls far below that which obtains in the Old World, or that which should exist here when our extensive and splendid supply of fish food is taken into account. Fish are worthy of a more prominent place in the dietary despite the fact that they are not the unique “brain food” that an antiquated fancy would have them be, and despite the unsupported belief that they are concerned in the transmission of cancer. Like other perishable products they are liable to objectionable decay; but this is not an insurmountable obstacle with present methods of food hygiene and practical conservation. Hence we accept the slogan, “Every day is fish day.”. Watching a solar eclipse without proper eye protection can burn the macula (the center part of the retina, the light-sensitive tissue that lines the inside back wall of the eye, specially designed to read or recognize faces). When the moon completely blocks the sun at the minute of a total solar eclipse, dangerously, a person might look at the eclipse without protection and falsely believe that it is safe to view. However, within a few seconds, as the moon continues to move, bright sunlight suddenly might be focused on the unprotected macula. Even a few seconds of such viewing can temporarily or permanently burn the center of the retina. Once retina tissue is destroyed, like brain tissue, it cannot regenerate, resulting in permanent central vision loss. Pinhole projector: Make a pinhole in a piece of cardboard and hold it in front of the sun just before the eclipse. With your back to the sun, focus the light going through the pinhole onto another piece of cardboard behind the pinhole so that you see the sunlight focused onto the second piece of cardboard. As the eclipse occurs, you can see the focused sunlight become blocked by a dark circle (the shadow of the moon). Look only at the image on the paper. Source: Cogan DG. On viewing the eclipse. Arch Ophthalmol. 1963;69(6):690-692. Because the number of patients participating in premarket clinical studies will be influenced by disease prevalence, for each non-orphan-designated therapeutic, we estimated the relative, as opposed to the absolute, size of the premarket clinical development program. To do this, we first used the Drugs@FDA database to determine the total number of patients included in the FDA’s therapeutic safety analysis (safety population), described within the medical review. Next, we used Thomson Reuters’ Incidence and Prevalence Database,3 primary epidemiology literature, and summary reports to estimate the number of US patients who were potentially eligible for treatment (target population). We then calculated the ratio of safety population to the target population, categorizing therapeutics as having a ratio in the top quartile (ie, larger safety population relative to target population) or below the 75th percentile. The threshold ratio for categorization in the top quartile was 0.005, implying that the equivalent (or more) of 0.5% of the total number of US patients who were potentially eligible for treatment were used for the FDA’s therapeutic safety analysis. We repeated our original multivariable model, including the 7 novel therapeutic characteristics and features of their regulatory approval, as well as expected length of treatment and safety population:target population ratio. Neither characteristic was statistically associated with increased risk of postmarket safety events. For expected length of treatment, using short-term treatment as the reference, the incidence rate ratio (IRR) for intermediate treatment was 0.67 (95% CI, 0.35-1.29), whereas the IRR for long-term treatment was 0.88 (95% CI, 0.34-2.29). For the safety population:target population ratio, using the top quartile as the reference, the IRR for therapeutics below the 75th percentile was 1.17 (95% CI, 0.63-2.16). Further research should consider additional novel therapeutic characteristics or features of their regulatory approval that may be potential predictors of an increased risk of postmarket safety events. At the time of enrollment, patients were randomized to lorazepam or placebo. All enrolled patients immediately began a standardized regimen with haloperidol 2 mg every 4 hours intravenously and 2 mg every hour as needed for agitation. Because of the fluctuating nature of delirium, the Richmond Agitation-Sedation Scale score of each patient was monitored every 2 hours until the patient’s score was 1 or more and required rescue medication per the judgment of the bedside nurse. Once the dose of haloperidol was increased and standardized, 27 of 90 randomized patients (30%) did not develop further agitation until death or discharge and thus did not require the study medication. A, Time 0 indicates immediately before treatment administration. Error bars indicate 95% CIs. Both treatments were associated with significant reduction in the mean RASS score within the first 30 minutes of treatment. RASS score remained relatively stable for both groups over the 8-hour observation period. Lorazepam + haloperidol was associated with a significantly greater reduction in RASS score than placebo + haloperidol at 8 hours (P < .001, 2-sided Wilcoxon rank sum test). B, A larger proportion of patients had hyperactivity (RASS score, 1 to 4) in the placebo + haloperidol group at both 30 minutes and 8 hours (P = .001 for both time points). In contrast, a larger proportion of patients had sedation in the lorazepam group (RASS score, −3 to −5). The 2-sided Fisher exact test was used to compare the 3 categories of RASS scores between groups. This was a double-blind, parallel group, placebo-controlled, randomized clinical trial in which patients with hyperactive or mixed delirium were allocated in a 1:1 ratio to receive lorazepam + haloperidol or placebo + haloperidol as treatment for a single episode of restlessness or agitation. The trial protocol and a list of the revisions with justifications are available in Supplement 1. Key protocol changes related to study objectives, eligibility criteria, and statistical analyses are highlighted in eTables 1 to 3 in Supplement 1. The institutional review board at MD Anderson Cancer Center approved this study. Written surrogate consent was obtained from the medical power of attorney or legal representative. The institutional review board did not require caregivers or nurses to sign an informed consent for their involvement. Enrollment occurred from February 11, 2014, to June 30, 2016. Data collection was completed in October 2016. The acute palliative care unit was selected as the study setting because of the high prevalence of persistent agitated delirium at the end of life and because patients received standardized care for delirium by an experienced interdisciplinary palliative care team consisting of physicians, nurses, and pharmacists. Patients were routinely treated for any potentially reversible causes (eg, opioid neurotoxicity, polypharmacy, infections, hypercalcemia, and other metabolic causes) and provided with nonpharmacologic measures (eg, orientation cues, avoiding unnecessary stimuli, window light, and caregiver education and involvement) and intensive symptom management. Blinded physicians and nurses were involved in the identification of potential patients, administration of study medications and documentation of study outcomes. A do-not-resuscitate order was not required for admission. The bedside nurses conducted shift change sign-out at 7 am and 7 pm at the bedside to communicate patient care issues to maximize continuity of care and study data collection. Web-based simple randomization was used to assign patients to the 2 treatment groups. All enrolled patients immediately initiated a standardized open-label regimen with haloperidol (2 mg) every 4 hours intravenously and another 2 mg every hour as needed for agitation. Because of the fluctuating nature of delirium, we monitored the RASS score of each patient every 2 hours until the score was 2 or more and required rescue medication according to the bedside nurse’s judgment before administering the blinded study medications (lorazepam or placebo). Once the patient met this threshold, a single dose of 3 mg of lorazepam in 25 mL of 0.9% normal saline solution or identically appearing placebo (25 mL of 0.9% normal saline) was infused intravenously over 1.5 minutes. The timing of the primary outcome was 8 hours from when the blinded study medication was administered. Patients in both groups also received 2 mg of haloperidol intravenously immediately afterwards. The RASS score threshold for blinded study medication administration was revised to 1 or more in September 2014 to ensure that patients who had any agitation could proceed to the blinded phase. All patients had at least 2 days of delirium with documentation of agitation before starting the study intervention. The use of other medications and withholding of scheduled haloperidol were permissible as per standard of practice according to the clinical judgment of the attending physician and bedside nurse. Quiz Ref IDOur prespecified primary outcome was the RASS score, a validated 10-point numeric rating scale that ranges from −5 to 4, at 8 hours.12,13 The score definitions were as follows: −5, unarousable; −4, deep sedation; −3, moderate sedation; −2, light sedation; −1, drowsy; 0, alert and calm; 1, restless; 2, agitated; 3, very agitated ; 4, combative. This was assessed by the bedside nurse immediately prior to study medication administration and then at 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, and 8 hours. Subsequently, RASS scores were documented daily until discharge or death. To determine interrater agreement, the research staff and nurses both rated RASS scores independently at the time of study enrollment. Secondary outcomes defined a priori included (1) the severity of delirium assessed with the Memorial Delirium Assessment Scale (MDAS; range, 0-30; higher scores indicate greater severity) at baseline, 2, 4, and 8 hours and then daily until discharge, (2) the use of any additional psychotropic agents during the first 8 hours after study medication administration and then daily until discharge, (3) the Edmonton Symptom Assessment System (ESAS; range, 0-10; higher scores indicate greater severity) with proxy ratings provided by family caregivers daily until discharge,14,15 (4) patient comfort perceived by caregivers and bedside nurses daily (5-point Likert scale ranging from “strongly agree” to “strongly disagree”), (5) the recalled frequency of 6 delirium symptoms (ie, disorientation to time, disorientation to place, visual hallucinations, tactile hallucinations, auditory hallucinations, delusional thoughts, and psychomotor agitation) and related distress in the rater recorded by family caregivers and bedside nurses daily until discharge (range 0-4; higher scores indicate greater frequency or distress),2,4 (6) communication capacity perceived by caregivers and bedside nurses was assessed daily, (7) adverse effects related to the use of benzodiazepines and neuroleptics were documented using the Udvalg for Kliniske Undersøgelser assessment (range 0-3; higher scores indicate greater severity),16 (8) duration of stay in acute palliative care unit, and (9) overall survival from the time of study medication administration. Further details of study assessment are available in the eAppendix in Supplement 2. Salivary biomarkers were also collected but results are not reported here. Race/ethnicity data were collected based on patient or family caregiver self-report as mandated by National Cancer Institute using fixed categories.17. Baseline characteristics were summarized by descriptive statistics. The prespecified primary outcome, change in RASS score from immediately before blinded study medication administration (time 0) to 8 hours, was compared between study groups by using the Wilcoxon rank sum test. Because of the nature of the study population, many patients died or were discharged before requiring the study medication; thus, a modified intention-to-treat analysis including only patients who started the study interventions was specified a priori. Because the RASS score is a momentary measure, we also conducted post hoc analyses to assess the proportion of patients documented to have any RASS score of 1 or more documented during the first 8 hours. The change in secondary outcomes before and after medication administration was compared between groups using a 2-tailed Wilcoxon rank sum test for continuous variables and 2-tailed Fisher exact test for categorical variables. The difference in the change of these end points before and after treatment was summarized by mean, median, and proportion along with the associated 95% CIs for parametric, nonparametric continuous, and categorical variables, respectively. The Kaplan-Meier method was used for time-to-event analysis and the log-rank test and univariate Cox regression analysis to compare overall survival between groups. All analyses were 2-sided tests. For our prespecified primary outcome analysis, a 2-sided P value of .05 or less was considered to be statistically significant. We did not adjust for multiple comparisons and all secondary findings are considered to be hypothesis-generating. The interrater reliability of RASS scores between the bedside nurse and the research nurse were determined at the time of study enrollment using the κ statistic. In post hoc analyses, missing data on the primary outcome were imputed using the multiple imputation method under the assumption of a monotone missing pattern (eTable 1 in Supplement 2). Post hoc worst-case sensitivity analysis was conducted by assuming that the patients who started but did not complete the study intervention had no change in RASS score from baseline at 8 hours. Missing data were not imputed for secondary outcomes. Among the 144 eligible patients, 93 (65%) were enrolled and 90 (63%) were randomized and started on the standardized haloperidol regimen. After a median observation period of 6.4 hours (interquartile range [IQR], 4.4 to 15.7), 58 patients (64%) developed an agitation episode requiring rescue medication and received lorazepam or placebo in conjunction with haloperidol. Fifty-two patients (90%) had at least 8 hours of monitoring (Figure 1). Among the 32 patients (36%) who did not receive the study medication, 27 (84%) did not develop further agitation necessitating intervention until discharge or death after the standardized dose increase of haloperidol, 4 (13%) dropped out, and 1 (3%) was deemed ineligible (Figure 1). The mean RASS score prior to medication administration was 1.6 points (SD, 0.6) in both groups. Lorazepam + haloperidol was associated with a significantly greater reduction of RASS score at 8 hours than placebo + haloperidol (−4.1 points for the lorazepam + haloperidol group vs −2.3 points for the placebo + haloperidol group; mean difference, −1.9 points [95% CI, −2.8 to −0.9]; P < .001) (Figure 2A and Table 2). As shown in Figure 2A, patients in the lorazepam + haloperidol group had a significant within-group reduction in RASS score within the first 30 minutes of treatment administration and this effect was maintained at 8 hours (mean change, −3.6 points at 30 minutes and −4.1 points at 8 hours). A smaller decrease in RASS score was also observed in the placebo + haloperidol group at 30 minutes and at 8 hours (mean change, −1.6 points at 30 minutes and −2.3 points at 8 hours). The κ for RASS score assessment at the time of study enrollment between research staff and nurse was 0.79 (P < .001). The ESAS showed no statistically significant difference between the 2 study groups, except for greater level of drowsiness as rated by caregivers (1.9 in the lorazepam + haloperidol group vs −2.0 in the placebo + haloperidol group; mean difference, 3.9 [95% CI, 0.8 to 7.1]; P = .03) (Table 2). During the first 8 hours after study medication administration, MDAS score and respiratory rate did not differ between study groups and remained stable over time (MDAS score: 2.5 points in the lorazepam + haloperidol group points vs 0.4 points in the placebo + haloperidol group; mean difference, 2.1 points [95% CI, −1.0 to 5.2], P = .18; respiratory rate: −1.5 in the lorazepam + haloperidol group vs −0.5 in the placebo + haloperidol group; mean difference, −1.0 [95% CI, −3.4 to 1.4], P = .80) (Table 2). We did not identify any significant difference in other secondary measures, including delirium recall and related distress and communication capacity (eTable 2 in Supplement 2). The most common adverse effects were hypokinesia and akathisia (hypokinesia: 3 patients [19%] in the lorazepam + haloperidol group and 4 patients [27%] in the placebo + haloperidol group; akathisia: 3 patients [19%] in the lorazepam + haloperidol group and 1 patient [7%] in the placebo + haloperidol group). One patient (3%) in the lorazepam + haloperidol group and 3 patients (10%) in the placebo + haloperidol group died within 8 hours of study medication administration. No significant differences were found in discharge outcomes and overall survival (Table 2 and eFigure 3 in Supplement 2). Agitation in the setting of delirium is distressing for patients, their caregivers, and clinicians. The RASS score enables evaluation of the effect of alternative pharmacologic interventions to treat agitation from multiple perspectives. However, the desirable RASS score among patients with agitated delirium is ill defined and is likely to depend on how much caregivers and patients value alertness in the context of the dying process. Patients and their caregivers wish to avoid both agitation (ie, RASS score, ≥1) and excessive sedation (ie, RASS score, ≤−3). In this study, the lorazepam + haloperidol group not only had fewer patients with a RASS score of 1 or more anytime during the first 8 hours, but also required fewer doses of rescue medications, supporting the hypothesis that lorazepam + haloperidol could effectively control agitation. The number needed to treat based on this metric was 2.1 (95% CI, 1.4 to 3.9). Whether a RASS score of 0 to −2 might be considered a more desirable outcome than a RASS score of −3 to −5 is uncertain and is likely to vary among patients, their caregivers, and clinicians. In this trial, the mean RASS score was approximately 0 in the placebo + haloperidol group and was below −2 in the lorazepam + haloperidol group, suggesting a trade-off between more-effective treatment of agitation and higher levels of sedation (Figure 2). Yet, patients in the lorazepam + haloperidol group were perceived to be more comfortable, suggesting that caregivers and nurses valued lack of agitation over the risk of  greater sedation. More research is needed to define the optimal RASS score range in the context of terminal delirium. Further research should also examine various pharmacologic combinations and dosing to minimize oversedation while achieving optimal control of agitation.27,35,36,37. Quiz Ref IDIn contrast to a majority of clinical trials on delirium that focused on reducing the overall delirium severity or a composite of symptoms,9,24 the primary goal of this study was to control a specific symptom of delirium—agitation—because it causes high levels of distress among patients and caregivers.4 The study findings support the therapeutic role of lorazepam when given in combination with haloperidol as a single-dose rescue to patients with refractory agitation despite scheduled haloperidol. The use of lorazepam in other combinations, populations, and indications needs to be thoroughly investigated in future clinical studies. Currently, patients with severe refractory agitated delirium often require hospitalization for control of this highly distressing syndrome. However, many patients and caregivers prefer to die at home with support from home hospice. Both lorazepam and haloperidol are available as oral medications including a rapid sublingual form of lorazepam. Further research is needed to examine if these treatment options are feasible and effective for patients with agitated delirium in the home setting. There were no significant between-group differences in multiple exploratory outcomes, such as delirium severity, delirium-related distress, and communication capacity. However, this study was not powered to examine these secondary outcomes. Specifically, there was no significant worsening of agitation, respiratory rate, or other adverse effects. A single dose of lorazepam was not associated with a shortened survival consistent with nonrandomized observational studies examining the effect of continuous benzodiazepine infusion on survival.32,33. This study has several limitations. First, this was a single-center study conducted at a tertiary care cancer center. Although the mortality rate of this acute palliative care unit is similar to other US centers34 and a majority of patients who were eligible enrolled onto this study, the study findings may not be generalizable to other settings (eg, patients earlier in the disease trajectory or those treated at home) and the external validity needs to be further assessed. Second, only a single dose of study medication was administered as rescue. Future studies will need to assess the effects of repeated dosing. Third, a single lorazepam dose of 3 mg might be too high for some patients, especially those with severe liver failure who cannot metabolize lorazepam. Further studies are needed to examine different doses. Fourth, several secondary outcomes in this study, such as the delirium recall questionnaire, require further validation. Fifth, this study had a small sample size and thus wide CIs in many measures. It was not powered to examine the multiple secondary outcomes and thus the secondary findings should be considered as exploratory. To the Editor The Viewpoint by Ms Gudbranson and colleagues calculating the number of physicians needed in the United States did not consider the numerous roles that physicians play in the health care sector.1 If administrative workload, teaching of trainees (medical students, residents, and fellows), and research are included, the estimates may be quite different. The administrative burden of billing, coding, and entering data in electronic medical records requires several extra hours of work per week. No physician can be expected to assign 100% of their time to clinical care, and although some previously published models account for clinical full-time equivalents,2 it is unclear whether Gudbranson and colleagues took this into consideration. Furthermore, models accounting for clinical full-time equivalents project a serious shortage of physicians over the next decade. A report prepared for the Association of American Medical Colleges notes that the demand for physicians in 2025 will exceed supply by 46 100 to 90 400.2 Physicians in the United States (excluding residents) work an average of 49.6 hours per week compared with 44.9 hours for lawyers, 43.0 hours for engineers, and 37.3 hours for registered nurses.3 This estimate does not take into account being on call when physicians need to be immediately available but not present on-site. To the Editor The Viewpoint by Ms Gudbranson and colleagues calculating the number of physicians needed in the United States did not consider the numerous roles that physicians play in the health care sector.1 If administrative workload, teaching of trainees (medical students, residents, and fellows), and research are included, the estimates may be quite different. The administrative burden of billing, coding, and entering data in electronic medical records requires several extra hours of work per week. No physician can be expected to assign 100% of their time to clinical care, and although some previously published models account for clinical full-time equivalents,2 it is unclear whether Gudbranson and colleagues took this into consideration. Furthermore, models accounting for clinical full-time equivalents project a serious shortage of physicians over the next decade. A report prepared for the Association of American Medical Colleges notes that the demand for physicians in 2025 will exceed supply by 46 100 to 90 400.2 Physicians in the United States (excluding residents) work an average of 49.6 hours per week compared with 44.9 hours for lawyers, 43.0 hours for engineers, and 37.3 hours for registered nurses.3 This estimate does not take into account being on call when physicians need to be immediately available but not present on-site. To the Editor The Viewpoint by Ms Gudbranson and colleagues calculating the number of physicians needed in the United States did not consider the numerous roles that physicians play in the health care sector.1 If administrative workload, teaching of trainees (medical students, residents, and fellows), and research are included, the estimates may be quite different. The administrative burden of billing, coding, and entering data in electronic medical records requires several extra hours of work per week. No physician can be expected to assign 100% of their time to clinical care, and although some previously published models account for clinical full-time equivalents,2 it is unclear whether Gudbranson and colleagues took this into consideration. Furthermore, models accounting for clinical full-time equivalents project a serious shortage of physicians over the next decade. A report prepared for the Association of American Medical Colleges notes that the demand for physicians in 2025 will exceed supply by 46 100 to 90 400.2 Physicians in the United States (excluding residents) work an average of 49.6 hours per week compared with 44.9 hours for lawyers, 43.0 hours for engineers, and 37.3 hours for registered nurses.3 This estimate does not take into account being on call when physicians need to be immediately available but not present on-site. The article only discussed 1 primary efficacy end point of the 2 phase 3 trials.2,3 It did not mention secondary efficacy end points, such as quality-of-life (QOL) measures. The second primary outcome was a responder analysis, which required a 50% or greater reduction in the mean number of nocturic episodes. The percentage of responders was 48.5% for desmopressin vs 30% for placebo (P < .001),4 a 62% increment. The studies also used the Impact of Nighttime Urination (INTU) QOL questionnaire, the first validated and FDA-accepted QOL questionnaire for nocturia. Statistically significant differences in QOL favoring Noctiva were found. The secondary efficacy end points included first period of uninterrupted sleep and percentage of nights with no or 1 nocturic void, which were all statistically significant in favor of this product. Regardless of whether we use the AMA Masterfile, the Kaiser Family Foundation estimates derived from state licensing information, or the 7-year-old AHRQ data preferred by Kruse, our results are the same—we have more than enough primary care physicians to support the US patient population. Kruse argues that there are actually 209 000 primary care physicians, which happens to be the number needed based on our conservative calculations. Adding part-time primary care physicians further supports our finding that physician supply is not a problem in primary care. The access problems faced by patients seeking primary care are more the result of how the health care system deploys these physicians—a management problem—than a sheer numbers problem. Fein and Herschkowitz overstate desmopressin nasal spray’s benefits. For example, their description of some of the positive secondary end points in the trials is incomplete. They report desmopressin’s effect on QOL as measured by the INTU questionnaire developed by Serenity Pharmaceuticals. The INTU questionnaire is scored from 0 to 100; the mean score improved by approximately 14 points in the group receiving high-dose desmopressin compared with approximately 12 points for the placebo group. This result may have been statistically significant, but this difference of about 2 points between the drug and placebo on a 100-point scale is of unclear clinical importance.2 We worry that such limitations may not be communicated adequately by Serenity Pharmaceuticals’ sales representatives in promotional statements to physicians, or will not be fully understood by patients considering use of this FDA-approved drug. Readers noticed that the excess readmission ratios were lower than would be expected. In reviewing our data, we found that we had made a coding error while conducting initial sensitivity analyses. We assigned hospitals that had an insufficient volume of patients for the given HRRP conditions as having an excess readmission ratio value of 0, rather than a value of 1. Assigning a value of 1 implies that the low-volume hospitals’ readmissions were no different than expected, which is what we intended. In contrast, assigning them a 0 implied perfect performance (no readmissions), which we did not intend and which overstates performance. In addition, in reviewing our analytic files, we discovered we had applied exclusion criteria to our sample of hospitals that were salient to earlier published work but not to this study, resulting in 163 hospitals being excluded from the analysis. Including the 163 additional hospitals had little effect on our main results. However, correcting the coding error and assigning hospitals with low volumes of patients in a particular condition category substantially changed the potential implications of this result. As such, we have requested the retraction and replacement of our original article. We have also gone back through our analysis in detail to ensure there are no other errors. The effect of including the 163 hospitals is seen in minor changes to Table 1. The correction of the miscoding of the excess readmission ratios for low-volume hospitals with a 1 instead of 0 has resulted in changes to Table 2 of substantial magnitude. In sum, the mean excess readmission ratios for safety-net hospitals were above 1 throughout the period for all conditions tracked under the HRRP (not just pneumonia and heart failure as originally reported) and did not change in a meaningful way over time (in contrast to the decrease in fiscal year [FY] 2016 originally reported). Among non–safety-net hospitals, the mean excess readmission ratio was below 1 throughout the period for acute myocardial infarction, pneumonia, heart failure, and chronic obstructive pulmonary disease, as originally reported, but changed for knee and hip arthroplasty, for which it was just above 1 in both years these procedures were included in the HRRP. Our original conclusion that the performance of safety-net hospitals on the HRRP improved between FY 2013 and FY 2016 is unchanged. However, that the improvements may have been associated with improvements in readmission rates for pneumonia and heart failure is no longer correct. Rather, as we now point out in the replacement article, “The performance of safety-net hospitals on the HRRP has improved between FY 2013 and FY 2016 as reflected in the penalties. This does not appear to be driven by meaningful reductions in the excess readmission rates at safety-net hospitals but rather may have been driven by the design of the program and addition of arthroplasty readmission to the program in FY 2015, with mean excess readmission ratios above 1 for both types of hospitals in 2015 and 2016.”. Period life tables for the US population in 2000 and 2015 were calculated to estimate life expectancy by age. Life expectancy at any given age is the average number of years of life remaining for those surviving to that age, based on observed period death rates. Changes in life expectancy at birth were partitioned into component parts using the change in the proportion of deaths from specific causes for each age group for 2000 vs 2015.4 Stata (StataCorp), version 13, was used to calculate life tables and Excel 2013 (Microsoft) for partitioning. The US Centers for Disease Control and Prevention determined the research was exempt from human subjects regulations because it used existing deidentified data. Drug-poisoning deaths increased from 17 415 in 2000 to 52 404 in 2015; the age-adjusted death rate per 100 000 population increased from 6.2 to 16.3 (difference, 10.1 [95% CI, 10.1 to 10.2]), with most of the increase (7.4 [95% CI 7.3 to 7.4]) related to opioid deaths (Table). Drug-poisoning deaths contributed a loss of 0.28 years in life expectancy. Most of this loss (96%) was unintentional; 0.21 years were lost to opioid-involved poisoning deaths. Alcohol poisoning contributed a loss of 0.02 years (Figure). Between 2000 and 2015, life expectancy increased overall but drug-poisoning deaths contributed a loss of 0.28 years. This loss, mostly related to opioids, was similar in magnitude to losses from all the leading causes of death with increasing death rates during this period combined. Nearly all the life expectancy lost due to drug-poisoning deaths was unintentional and was therefore reflected in life lost to unintentional injury. However, unintentional injury appeared to account for less life lost than drug-poisoning deaths because of counterbalancing gains related to decreasing death rates from other unintentional injuries, particularly motor vehicle crashes.3. The modifier “involuntary” is generally used to describe these cases. For example, it is said that a patient has been involuntarily hospitalized or is receiving involuntary medication ostensibly because the patient did not consent and was forced or strongly coerced into treatment. Importantly, a person may be involuntarily hospitalized but retain the right to refuse treatment. “Involuntary” is also used to describe instances when an individual is committed to outpatient treatment by a court. The fact that a person is being treated involuntarily raises numerous challenges; it raises concerns about protecting individual liberty, respect for patient autonomy, and the specter of past abuses of patients in psychiatric institutions. Although it has become both a clinical colloquialism and legal touchstone, the concept of involuntary treatment is used imprecisely to describe all instances in which a patient has refused the treatment he or she subsequently receives. In some cases, a patient outwardly refuses treatment but may have previously expressed a desire to be treated in crisis or, according to a reasonable evaluator, he or she would have agreed to accept stabilizing treatment, such as antipsychotic medication. A similar scenario occurs in the treatment of individuals who experience a first episode of psychosis and who outwardly refuse treatment. With no prior experience of what it is like to have psychosis, these patients are unable to develop informed preferences about treatment in advance of their first crisis. In these cases, some believe it is reasonable to provide treatment despite the opposition of the patient, although this could be debated. To more precisely distinguish such cases, clinicians and policy makers should begin to refer to these instances as nonvoluntary, not involuntary, treatment. Nonvoluntary treatment suggests that the patient exists in an intermediate domain of decision-making capacity and voluntariness. In this momentary refusal of care, the patient contradicts long-held values and a deeper desire to be autonomous. This nomenclature may provide additional ethical justification for treating patients who momentarily refuse psychiatric treatment and may provide nuance about challenging cases. The distinction between the concepts of involuntary and nonvoluntary has been recognized particularly in other areas of biomedical ethics, including critical care, end-of-life decision making, and clinical research.1 For example, the controversial practice of nonvoluntary euthanasia refers to cases involving a gravely ill patient who lacks capacity or the potential for capacity. Some theories of nonvoluntary treatment stipulate that no knowledge of patient values exists. In the case of psychiatric treatment, nonvoluntary treatment should be considered in a broader sense that allows for evidence of past values, whether explicitly expressed or tacitly demonstrated by a patient’s life in the community. Although evidence of previous values would seem to move such cases under the penumbra of voluntary treatment, it seems illogical to refer to any instances of forced psychiatric treatment as voluntary. The distinctions between voluntary, involuntary, and nonvoluntary treatment turn, in part, on patients’ capacity to indicate either directly or through a surrogate their wishes and values pertaining to a specific medical decision. Voluntary decisions are typically  those made by capacitated patients who are free of coercive influences. These are the decisions that are enacted in clinical or research settings in the final stage of the informed consent process. Involuntary treatments are those imposed on a person who in some way is coerced, incapacitated, or dangerous. For example, involuntary treatment is justified when a patient is found to be an imminent threat to his or her own or another’s safety, whether that patient retains decision-making capacity or not. Despite public health efforts, use of PADs has been limited. Therefore, it is more likely that historical facts and collateral testimony by case managers, family members, and primary care physicians could yield insights into the patient’s values. Perhaps even a patient’s social media content that describes the patient’s desire to live in recovery would also be admissible. Such data would inform clinicians if a patient is momentarily refusing treatment but would want treatment if he or she were stable. These are patients who, once they are stabilized, may feel as though their initial refusal had been correctly overridden.5 Such retrospective approval is not sufficient to justify nonvoluntary treatment, but it does provide assurances that nonvoluntary treatment is a worthy option.6. The distinction between involuntary and nonvoluntary treatment becomes particularly salient in the context of forced addiction treatment. Patients with addiction may understand and appreciate their condition but nonetheless continue to refuse treatment. Addiction is intrapersonally controlling; by its nature, addiction restricts an individual’s self-determination.6,7 A person with an addiction may express an outward desire to continue to use drugs but may also harbor a deeply held and arguably more authentic desire to live addiction-free. Evidence of this desire might be in expressions of regret or a longing to reconnect with alienated family members. In such cases, nonvoluntary treatment would be ethically defensible because it recognizes the higher-order values of the patient and seeks to help the patient actualize them. There is certainly a risk that the tools provided by nonvoluntary treatment may be misused or abused. For example, the preferences of individuals who are authentically refusing treatment may be violated because evidence was unclear or misinterpreted. It is not uncommon for people to hold and express conflicting values simultaneously. However, this challenge is no different than in other areas of medicine involving surrogate decision makers, substituted judgment, and advance directives. Moreover, the current strict limitations on involuntary treatment risk allowing individuals with psychiatric illness to have progressive symptoms—a phenomenon previously referred to as “rotting with their rights on”— despite compelling evidence they would want to be well.8. The semantics around mental health care—particularly involuntary treatment—are ethically challenging and lead to interminable controversy. However, an important aspect of the debate seems to be a result of the imprecise use of language when involuntary treatment is used to describe forms of treatment that would otherwise be agreed to. The concept of nonvoluntary treatment provides a more precise categorization of certain cases for which there is reason to believe, despite a patient’s outward resistance, that patient would have agreed to treatment. In other words, nonvoluntary treatments are not a violation of patient autonomy but respect and serve to restore autonomy. Determining the appropriate duration of nonvoluntary treatment is a separate and complicated matter. Psychiatric and behavioral health care professionals, policy makers, and advocates should rethink their use of the term “involuntary” as new laws are developed that include ethically acceptable but coercive forms of mental health treatment when individuals would benefit from and normally accept medical care. Charlie Gard is an 11-month-old boy who, according to court records, “suffers from a rare inherited mitochondrial disease called infantile onset encephalomyopathic mitochondrial DNA depletion syndrome.”1,2 During the several months he has been hospitalized at Great Ormond Street Hospital his condition has steadily deteriorated, and he is now dependent on life support and mechanical ventilation. His physicians believe he has no reasonable remaining treatment options, and they have recommended palliative care and withdrawal of the ventilator. Charlie’s parents, however, are asking that he be transported to the United States, where an unnamed US physician has offered to treat Charlie with nucleosides, an experimental treatment that has been used on a few children with a less severe form of the disease caused by a different mutation. The parents have raised more than $1.6 million in contributions to fund his transport and treatment.3. Several experts in various specialties, including pediatric intensive care, neurology, and mitochondrial diseases, have argued against treating Charlie with this therapy, pointing out that the treatment has not been tried on humans or even animals with the mutation causing Charlie’s disease.1 The US physician acknowledged in his testimony that his recommendation was theoretical and based on very limited evidence from a related but less severe condition caused by a different mutation. After reviewing the records, the physician noted that the damage to Charlie’s brain was more severe than he had thought. He said that the chances of meaningful brain recovery would be small, which he agreed he could not distinguish from vanishingly small. He conceded that to a large extent, if not entirely, the brain damage was irreversible. Nevertheless, he concluded that if Charlie were in the United States, he would treat him if the parents so desired and could pay for it: “I would just like to offer what we can. It is unlikely to work, but the alternative is that he will pass away.”1. At least 3 issues are involved in this case that are at the core of many of the worldwide debates regarding health care delivery: the best interest of the patient, financial considerations, and scientific validity. First, the judge in the trial court explained that the law required him to base his decision solely on what he deemed to be the child’s best interest.1 Based on testimony from the physicians, the judge ruled that continued treatment would cause Charlie pain and suffering and that, in the absence of a realistic chance of benefit, continued treatment would not be in Charlie’s best interest.1 The problem with this decision is 2-fold. First, this decision depends on an objective opinion about a subjective phenomenon, namely pain and suffering. Although the clinicians caring for this child apparently believe that he is experiencing pain and discomfort, his parents disagree and have said that they would not be insisting on continued treatment if they thought that he was. In addition, medical treatments in the intensive care unit are almost always associated with some degree of discomfort, but intensive care unit clinicians are well trained and equipped to effectively manage these symptoms. If pain and suffering were the only issue, then an alternative to treatment withdrawal would be to use a standard regimen of analgesia and sedation. If Charlie’s continued treatment were to be funded through either governmental or private insurance, then a strong case could be made that acceding to the parents’ request would be an irresponsible use of limited resources. But the notion that Charlie’s parents are paying out-of-pocket for this treatment is misleading. The very existence of tertiary care medical centers that are able to do the research and provide the care requested by Charlie’s parents depends on the vast communal investment that has been made by society in the infrastructure of our health care system over many decades. Paying the incremental expenses for the care of 1 patient in a hospital no more covers the cost of that care than paying the incremental costs for a fire truck to come to a home covers the cost of the fire department that makes the trip possible. In other words, society always has a financial stake in how communal services, and in this case health care resources, are used and has a legitimate claim in insisting that these resources be used wisely and for the benefit of all. No one can demand nonbeneficial treatments simply by claiming that they are paying out-of-pocket. Third is the commitment of the medical profession to scientific integrity. Today, some segments of society regard the claims of medical experts, and of scientists in general, not as expressions of efforts to uncover truths about the physical world, but merely as reflections of a particular set of values. Putting aside the dubious credibility of such an opinion, the historical development of the profession of medicine has been characterized by an increasingly strong commitment to grounding practice on evidence generated through research. Today, for example, it would be extremely rare for a hospital in the United States to admit patients for the exclusive purpose of receiving homeopathic therapy or unproven stem cell infusions, regardless of how much the patient paid. This does not mean, of course, that medicine does not seek to improve through experimentation. But as the concept of evidence-based medicine implies, progress occurs through careful application of the scientific method. While the most trustworthy advances come through the performance of well-designed trials, sometimes experimental treatments based on theoretical considerations alone may lead to major breakthroughs. But such successes are rare, and to be worth trying they must meet a threshold of scientific plausibility that transcends theoretical possibility. Whether the proposed nucleoside treatment meets that threshold is a legitimate matter for debate, but based on the lack of either animal or human data to support the use of this approach, as well as evidence of severe neurologic injury that could not be reversed in any case, the consensus of respected experts in the United Kingdom was that it did not. The development of a new drug is often portrayed as a series of increasingly demanding clinical trials, performed in patients with a single target condition, as the development program advances toward drug approval. However, rarely does drug development involve a single approach to using a drug. Instead, drugs are tested in multiple clinical trials—many pursued after initial drug approval and licensure—involving different clinical indications or drug combinations. This family of trials could be referred to as a drug development “portfolio.” For example, the portfolio of the anticancer drug sorafenib included 203 clinical trials conducted over 13.2 years, spanning 26 different malignancies and 67 drug combinations.1 A portfolio can be enormous if defined in terms of drug classes. For instance, at least 803 trials are testing checkpoint inhibitors for the treatment of different malignancies.2. Drugs are not tested for randomly selected indications but, in contrast, are tested for diseases thought to share some common features (eg, metabolic pathways, pathophysiology, pathogens). The result is that trials in portfolios test hypotheses that are correlated with each other; if the drug is truly effective in one tested disease state, that generally increases the chance it is truly effective in another, and vice versa. Although results from these related trials are correlated, they can also differ from each other because of true differences in efficacy in different disease states, populations, and settings, as well as random variation in observed data sets. The fact that drug testing often involves multiple and related trials has unappreciated implications for clinical decision making, human protections, and policy. The traditional interpretation of a single well-powered clinical trial as providing the most accurate estimate of a drug effect is incorrect when multiple treatment effects have been measured by drug developers in numerous trials in a portfolio and a decision maker selects a trial based on a large observed treatment effect. This happens because there are 2 sources of variability in treatment effect estimates in the portfolio. The first is the underlying heterogeneity of treatment effect due to differences in the trials (eg, populations, outcomes, diseases). The second is random variation in each of the observed treatment effects. In the case of a single trial, the traditional unit used to drive decisions in drug development, random variation is the only source of variability. However, in the context of the broad portfolio in which that trial is embedded, the difference between the observed treatment effect and the mean treatment effect of the portfolio is the sum of these 2 variances. As a consequence, the variability in the estimated treatment effects is greater than the true variability in the real treatment effects. Thus, a result selected based on a larger observed treatment effect likely represents an overestimate of the real benefit. For example, within 5 years after sunitinib received US Food and Drug Administration approval, this drug was tested for 33 different indications in monotherapy alone. One indication, endometrial cancer, showed a high objective response rate (18.1%) in an initial trial.7 This unusually high response may have reflected random variation (that is, the drug may not actually be effective for that type of cancer); there has been no follow-up trial registered on the ClinicialTrials.gov website. Ideally, overestimates of clinical effects in phase 2 studies (small trials typically using surrogate end points) would be corrected with later phase 3 trials (large, randomized trials often using clinical end points). However, regulatory authorities might extend approvals to new indications for drugs based on effects in phase 2 trials but before phase 3 trials are complete; to our knowledge, drug regulators usually do not consider outcomes from trials testing other indications in a portfolio when approving therapies for a given indication. Further, positive phase 2 studies are not always followed up immediately with randomized trials, leaving physicians to make decisions based on effects observed in single, small trials. Anomalously large effects within drug development portfolios are at high risk of overestimating treatment effects. The multiple testing of correlated hypotheses in drug development portfolios also presents liabilities for physicians and other clinicians. Just like treatment effect estimates, P values from individual tests in a portfolio are an optimistic representation of significance, especially when trials are selected to inform clinical practice or guideline recommendations on the basis of individual P values. This is because there are many ways a false-positive finding may arise in a portfolio of trials. Even though there are statistical methods that can be tailored to adjust P values to reduce the likelihood of false-positive findings,8 without awareness of these issues, sponsors, guideline developers, and physicians have made little effort to apply these methods. An important implication is that if a drug is tested in a large number of indications or combinations, some trials will show exceptionally large responses because of random variation around a small effect. Thus, with 803 checkpoint inhibitor trials being pursued, there will almost certainly be many false-positive trials that suggest clinical promise. Another example involves drugs for Alzheimer disease, for which the portfolio of current research activities testing the efficacy of disease modifying treatments exceeds 50 trials. Guideline recommendations and regulatory approval decisions are often based on single trial reports, especially when a drug is new. For example, the National Comprehensive Cancer Network recommendation of sorafenib monotherapy for osteosarcoma was informed by a phase 2 study that involved 35 patients and reported 46% progression-free survival at 4 months.9 Unless outcomes in entire portfolios are used to inform recommendations in care or policy, drug companies have incentives to exploit the effects of random variation by supporting large numbers of trials for a new drug. Another way portfolios matter for human protections involves how trials are coordinated with each other. Knowledge of other active trials in a drug portfolio can prevent redundant testing, thus sparing patients exposure to unproven treatments. For adaptive studies, information emerging from other parts of the portfolio might be considered when modifying trials midstream. Also, if hypotheses in trial portfolios are correlated, the same logic used within trials (which often use cohort staggering or interim analysis so that investigators can modify protocols and adapt to emerging safety and efficacy information) can be applied across trials. If testing of related indications within a portfolio is staggered rather than simultaneous, some trials—and their associated burdens and expenditures—can be obviated. The development of new treatments often involves testing interventions under many different conditions. Such vigorous research activities generate a wealth of information that can be used in ethical review, research planning, and clinical care. However, single or small numbers of trials showing large effects within portfolios of numerous trials are highly likely to produce exaggerated estimates of the efficacy of a drug. For settings in which clinical practice guidelines issue recommendations based on single, small trials that have demonstrated large effects, this can lead to inappropriate and costly care. When decisions about risk and benefit hinge on smaller, single trials, ethics committees, policy makers, payers, and clinicians should be aware of the potential for exaggerated estimates arising from a trial selected from larger portfolios. They should instead leverage the totality of evidence within portfolios toward decisions that better protect research study volunteers, patients, and payers. I ate only one meal in a restaurant during my five-day visit to Paris in the fall of 2011: lunch at Les Deux Magots. It took an hour. Not because I am a slow eater and not because I was in awe of the ambience but because I had tic douloureux, better known as trigeminal neuralgia. I’d take a bite, and excruciating pain would shoot up above my right eye. I thinned my sandwich by removing the top piece of bread so the bite would be smaller. I found that I could best eat by taking food to my hotel room, chewing slowly and gingerly, thinned as much as possible, while lying on my back. After I returned home, the pain got worse. It would sometimes hit when I was interviewing medical students or lecturing to residents, and I’d have to leave the room. Sometimes blinking my right eye brought it on. Once it came on out of the blue; I had no clue why, and it lasted for what seemed like an eternal five minutes. I switched neurologists and quadrupled my medications. The pain was controlled, but the cost considerable. I once forgot the name of my grandchild. I needed naps in my office. I stopped biking because I was afraid of losing my balance. Viagra-unresponsive erectile dysfunction plagued me. I cried at the least emotional insight. I scoured everything I could find about my condition, not in the same intellectual manner that I would for an unusual disease a patient of mine might have, but as a prisoner that must study a secret journal about maps of underground tunnels to freedom. One option for treatment of the faulty trigeminal nerve is to jab a long needle up through the face and through a hole in the skull to inject glycerol around the nerve. Another option is to radiate the nerve with gamma rays. The most invasive treatment is neurosurgery. Through a quarter-sized hole behind the ear, the surgeon delicately pulls aside a chunk of cerebellum, picks a dicey route around the seventh and eighth cranial nerves, finds the trigeminal nerve using an operating microscope, and puts a Teflon pledget between the artery (whose pulsations irritate the trigeminal nerve) and the nerve. I chose the surgery, “microvascular decompression,” as it’s called. Although surgery was riskier than the other options, it was least likely to result in anesthesia dolorosa, a very painful condition resulting from permanent damage to the trigeminal nerve for which there is no effective treatment. I remember waking in the operating room, asking if the compressing artery had been found. Reassured that it had, I fell asleep. Postoperatively, I was quickly weaned off the high doses of medicines. I have had no recurrence since the surgery. Hemingway, a famous patron of Les Deux Magots, had an idea that has come to be known as the iceberg theory, that seven-eighths of the reality of any given event lies beneath the surface. For me, the scientific description of the cause of trigeminal neuralgia and the scholarly attempt to describe the pain was the top of the iceberg, the part that could be written down. But there was much more to the condition than pain, and that was the more sinister seven-eighths. With the surgery behind me, I remained debilitated by anxiety that the pain would return. Even now, 5 years later, if I wake at night with a twinge of discomfort in my right eye, I’ll lie awake worrying that the tic has returned. I weigh in my mind what treatment option I would choose if the pain returns: another brain operation (wouldn’t that be risky, going through the scars of the previous cutting?) or the “zap” (too much radiation might cause terrible facial pain that medicines couldn’t stop; not enough would leave me with pain) or the needle (hope the hand that wields it is steady, doesn’t hit the carotid artery). To my colleagues, I probably seem back to my usual self; they don’t know I took Paxil for more than a year after the surgery. Some good has come from all this suffering. I am now more aware of how pain dominates my patients’ lives and drives their decisions. In my cardiology world, I focus much more on trying to prevent heart attacks and sudden cardiac death than managing the actual symptoms my patients live with. As a patient, pain and the anticipation of pain were more pressing concerns than the risks cited for my operation: death, stroke, a chronic leak of cerebrospinal fluid, and permanent hearing loss. With my patients, I now realize that there may be times when these concerns trump even the fear of dying. For me, what was once head knowledge is now heart knowledge. Pain also affects autonomy. Although I had scientific training and easy access to the latest literature on trigeminal neuralgia, I had trouble weighing the risks and benefits of each treatment option. At first, I was in too much pain. Then, when the medicines dampened the pain, I was too blunted cognitively to make rational personal decisions. I sometimes thought my doctors assumed I had the scientific insight of a neurologist or that they wanted to be careful not to insult my knowledge as a colleague. They didn’t seem to understand that what I wanted was a recommendation: zap, or needle, or cut. Of course I could choose what treatment I wanted, but the basis for making that decision seemed flimsy. The entire experience made me think more about how I recommend treatment to my patients. I am less swayed by what I think they know. A fellow cardiologist as a patient may require as thorough an explanation of his condition as the chef. I also empathize with a patient who says he’s anxious, and I accept at face value reasons for anxiety that I once belittled. I sometimes wonder if the etiology of trigeminal neuralgia is a metaphor for life. Tic douloureux is thought to be caused by an artery lying too close to the trigeminal nerve and hammering away for long enough that it damages the myelin sheath of that nerve. When the nerve receives the sensation of chewing or the touch of a kiss, those signals mutate into severe pain. So it is with life, perhaps. The pressures and stresses—little and big—pound away, to the point our nerves fire our brains with physical pain or mental anguish. Perhaps Hemingway had a kind of emotional trigeminal neuralgia. He took his own life, as did some people suffering from trigeminal neuralgia, before effective treatments were available. Transitions are a way of life in academic medicine. While much has been written about these changes,1,2 less attention has been devoted to a more common shift: the move between academic work and clinical ward time.3 This gap has important consequences. For example, some faculty dread upcoming ward blocks. When on clinical rotations, they struggle to juggle patient and academic activities—often doing neither well. The resulting perceived lack of efficiency and effectiveness leads to weariness, fatigue, frustration, and symptoms of burnout.4 Regardless of clinical effort, most faculty will face this problem. Unfortunately, limited guidance on how best to overcome this common challenge exists. Clinical service blocks are almost always known ahead of time. Use this information to plan for (and around) clinical activities. For example, one month before wards, project out your workload, administrative, research, or other activities knowing that you will not be available to pay day-to-day attention to these when you are on service. Ask, “What needs to be done now?” and “What can wait for later?” This assessment of priority is especially important for physician scientists who face deadlines related to grants, presentations, or manuscript resubmissions. Submit papers before you go on wards, ensure that major portions of analyses or reports for projects and grants are done ahead of time, and delegate work to others when possible. Don’t forget family and those you love: specifically, maximize quality time with them before service. In the words of a former chief of medicine: “When your career is over, you will have many people who will remember you fondly. Make sure they include your family.”6. Attending on the wards is a full-time job. As obvious as this sounds, avoid meetings related to nonclinical activities when on service. If meetings are absolutely necessary, schedule them after 3 pm. Let conveners know you are on a clinical rotation, so you have limited time and may need to leave unexpectedly. This maxim also holds true for work on committees, conference calls, or other assignments outside of clinical duties. Your primary responsibility when on service is patient care. Do that well. Failure to do so may permanently damage your reputation or cause something far worse for those whose care is entrusted to you. Think of clinical wards as a break from all your other activities—treat it as such. Set and communicate realistic expectations about your ability to honor commitments when on service. This will help preserve not only the quality of your work but also your sanity. Ask for extensions on deadlines for manuscript revisions if you cannot submit before wards. Tell your collaborators you will be on clinical duties. Ask them to reach out either before or after you finish service for project or grant-related matters. Similarly, let your mentor know you will be on wards—a simple act that highlights your foresight and absolves you from delays in work. Reciprocate this kindness by affording others the same allowances when they are on service. After all, turnabout is fair play. It is tempting to try to answer every email when on service to keep the trains running on time. While this is sometimes necessary, responding to messages may trigger an avalanche of responses that become difficult to manage. Rather, we suggest practicing what we call “email hygiene.” Set an out-of-office reply that lets people know you are on clinical duties and will not answer straightaway. Let go of checking your email frequently and the urge to answer messages. Rather, consider instituting an email triage system—put things that do not need immediate attention into a “Post Service” folder. This sorting ensures that messages don’t get buried in your in-box and are easily retrievable when the time is right. If these recommendations make you think you can stop doing all activities while on wards, think again. These tips ensure you concentrate on what you are doing—to be present in the moment. However, you should plan for who will manage activities and keep projects moving forward when you are on service. Tasks like getting manuscripts circulated, completing analyses, or formatting sections of grants should be delegated to appropriate individuals so that the work continues. Submit manuscripts before going on service, or circulate drafts of slides or grants so that feedback (which often takes a week or two) is ready on your return. In other words, use clinical time to your advantage. A key component of “resurfacing” after clinical duties is building in time to catch up. We recommend blocking at least half to one day a week for a few weeks after you finish service for this purpose. This day becomes an oasis where edits to manuscripts, grants, or analyses can be resumed. It also serves as a day to sift through emails in your “Post Service” folder or address tasks you had prioritized for later. Also, structure decompression time for yourself after a hectic two or four weeks of wards. Take an outdoor walk, catch up with sleep, or grab coffee at your favorite coffee shop with collaborators and friends. Take care of yourself, not just the work. Many faculty view their upcoming ward assignment with trepidation: the tunnel at the end of the light, if you will. They struggle because they either take on too much, plan poorly, or both. While these tips may sound overly simplistic, we have found them to be effective and enduring. By consistently using these modest fixes, we hope you will come to appreciate that the tunnel is actually well lit. And you have more control than you think. We compared PTTR using an unpaired t test. Linear regression was used to test for an interaction between PTTR and target INR. For time to event analyses, we censored participants at the time of withdraw or loss to follow-up or 30 days after arthroplasty (whichever came first). For the time to therapeutic INR analysis, patients who had fewer than 24 days of INR monitoring were censored on the day of their last measured INR. We compared the number of days until an INR exceeded the target INR by 1.5 using the log-rank test and the Cox proportional hazards model. For the Cox models, we confirmed the proportional hazard assumption by verifying that there was no interaction between predictor variables and time. All statistical tests were 2-sided. Statistical analyses were conducted using SAS analytical software version 9.4 (SAS Institute Inc) and R version 3.3.1 (R Project for Statistical Computing). Eighty-seven of 808 participants (10.8%) in the genotype-guided group and 116 of 789 participants in the clinically guided group (14.7%) experienced at least 1 composite end point, corresponding to an absolute risk difference of 3.9% (95% CI, 0.7% to 7.2%; P = .02). The results for the genotype-guided dosing group were similar in the mixed model (P = .02). The rate difference for individual adverse events was 0.8% (95% CI, –0.2% to 1.8%) for major bleeding, 2.8% (95% CI, 0.1% to 5.6%) for INR of 4 or greater, and 0.7% (95% CI, –1.3% to 2.8%) for VTE (Table 3). None of the participants died. This study has several limitations. First, although participants and study personnel were blinded to study group and to genotype, the warfarin dose was open label. Therefore, study personnel may have been able to infer the study group, particularly in participants who only rarely needed dose adjustments. However, the warfarin dosing algorithms used in both of the study groups adjusted for many factors so dose estimates varied widely among patients in both the genotype-guided group and the clinically guided group. As a further protection against bias, the end points were adjudicated without knowledge of study group or genotype. Second, the 3.9% absolute reduction in the primary outcome (death, major bleeding event, INR ≥4, or VTE) was primarily related to differences in rates of INR of 4 or greater (Figure 2). The 1.4% reduction in symptomatic major clinical adverse events (major bleeding, symptomatic DVT, or pulmonary embolism) did not achieve independent statistical significance (P = .051). Likewise, the risk of an INR exceeding the target INR by 1.5 or greater was not significantly reduced during the 90 days of follow-up (P = .08). Third, in this multicenter trial, most participants were enrolled at high-volume academic medical centers, which may limit generalizability. However, genotype-guided warfarin dosing may be more beneficial at low-volume hospitals, which may have higher rates of adverse events.30,31. The x-axis depicts the common odds ratio (OR) for a better outcome over all 7 levels of the modified Rankin Scale score (mRS), derived from ordinal logistic regression. ORs greater than 1 indicate that a good outcome (low mRS) is more likely with oxygen than with control (reference category). The size of the markers reflects the total sample size in each subgroup, with larger markers indicating more precise estimates. The subgroup thresholds for oxygen concentration at randomization were revised from the prespecified thresholds because the analysis did not converge using the prespecified values. SSV indicates Six Simple Variables risk score; COPD, chronic obstructive pulmonary disease; GCS, Glasgow Coma Scale. This was a multicenter randomized clinical trial of oxygen supplementation with single-blind outcome assessment. The protocol and statistical analysis plan (Supplement 1 and Supplement 2),16,17 and data collection forms18 are published. Fully informed written or witnessed oral consent was given by the participants or, if they did not have capacity to consent, by a legal representative. The protocol was approved by the North Staffordshire Research Ethics Committee (06/Q2604/109). Participants were allocated 1:1:1 via central web-based minimized randomization19 to (1) continuous oxygen supplementation; (2) nocturnal oxygen supplementation only; or (3) no routine oxygen (control). The factors for which imbalances were minimized were the Six Simple Variables prognostic index for independent survival at 6 months20 (cutoffs: ≤0.1, >0.1 to ≤0.35, >0.35 to ≤0.70, >0.70), oxygen treatment before randomization (yes, no, unknown), baseline oxygen saturation on air (<95%, ≥95%), and time since stroke onset (cutoffs: ≤3, >3 to ≤6, >6 to ≤12, >12 to ≤24, >24 hours). Stroke onset was defined as the last time well for wake-up strokes. No blocking was used. Oxygen was administered per nasal tubes either continuously (day and night) during the first 72 hours after randomization or overnight (21:00 hours to 07:00 hours) for 3 nights. Oxygen was given at a flow rate of 3 L/min if baseline saturation was 93% or below or at a flow rate of 2 L/min if baseline saturation was greater than 93%. In the control group, no routine oxygen supplementation was given. Outcomes were assessed at 1 week by a member of the local research team and at 90 days via postal questionnaire. Telephone interviews were conducted with nonresponders or to clarify unclear or missing answers. Quiz Ref IDThe primary outcome was the modified Rankin Scale (mRS) 21 score (disability range, 0 [no symptoms] to 6 [death]; minimum clinically important difference 1 point) assessed at 90 days. Secondary outcomes were number of participants with neurological improvement (≥4-point decrease on the National Institutes of Health Stroke Scale [NIHSS])22,23 between randomization and day 7, the highest and lowest oxygen saturations within the first 72 hours, and mortality at 1 week. Further secondary outcomes at 90 days were mortality, number of participants alive and independent (mRS ≤2), number of participants living at home, Barthel Index activities of daily living (ADL) score,24 quality of life (EuroQol [EQ5D-3L]) score,25 and Nottingham Extended Activities of Daily Living score.26 For the NIHSS and Barthel Index, deaths were recorded as the worst outcome on the scale.27 Participants, their physicians, and local research staff who recorded the 1-week outcomes were not blind to the study interventions. Ninety-day assessments were undertaken by the SO2S study office, which was blind to treatment allocation. The mRS was analyzed by ordinal logistic regression, which yields a common odds ratio (OR) for a move from one level to the next better (lower) level with an OR more than 1.00 indicating an improvement. For this and other outcome variables, a primary unadjusted analysis and a secondary covariate-adjusted analysis were performed. Adjusted analyses incorporated the following covariates: age, sex, baseline NIHSS score, baseline oxygen saturation, and the Six Simple Variables prognostic index for 6-month independence (or for analysis of mortality, the Six Simple Variables prognostic index for 30-day survival). Sensitivity analysis for the mRS used multiple imputation of missing values (using a chained equations method with 20 imputed data sets). Additional imputations were performed to allow for the possibility that data were missing not at random and were either better or worse than expected; missing values were thereby replaced by either very good (ie, lowest) or very poor (ie, highest) scores on the mRS as appropriate (eTable 3 in Supplement 3). Subgroups, for the mRS only, were analyzed by an interaction term and were predefined in the statistical analysis plan.17. For continuous outcomes, means and standard deviations or medians and interquartile ranges (IQRs) are reported, as appropriate. Unadjusted analyses used unrelated t tests, with the mean difference between treatments and corresponding CIs reported. The adjusted analysis used analysis of covariance, with the covariates specified earlier included in the analysis. For dichotomous outcomes, percentages were compared across the treatment comparisons using a χ2 test (unadjusted analyses). Adjusted analyses of dichotomous outcomes used binary logistic regression, with the covariates listed earlier; ORs and CIs are reported. All analyses were by intention to treat, ie, according to the treatment group to which participants were allocated, irrespective of treatment actually received. Statistical significance was set at a P value of less than or equal to .05 with 95% CIs for the primary outcome and at a P value of less than or equal to .01 with 99% CIs for secondary outcomes. All reported P values are 2-sided. The main analysis was performed in SAS software for Windows, version 9.4 (SAS Institute Inc), and IBM SPSS for Windows, version 22 was used for sensitivity analyses. A total of 8003 participants from 136 collaborating centers in the United Kingdom were randomized and followed up between April 24, 2008, and January 27, 2015, (Figure 1). Baseline demographic and clinical characteristics, including stroke severity and oxygen saturation at randomization, were well-balanced in the 3 groups (Table 1). The mean (SD) age of participants was 72 (13) years, 4398 (55%) were men, and 7332 (92%) could undertake activities of daily living independently before the stroke. The mean (SD) NIHSS score was 7 (6) and the median score was 5 (IQR, 3 to 9). Prior to randomization, oxygen had been given to 1601 (20%) participants either in the ambulance or in the hospital. Patients were enrolled at a median of 20:43 hours (IQR, 11:59 to 25:32 hours) after symptom onset. The mean (SD) oxygen saturation at randomization was 96.6% (1.7%). All participants had a clinical diagnosis of stroke at the time of enrollment. The final diagnosis at 7 days was ischemic stroke in most cases (n = 6555; 82%), 588 (7%) had a primary intracerebral hemorrhage, and 294 (4%) were strokes without computed tomography diagnosis. There were 168 (2%) participants who were given a final diagnosis of transient ischemic attack, and 292 (4%) were found to have other nonstroke diagnoses with missing data in 106 (1%). Adherence was similar in the continuous oxygen group (2158 [81%]) and the nocturnal oxygen group (2225 [83%]), all of whom were prescribed the full course of treatment (eTable 2 in Supplement 3). Use of oxygen was discontinued prematurely among 433 (16%) participants in the continuous oxygen group and 361 (14%) in the nocturnal oxygen group. The most common reason for early discontinuation of oxygen was discharge from the hospital. In the control group, trial oxygen was recorded as being given to 33 (1.2%) participants, with no recording of whether oxygen was given among 406 (15%). Quiz Ref IDThe primary analysis demonstrated that oxygen supplementation did not significantly improve functional outcome at 90 days (Figure 2). The unadjusted OR for a better outcome (lower mRS) was 0.97 (95% CI, 0.89 to 1.05; P = .47) for combined oxygen vs control, and 1.03 (95% CI, 0.93 to 1.13; P = .61) for continuous oxygen vs nocturnal oxygen. Secondary analyses adjusted for age, sex, baseline NIHSS score, baseline oxygen saturation, and the Six Simple Variables prognostic index yielded very similar results for the combined oxygen group vs control (OR, 0.97 [95% CI, 0.89 to 1.06]; P = .54) and for continuous oxygen vs nocturnal oxygen (OR, 1.01 [95% CI, 0.92 to 1.12]; P = .81). With similar numbers of missing responses in the 3 groups (continuous oxygen, n = 101; nocturnal oxygen, n = 106; and control, n = 119), findings were much the same in sensitivity analyses using multiple imputation or analyzing only participants who adherered to protocol (eTable 3 in Supplement 3). Analyses of secondary outcomes also showed no benefit from oxygen (Table 2). Neurological impairment at 1 week improved from baseline to the same degree in all 3 groups with median NIHSS scores of 2 (IQR, 1 to 6) by 1 week. Oxygen treatment did not increase the number of participants who were alive and independent or back in their home, the ability to perform basic (Barthel Index) or extended (Nottingham Extended Activities of Daily Living) activities of daily living, or quality of life (EuroQol-5D-3L) at 90 days. The results remained unchanged after adjustment for baseline prognostic factors (eTable 4 in Supplement 3). Mortality (Figure 4) was similar in the oxygen (both groups combined) and control groups (hazard ratio [HR], 0.97 [99% CI, 0.78 to 1.21]; P = .75), and for continuous oxygen vs nocturnal oxygen (HR, 1.15 [99% CI, 0.90 to 1.48]; P = .15). In this clinical trial of patients with acute stroke, routine prophylactic low-dose oxygen supplementation did not improve outcome among patients who were not hypoxic at baseline, whether oxygen was given continuously for 72 hours or at night only. This applied to the primary 90-day functional outcome and to all other tested outcomes, including early neurological recovery, mortality, disability, independence in basic and extended activities of daily living, and quality of life. The results remained unchanged in analyses adjusted for baseline prognostic factors and in sensitivity analyses using multiple imputation or analyzing adherers only. Quiz Ref IDSubgroup analyses did not identify any characteristics that would make a patient more likely to benefit from oxygen treatment (includes enrollment between 3 to 6 hours after stroke onset, patients with a lower baseline oxygen saturation, severe strokes, a reduced level of consciousness, and a history of heart failure or lung disease [ie, characteristics for which benefit from oxygen was most anticipated]). Because of the large overall size of this trial, these patient subgroups were each sufficiently large for the lack of observed benefit to be likely real and not a false negative. This study has several limitations. Minor benefits from oxygen treatment might have been masked by poor adherence. However, this seems unlikely given the high statistical power to detect even small improvements. Moreover, sensitivity analyses did not show better outcomes in the adherers-only group (eTable 3 in Supplement 3). Furthermore, this trial found significant increases in the oxygen saturations in the treated groups compared with the control group. Patients with acute stroke are often restless and confused. Ensuring full adherence would ideally require a 1 to 1 nurse-to-patient ratio. However, this is not possible outside an intensive care setting. The main outcome was assessed by postal questionnaire and supported by telephone interviews with nonresponders. This method has been used successfully in large pragmatic trials32,33 but has been replaced by remote multiple-rater video-recorded interviews or in-person interview and examination by an allocation-blinded rater using formal structured assessments in several more recent studies.34 Low-dose oxygen supplementation may not be sufficient to prevent severe desaturations; both the SOS Pilot15 and this trial found no significant difference in severe desaturations between the treatment and control groups. A small (N = 46) nonrandomized study comparing high-flow oxygen treatment via mask with low-flow supplementation via nasal cannula showed a trend toward lower mortality with high flow that was not statistically significant. However, evidence from randomized trials of high-flow oxygen treatment in acute stroke11-13 does not show that higher doses of oxygen are associated with better outcomes. Early administration of high-dose oxygen might help maintain the viability of the ischemic penumbra and allow a broader time window for neuroprotection or thrombolysis. This question was not addressed in this trial of prophylactic oxygen, but will be tested in the PROOF trial.35. The median time from stroke onset to randomization in this trial was 20 hours, 43 minutes. However, 101 participants were enrolled early (within 3 hours of symptom onset). Subgroup analysis (Figure 3) showed a similar lack of effect for oxygen in the small subset of patients enrolled early as in those enrolled later but was underpowered. Larger trials in the early time window would be needed to definitely exclude a benefit. Additional Contributions: I thank and acknowledge the contributions of Eric D. Peterson, MD, MPH, and Derek C. Angus, MD, MPH (Associate Editors, JAMA), Philip Greenland, MD, and Mary McGrae McDermott, MD (Senior Editors, JAMA), Phil B. Fontanarosa, MD, MBA (Executive Editor, JAMA), and Annette Flanagin, RN, MA (Executive Managing Editor, JAMA and the JAMA Network). Over the last 2 decades, the genetic causes of variation in warfarin response have been extensively studied. Clinically the most important are variants in CYP2C9, VKORC1, and CYP4F2.1 Although these variants contribute relatively little to variability in INR, they are important determinants of steady-state dosage. The principle hepatic enzyme in CYP2C9 metabolizes the S stereoisomer of warfarin, which is the more potent component of the racemic mixture. Eighteen variants in CYP2C9 are associated with reduced enzyme activity. The CYP2C9*2 and CYP2C9*3 alleles most commonly occur among individuals of European ancestry. A meta-analysis of observational studies demonstrated associations between these 2 variants with lower mean dose and risk of bleeding due to greater risk of poor INR control.2,3 The CYP2C9*5, CYP2C9*6, CYP2C9*8, and CYP2C9*11 alleles are more frequent variants among individuals with African ancestry and are associated with lower warfarin dosages necessary to achieve therapeutic INR values.1 The variant VKORC1 encodes the target enzyme of warfarin and a single common variant (c-1639G>A) affects warfarin sensitivity.4 The CYP4F2*3 variant (CYP4F2 is a liver vitamin K oxidase) is associated with a higher warfarin dose requirement necessary to achieve therapeutic INR values among those of European or Asian ancestry.5. In all 3 trials,9-11 the most common indications for anticoagulation were either atrial fibrillation or venous thromboembolism (VTE). Two trials compared a genotype-guided dosing algorithm with a clinical algorithm but neither found a statistically significant difference in PTTR at their respective primary end points of 4 or 12 weeks.10,11 The third trial compared a genetic algorithm against a standard loading dose regimen, which is probably more representative of usual care.9 In this trial, there were significant benefits from the genotype-based regimen in terms of PTTR, fewer cases of elevated INR, and shorter time to reach the therapeutic range. None of these trials had sufficient power to detect more clinically meaningful outcomes such as overall drug-related mortality, bleeding, or thrombotic events. In a subsequent meta-analysis that combined data from 9 trials involving 2812 participants comparing genotype-based algorithms with clinical algorithms,12 no evidence was found for a significant effect between genotype-based algorithms and PTTR, excessive INR rates, or bleeding or thrombotic events. In this issue of JAMA, Gage and colleagues13 report the results of the Genetic Informatics Trial (GIFT) of Warfarin to Prevent Deep Vein Thrombosis. This 2 × 2 factorial trial of patients aged 65 years or older undergoing hip or knee arthroplasty compared genotype-guided dosing of warfarin (n = 808) vs a clinically guided dosing algorithm (n = 789), and also tested 2 different INR target ranges (either 1.8 or 2.5). Only the results of the genotype-guided dosing algorithm are reported in this article. GIFT is the largest trial to date in this area and applied a slightly larger genetic panel than previous trials examining CYP2C9*2 and CYP2C9*3, VKORC1, and CYP4F2. The more frequent CYP2C9 variants among people of African ancestry were not included in the panel (only 6.4% of the 1597 participants were black) and randomization was stratified by race and site and type of arthroplasty (hip or knee). The modified warfarin dosing regimen extended to the first 11 days compared with 4 to 5 days in previous trials.9-11 The trial was powered for a composite primary outcome of INR of 4 or greater, major bleeding, or death within 30 days, or VTE (including objective assessment for asymptomatic VTE) at 60 days. There was a statistically significant reduction in the composite end point in the genotype-guided dosing group (87; 10.8%) compared with the clinically guided dosing group (116; 14.7%) (absolute difference, 3.9% [95% CI, 0.7%-7.2%]; relative rate, 0.73 [95% CI, 0.56-0.95]). This difference was predominantly due to the between-group difference in episodes of elevated INR (56 in the genotype-guided group vs 77 events in the clinically guided group); however, the direction of the effects on major bleeding events (2 vs 8, respectively) and VTE (33 vs 38 events) were consistent with the original hypothesis. There was a difference in combined major and clinically relevant nonmajor bleeding events (57 in the genotype-guided group vs 74 in the clinical group); however, the trial was not powered to detect such differences in single clinical outcomes. Although there is statistical uncertainty about the magnitude of this effect, this size difference could potentially be of clinical importance. Patients in the genotype-guided group had significantly higher PTTR (54.7% vs 51.3% in the clinically guided group) and this effect was greater in those with a high-risk genotype. Therefore, these data are consistent with a reduction in adverse events from a genotype-guided dosing regimen at initiation of warfarin due to improvements in INR control. There are some important issues of external validity to consider. First, the GIFT trial included older patients (mean age, 72.1 years) undergoing hip or knee arthroplasty who were at higher risk of VTE than patients with atrial fibrillation. Therefore, the VTE event rate was higher in the GIFT trial (4.4% of the total cohort) compared with the COAG or EU-PACT trials (with VTE rates of 0.9%,10 1.8%,11 and 0.2%,9 respectively). Second, the elective nature of arthroplasty meant that the genotype results were available before initiating warfarin in all but 1 participant in the GIFT trial. This is an important practical challenge of implementing pharmacogenomic testing prior to anticoagulation, especially in the treatment of VTE. In the COAG trial, only 45% of patients were genotyped in time to inform the first dose of warfarin. This is likely to be a greater practical barrier outside trial settings, although perhaps not for hip or knee arthroplasty, which are elective procedures. Third, none of the trials to date have included the CYP2C9 variants that are more frequent in people of African ancestry. The generalizability of the GIFT trial to this population remains uncertain but genotype-based algorithms should incorporate these variants if they are to be applied to patients of African ancestry. Fourth, the GIFT trial was conducted at academic medical centers and used a clinical dosing algorithm as comparator. Both of these factors likely resulted in lower adverse event rates in the comparison group than would be expected in other clinical settings with less-intense INR monitoring or empirically based initiation regimens. It is possible that larger effect sizes would be seen in other clinical settings. Of the 4.6 million deaths due to chronic respiratory diseases during the study period, 85% were attributable to chronic obstructive pulmonary disease (COPD). During this period, COPD increased from being the fourth to the third leading cause of death in the United States, surpassing stroke.4 In 2014, the age-standardized mortality rate from COPD was 45.1 deaths per 100 000 population. Age-standardized mortality rates from COPD decreased among males and increased among females from 1980 to 2014, although these rates remain slightly higher among males.2 As the authors noted, much of the observed county-level and spatial variation seen with deaths due to COPD is likely related to current and historical tobacco use, which remains the most important risk factor for COPD in the United States.5 Many of the counties with both a higher mortality rate and an increase in that rate during this period correspond to those areas that include low-income white individuals in Appalachia and the Mississippi Valley, as described by Murray et al6 in the Eight Americas project. These areas also correspond to those counties in which life expectancy for US women actually decreased from 1983 through 1999, suggesting that COPD may have been a factor in the mortality increase.7. In this study, more than 157 000 deaths were attributable to asthma during the study period, with overall mortality rates decreasing by 46% in this period. In 2014, the age-standardized mortality rate from asthma was 1.2 deaths per 100 000 population. The maps of asthma mortality (Figure 3 in the article by Dwyer-Lindgren et al2) suggest that the age-standardized mortality rates from asthma appear to be the “photographic negative” of the map presenting the change in asthma mortality between 1980 and 2014. This suggests that the areas with higher mortality rates did not benefit from factors associated with reduced asthma mortality in other parts of the country. Although race data were not available in this analysis, asthma mortality rates in the United States have been higher among black individuals than among white individuals.10 The areas with the highest asthma mortality in the article by Dwyer-Lindgren et al2 correspond to the areas of southern, low-income, rural, black individuals in the Eight Americas project.6 The overall decrease in asthma mortality between 1990 and 2014 is encouraging, particularly given that asthma prevalence increased in the United States during this period.11. The reasons for these findings are likely multifactorial, including a combination of therapeutic and public health interventions. For example, the National Asthma Education Program expert panel report guidelines for the diagnosis and management of asthma were published in 199311 and fundamentally altered the recommended approach for clinicians to treat asthma, stressing the importance of anti-inflammatory agents over bronchodilators alone. Since then, more therapeutic options have become available for patients. In addition, public health interventions such as ordinances limiting smoking and the implementation of the National Asthma Control Program, launched by the Centers for Disease Control and Prevention (CDC) in 1999, may have played a role. Further interventions will need to better target the areas that have lagged behind. For example, the CDC currently funds asthma control programs in 24 states, but not in Arkansas, Alabama, South Carolina, North Carolina, and Virginia, which are among the states that both have higher asthma mortality and have lagged behind the rest of the country in the decline of deaths due to asthma from 1980 to 2014.12. From 1980 through 2014, 21 592 deaths were attributable to coal workers’ pneumoconiosis, which occurs only among coal miners—particularly those working underground. The mortality data from coal workers’ pneumoconiosis (Figure 6 in the article by Dwyer-Lindgren et al2) align with the location of underground coal mines. The authors note that deaths from coal workers’ pneumoconiosis have declined 85% since 1980, but the prevalence of coal workers’ pneumoconiosis, particularly in central Appalachia, has increased in recent years.13 In 2014, the age-standardized mortality rate from coal workers’ pneumoconiosis was 0.08 death per 100 000 population. The Mine Safety and Health Administration recently reduced the coal dust standard (the amount of coal dust that is allowed in the air the miners breathe underground) from 2 mg/m3 to 1.5 mg/m3 in an effort to further reduce the incidence of coal workers’ pneumoconiosis.14 Measurements to evaluate compliance with this standard require the use of particle-counting devices; these measurements do not allow for an estimation of the concentration of crystalline silica in the dust. At least some of the increased incidence of diagnosed coal workers’ pneumoconiosis may actually be related to silicosis caused by exposure to crystalline silica particles.15 The counties that have had increased mortality rates from coal workers’ pneumoconiosis over 35 years (Figure 6 in the article by Dwyer-Lindgren et al2) are not associated with the location of coal mines. A possible explanation is that this reflects the migration of miners to more attractive retirement locations and the subsequent development of coal workers’ pneumoconiosis. The 15 163 deaths due to asbestosis from 1980 through 2014 represent approximately a quarter of all deaths due to pneumoconiosis. In 2014, the age-standardized mortality rate from asbestosis was 0.17 death per 100 000 population. The counties with high asbestosis mortality rates (Figure 5 in the article by Dwyer-Lindgren et al2) are associated with particular industries known for high levels of occupational asbestos exposure. Occupations most commonly associated with asbestos exposure include construction (carpenters, electricians, plumbers, roofers), automotive repair, and certain manufacturing workers, but these occupations are widely dispersed across the nation. The coastal counties with the higher asbestos mortality rates represent areas where shipbuilding and petrochemical industries are concentrated. For example, the county in northern Vermont with higher mortality rates is the location of the last asbestos mine and mill that had been operating in the United States until closure of this facility in 1993. Figure 5 also shows that counties surrounding Libby, Montana, had higher asbestosis mortality. Libby is the site of a vermiculite mine that was contaminated with tremolite asbestos.16 Because asbestos utilization in the United States peaked in 1973 and then abruptly declined through the 1980s due to efforts to restrict its use, the increasing rates of death due to asbestosis in some of these counties likely reflect the latency between exposure and manifestation of asbestos-related diseases. While silicosis was responsible for only 4529 deaths during the study period, its mortality pattern (Figure 7 in the article by Dwyer-Lindgren et al2) was among the most intriguing. In 2014, the age-standardized mortality rate from silicosis was 0.022 death per 100 000 population. Silicosis is caused by extremely high or long-term occupational exposure to crystalline silica particles. The declining mortality rates for silicosis since 1980 are a testament to greater industrial control of dust exposures. The counties that have evidence of higher silicosis mortality rates correspond to the location of hard-rock mines in Western counties, granite production in the Northeast, and hydraulic fracking operations in Texas and some Eastern counties. The Occupational Safety and Health Administration and the National Institute for Occupational Safety and Health issued a hazard alert in 2012 because National Institute for Occupational Safety and Health investigations showed that the high quantities of sand used during hydraulic fracking led to high worker exposures to crystalline silica.17 Large-scale fracking operations in the United States began in Texas in the late 1980s, and there is typically a 1- to 2-decade lag from first exposure to diagnosis of silicosis, which may explain the increasing rates of death due to silicosis in the Texas region by 2014. The findings presented by Dwyer-Lindgren et al2 cover the gamut of chronic respiratory disease causes of death ranging from the rare (silicosis) to the third leading cause of death in the United States (COPD), and provide some insights about where interventions should be focused. For example, pneumoconioses should be nearly completely preventable by occupational safety measures, and these data highlight areas where efforts need to improve. For asthma, these data demonstrate success in reducing mortality over 35 years in most, but not all, parts of the country; future success will require focusing interventions in the counties that have lagged behind in the overall mortality decrease. For idiopathic interstitial lung diseases, including pulmonary sarcoidosis, for which mortality is increasing, further research into pathogenetic mechanisms will be essential to lead to breakthroughs in prevention and treatment. For COPD, the high and increasing mortality rate poses a particularly pressing challenge that can be met only by major investments in both prevention—including elimination of smoking and harmful occupational exposures, as well as research into early life factors leading to COPD in later life—and the development of disease-modifying treatments to prevent progression to disability and death. Delirium remains a clinical diagnosis, and the condition is easily overlooked.1 Recognition is based on brief cognitive screening and careful bedside observation of key features. The current reference standard diagnostic criteria are the Diagnostic and Statistical Manual of Mental Disorders (Fifth Edition) (DSM-5) from the American Psychiatric Association5 and the International Statistical Classification of Diseases and Related Health Problems, Tenth Revision from the World Health Organization.6Quiz Ref ID Key diagnostic features, derived from the DSM-5 and the widely used Confusion Assessment Method (CAM),7,8 include an acute onset and fluctuating course of symptoms, inattention, impaired level of consciousness, and disturbance of cognition indicating disorganization of thought (eg, disorientation, memory impairment, or alteration in language) (CAM algorithm in eFigure 2 in the Supplement). Other features supportive of the delirium diagnosis include alterations in sleep-wake cycle, perceptual disturbances (eg, hallucinations or misperceptions), delusions, inappropriate or unsafe behavior, and emotional lability.7 Delirium includes both hypoactive and hyperactive forms. The hypoactive form is more common among older persons, often goes unrecognized, and is associated with higher rates of complications and mortality.9,10. The cornerstone of diagnosis is determining the patient’s baseline mental status and the acuity of any changes; with delirium, the changes typically occur over hours to days. This step is critical and requires obtaining the history from a knowledgeable informant. Neglecting the baseline mental status assessment is a leading reason for a missed diagnosis, since the acute change might otherwise be missed. Once the baseline mental status is determined, delirium is diagnosed by using brief cognitive screening tests such as the Mini-Cog11 or the Short Portable Mental Status Questionnaire12 and rating with a validated delirium instrument. Conditions that may mimic delirium include dementia, depression, and psychosis (Table 1). As described above, an acute change in mental status from baseline may distinguish delirium from other conditions. Furthermore, inattention, while common in delirium, tends to occur in later stages of dementia. For accurate differential diagnosis, knowledge of the patient’s baseline is essential to make the diagnosis. Alteration in the level of consciousness is another feature unique to delirium that is less common with dementia, depression, or psychosis. Quiz Ref IDThe next step is a careful physical and neurologic examination, searching for possible causes. Because delirium can signify an acute medical emergency, all patients presenting with delirium need rapid, targeted evaluation for electrolyte or metabolic derangements, infection, or organ failure. The specific selection of tests should be based on information obtained from the history and physical examination, keeping in mind that delirium is often multifactorial in etiology and can be influenced by a number of predisposing factors (eg, older age, cognitive impairment, multiple comorbidities), precipitating factors (eg, infections, metabolic derangement, drugs), or both. Some conditions presenting with symptoms of delirium, such as hepatic or uremic encephalopathy, acute drug intoxication, alcohol withdrawal delirium (delirium tremens), or Wernicke-Korsakoff syndrome (WKS), have specific treatments (eg, thiamine supplementation for WKS) and therefore should not be overlooked in the evaluation. Delirium and dementia commonly coexist. It is important not only to distinguish between delirium and dementia diagnostically but also to recognize when delirium is superimposed on a preexisting dementia, which has important prognostic implications, including accelerated rate of cognitive and functional decline,17 increased length of hospital stay,18 and higher rates of rehospitalization,17 institutionalization,19 and death,19 compared with dementia alone. Interview with a caregiver for baseline mental status, prior diagnosis of mild cognitive impairment or dementia, and time course of cognitive changes (typically over months for dementia), plus administration of proxy-rated tools, such as the Informant Questionnaire on Cognitive Decline of the Elderly,20 can help establish the presence of an underlying dementia. The presence of depression should also be ruled out in the interview with the patient and family, using brief depression screening tools such as the Geriatric Depression Scale.21. Search was conducted in Ovid MEDLINE, Embase, and the Cochrane Library from January 1, 2011, through March 16, 2017, using a combination of controlled vocabulary and keyword terms. Concepts were created for the topics of (1) delirium or confusion, (2) diagnosis or prevention or therapy, (3) randomized trials (using the Cochrane highly sensitive search strategy for identifying randomized trials in MEDLINE, sensitivity- and precision-maximizing version, 2008 revision), and (4) elderly adults. The search was limited to articles published in English. In addition to randomized trials, the overall search strategy was also designed to find other types of studies (eAppendix 1 in the Supplement). We identified 2303 titles and abstracts from the electronic search and also found an additional 37 eligible articles from the reference lists of relevant studies. Two hundred fifty-four full-text articles were retrieved for manual review. One hundred twenty-seven articles were used for this review, of which 25 were clinical trials, 42 were cohort studies, 5 were systematic reviews and meta-analyses, and 55 were other categories including methodological papers, clinical guidelines, and biomarker studies that were not cohort studies. A total of 11 616 patients were represented in the treatment studies. The complete list of search strategies and a search flow diagram are provided in eAppendix 1 and eFigure 1 in the Supplement. Quiz Ref IDThe CAM,7 published in 1990, continues to be the most widely used delirium instrument worldwide, used in more than 4500 original published studies to date and translated into 19 languages. The CAM algorithm is based on the presence of 4 core features of delirium (acute onset and fluctuating course of symptoms, inattention, and either disorganized thinking or altered level of consciousness7) and has high sensitivity (94%-100%), specificity (90%-95%), and interrater reliability (κ = 0.92).8,27 More recently, more than 20 delirium screening tools have been introduced, many of which have been developed in the past 6 years (Table 2). These screening tools are used to alert clinicians to the presence of possible delirium. Since screening tools have varying sensitivity and specificity, a positive screening test result should lead to further investigation for more definitive diagnosis of delirium. Definitive diagnosis of delirium should be conducted by a trained, experienced clinician and would entail cognitive testing and neurologic examination for fulfillment of key diagnostic features, including disturbance in mental status that represents a change from baseline and fluctuates in severity during the day; inattention (reduced ability to sustain attention and follow conversations); disorganization of thought, such as problems with memory, orientation, or language; and impaired consciousness, such as hypervigilance, drowsiness, or stupor. The presence of an underlying organic etiology or multiple etiologies is also required. The 3-Minute Diagnostic Assessment (3D-CAM) provides a brief assessment (3 orientation items, 4 attention items, 3 symptom probes, and 10 observational items) that facilitates rating of the 4 core CAM features and demonstrated a sensitivity of 95% and specificity of 94% when compared with a clinical reference standard rating in a prospective validation study in hospitalized patients.28 Another screening tool is the 4A’s Test (4AT), which has been validated in various clinical settings.30 This tool is also brief and easy to administer and has a sensitivity of 89.7% and specificity of 84.1%. The 4AT provides a score range suggestive of cognitive impairment for which more detailed cognitive testing is advised.30 Both 3D-CAM and 4AT validation studies have high ratings by the Standards for Reporting of Diagnostic Accuracy criteria.39. In recent years, many well-established delirium screening tools have been adapted or used in various clinical and research applications. For instance, the CAM7 is often used as a reference standard in studies of more newly developed delirium screening tools.40 The Short CAM has been more recently adapted and validated across a large range of patient populations, including medical, surgical, ICU (CAM-ICU), emergency department, nursing home, and palliative care.40 Other screening tools with more recent validation studies include the Nursing Delirium Symptom checklist (Nu-DESC), which includes assessment of disorientation, inappropriate behavior, inappropriate communication, illusions or hallucinations, and psychomotor retardation. The checklist has sensitivity of 72% and specificity of 80%41; however, limitations include the potential for overweighting of hyperactive or agitation symptoms and the risk of missing hypoactive delirium. The Modified Richmond Agitation and Sedation Scale (mRASS), which measures arousal, sedation, and level of consciousness, has been advocated as a screening tool for delirium. However, the mRASS has a low sensitivity of 64% to 70%,42,43 and the usefulness of the scale depends on the prevalence of decreased mental status in the population. In settings with high prevalence of sedation and depressed sensorium, such as the postoperative recovery room and ICU, this approach may be valuable; however, routine use of the mRASS is not recommended outside of these settings, since many cases of delirium will be missed. Because of its fluctuating nature and frequent hypoactive presentation, the detection of delirium can be especially challenging. Interview-based methods are sometimes conducted during brief encounters and need to be applied multiple times a day to improve the detection of delirium; however, this may not be feasible in many settings. Standardized chart-based methods,48 based on identification of keywords (eg, mental status change, disoriented/reoriented) by trained clinician abstractors, can be used in combination with interviews to maximize detection of delirium, particularly episodes occurring during night shifts. These methods have been validated to show sensitivity of 74% and specificity of 83% in comparison with a reference standard rating or clinical consensus panel. Therefore, the combined method of interview plus chart review48 is the recommended approach when complete and highly sensitive detection of delirium is needed. Biomarkers have assumed increasing importance, since they may be useful for identifying patients at higher risk for developing delirium and yield clues to potential underlying pathophysiologic mechanisms. Because delirium can be due to different etiologies, various biomarkers, including inflammatory, neurodegenerative, metabolic, and neurotransmitter-based, have been examined in the past 6 years. Inflammation is thought to play an important role in the pathogenesis of delirium, and recent studies have focused on inflammatory markers, including interleukins and C-reactive protein50 (eTable 1 in the Supplement). Although numerous biomarkers have been studied, none have yet been validated for clinical application, such as diagnosis or monitoring of delirium. Selected pharmacologic delirium treatment studies from the past 6 years are summarized in Table 6. Most studies do not show benefit of antipsychotics in decreasing the duration or severity of delirium. A recent comprehensive, systematic review examined antipsychotic drugs including oral risperidone, oral olanzapine, oral seroquel, intramuscular ziprasidone, and oral, intravenous, and intramuscular haloperidol100 and concluded that the current evidence does not support the use of antipsychotics for treatment (or prevention) of delirium in hospitalized older adults. There was no significant decrease in delirium incidence among 19 studies and no change in delirium duration, severity, hospital or intensive care length of stay, or reduction in mortality. Potential harm was demonstrated in 2 studies in which more patients required institutionalization after treatment with antipsychotics. Moreover, in a randomized clinical trial of atypical antipsychotic drugs in palliative care settings, participants receiving oral risperidone or haloperidol had higher delirium symptom scores and were more likely to require breakthrough treatment compared with participants receiving placebo. Participants in the placebo/nonpharmacologic management group also had better overall survival compared with those in the haloperidol group.99 Only a few limited studies have considered pharmacologic approaches other than antipsychotics for the treatment of delirium, and no definitive recommendations can be made at this time. More research is needed to establish safe and effective pharmacologic treatment approaches. Advances in diagnosis have included the development of new brief screening tools (Short-CAM adaptations, 3D-CAM, and 4AT) to improve delirium identification. Delirium severity, such as that measured with the new CAM-S scoring, has been recognized as increasingly important for tracking clinical course, prognosis, and response to treatment. Measures that capture both intensity and duration of an episode of delirium (such as the sum of all CAM-S scores) correlate best with clinical outcomes in a direct, graded relationship. For complete capture of delirium episodes, a combined approach including interview and chart review is recommended. Intraoperative EEG monitoring and bispectral monitoring are emerging strategies that identify delirium risk and help to adjust depth of anesthesia, which may decrease risk. Primary prevention with multicomponent nonpharmacologic approaches such as reorientation, early mobilization, therapeutic activities, hydration, nutrition, sleep strategies, and hearing and vision adaptations are effective and cost-effective and remain the cornerstone of delirium management. However, these approaches can be labor intensive, and streamlined approaches include the use of volunteers, aides, or nonlicensed professionals to enhance feasibility and reduce costs of implementation. Development of effective treatments have been hindered by multiple challenges, including the multifactorial contributors, diagnostic complexity, multimorbidity, heightened risk of adverse effects (ie, drug interactions), and need for multicomponent approaches. Although promising approaches are emerging, safe and highly effective pharmacologic treatments for delirium have not yet been identified. Antipsychotics are often used for patients with delirium and with severe agitation and safety risks but may contribute to heightened adverse effects and poorer long-term outcomes. Therefore, similar to the initiative by the Centers for Medicare & Medicaid Services to reduce the use of antipsychotics for improved dementia care, a concerted effort to reduce the use of antipsychotics and focus on nonpharmacologic management may improve delirium care. Several limitations of this review must be acknowledged. The literature search was restricted to the past 6 years; however, inclusion of recent systematic reviews allowed incorporation of many additional years of evidence. Studies based solely in the ICU were excluded, because they were considered outside the scope of this review and already covered in recent comprehensive reviews. Moreover, only studies published in English were included. Last, for many areas explored, we found weak to insufficient evidence, which limited our recommendations. High-quality, adequately powered randomized clinical trials represent an important priority for the field. Advances in the pathophysiologic understanding of delirium will be critical to advance the diagnosis and treatment of delirium. High-priority areas for future investigation are outlined in eTable 2 in the Supplement. Biomarkers are likely to play an increasing role in confirming diagnosis, stratifying risk, monitoring severity, and providing mechanistic understanding of delirium. Because inflammation is thought to play an important role in the pathogenesis of delirium,101 inflammatory markers are widely studied for delirium risk stratification and monitoring (eTable 1 in the Supplement). Although several studies have shown the association of elevated levels of inflammatory biomarker levels, including interleukins and C-reactive protein, with delirium, the results are not always consistent and not yet ready for clinical application.50,102 Similar to biomarker studies in other fields, standardization of assay platforms across laboratories and validation across different clinical populations will facilitate incorporation of biomarkers into clinical practice. Delirium is a common, serious condition associated with increased morbidity and mortality in older patients as well as enormous societal costs. Advances in diagnosis can improve recognition and risk stratification of delirium, and many brief delirium screening tools have been developed in the past 5 years to allow improvement in recognition and risk stratification. Along with thorough clinical examination and laboratory testing, additional tools such as imaging and fluid biomarkers are being studied to enhance clinical risk stratification and diagnosis. Pharmacologic prevention and treatment of delirium remains controversial, and nonpharmacologic management of delirium remains the cornerstone of delirium prevention and treatment. Prevention of delirium using nonpharmacologic approaches is documented to be effective, whereas pharmacologic prevention and treatment of delirium remains controversial. On examination, he was afebrile and had diffuse rhonchi and expiratory wheezes. A chest computed tomography (CT) scan revealed bilateral nodular infiltrates and a 1.3-cm cavitary nodule in the right upper lobe. Bronchoscopy was performed on day 2. Blood and bronchoalveolar lavage fluid test results are presented in the Table. The patient was discharged home to continue therapy with voriconazole. Eight weeks later, a chest CT showed marked improvement. He completed 16 weeks of antifungal therapy with voriconazole. Three months later, he remained asymptomatic. A more promising alternative that is gaining traction is to evaluate tumor products such as circulating cell-free tumor DNA (ctDNA) and circulating tumor cells (CTCs) that are shed into the blood—or other bodily fluids, such as urine or saliva. Circulating tumor cells were first observed in 1869 in the peripheral circulation of a patient with metastatic cancer (Ashworth TR. Australian Med J. 1869;14:146-147), and ctDNA was first reported in the serum of people with cancer in 1977. “Our lung cancer patients don’t always have tissue biopsies that are sufficient to do the extensive molecular analyses that we need today to guide their therapy,” noted Pasi Jänne, MD, PhD, of the Dana Farber Cancer Institute, Boston. In these cases, a liquid biopsy can come to the rescue. Unfortunately, the test is not sensitive enough to say that a patient with a negative result would not benefit from these therapies. In these cases, when such mutations are not detected in the blood, a lung tissue biopsy is still recommended. There is also a glaring need to standardize methods, said Park. “Everyone is kind of doing their own thing. No one has come up with standardizations,” he said. “You can’t really compare studies because they are like apples and oranges.”. Adults had the same risky habits. In fact, about a quarter of adolescents said they didn’t replace their lenses soon enough but slightly more than half of young adults aged 18 to 25 years conceded they did the same. About 45% of adults aged 25 years or older also used the same lenses for too long a time. Some 23% of youths kept their lens cases past the recommended replacement date compared with about 40% of both young and older adults. Overall, 81% of young adults and 88% of older adults said they’ve fallen short of following important safety measures for contact lens wearers. A, Age-standardized mortality rate for both sexes combined in 2014. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. A and B, The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. A, Age-standardized mortality rate for both sexes combined in 2014. The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. The color scale is truncated at approximately the 99th percentile but not at the first percentile, to avoid combining counties where the mortality rate increased with counties where the mortality rate decreased in the same group. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. A, Age-standardized mortality rate for both sexes combined in 2014. The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. The color scale is truncated at approximately the first percentile but not at the 99th percentile, to avoid combining counties where the mortality rate increased with counties where the mortality rate decreased in the same group. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. A, Age-standardized mortality rate for both sexes combined in 2014. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. A and B, The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. A, Age-standardized mortality rate for both sexes combined in 2014. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. A and B, The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. A, Age-standardized mortality rate for both sexes combined in 2014. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. A and B, The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. A, Age-standardized mortality rate for both sexes combined in 2014. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. A and B, The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. “Other chronic respiratory diseases” is defined as the combination of all chronic respiratory diseases except chronic obstructive pulmonary disease, interstitial lung disease and pulmonary sarcoidosis, asthma, asbestosis, coal workers’ pneumoconiosis, silicosis, and other pneumoconiosis. A, Age-standardized mortality rate for both sexes combined in 2014. The color scale is truncated at approximately the first and 99th percentiles as indicated by the range given in the color scale. B, Relative change in the age-standardized mortality rate for both sexes combined between 1980 and 2014. The color scale is truncated at approximately the 99th percentile but not at the first percentile, to avoid combining counties where the mortality rate increased with counties where the mortality rate decreased in the same group. C, Age-standardized mortality rate in 1980, 1990, 2000, and 2014. The bottom border, middle line, and top border of the boxes indicate the 25th, 50th, and 75th percentiles, respectively, across all counties; whiskers, the full range across counties; and circles, the national-level rate. This study used the cause list developed for the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD).1 This cause list is arranged hierarchically in 4 levels; within each level, the list is exhaustive and mutually exclusive. eTable 3 in the Supplement lists all causes in the GBD cause list and the International Classification of Diseases, Ninth Revision (ICD-9) and Tenth Revision (ICD-10) codes that correspond to each cause. The focus of this study was on chronic respiratory diseases, specifically chronic obstructive pulmonary disease, interstitial lung disease and pulmonary sarcoidosis, asthma, pneumoconiosis (further subdivided into asbestosis, coal workers’ pneumoconiosis, silicosis, and other pneumoconiosis), and the combination of all other respiratory diseases. Although the focus of this study was chronic respiratory diseases, all causes of death in the GBD cause list were analyzed concurrently. where Dj,t,a, Pj,t,a, and mj,t,a are the number of deaths, the population, and the underlying mortality rate, respectively, for county j, year t, and age group a. The model for mj,t,a contained 6 components: an intercept (β0), fixed covariate effects (β1), random age-time effects (γ1,a,t), random spatial effects (γ2,j), random space-time effects (γ3,j and γ4,j,t), and random space-age effects (γ5,j and γ6,j,a). The model incorporated 7 covariates (Xj,t): the proportion of the adult population who graduated high school, the proportion of the population that is Hispanic, the proportion of the population that is black, the proportion of the population that is a race other than black or white, the proportion of a county that is contained within a state or federal Native American reservation, the median household income, and the population density. γ1, γ2, γ3, and γ5 were assumed to follow conditional autoregressive distributions, which allow for smoothing over adjacent age groups and years (γ1) or counties (γ2, γ3, and γ5).20,21 γ4 and γ6 were assumed to follow independent mean-zero normal distributions. Quiz Ref IDModels were fit using the Template Model Builder Package22 in R version 3.2.4 statistical software (R Foundation for Statistical Computing). One thousand draws of mj,t,a were taken from the posterior distribution. These draws were raked23 (ie, scaled along multiple dimensions) to ensure consistency between levels of the cause hierarchy and to ensure consistency with national estimates from the GBD.1 Mortality rates for both sexes combined were calculated from the population-weighted average of the sex-specific mortality rates. Age-standardized mortality rates were calculated using the US 2010 census population as the standard. Years of life lost (YLLs) were calculated by multiplying the mortality rate by population by age-specific life expectancy from the reference life table used in the GBD1 and then summing across all ages. Point estimates were calculated from the mean of all draws, and 95% uncertainty intervals (UIs) were calculated from the 2.5th and 97.5th percentiles. Changes over time were considered statistically significant if the posterior probability of an increase (or decrease) was at least 95%. No explicit correction for multiple testing (ie, across multiple counties) was applied; however, modeling all counties simultaneously is expected to mitigate the risk of spuriously detecting changes due to multiple testing. Between 1980 and 2014, 4 616 711 deaths due to chronic respiratory diseases were recorded in the United States. Among all deaths due to chronic respiratory diseases, 9.4% had been assigned garbage codes and were reassigned via garbage code redistribution; at the county level, this ranged from 0% to 58.5% (interquartile range, 8.1%-12.6%) of deaths due to chronic respiratory diseases. In 2014, there were 177.3 (95% UI, 172.8-182.2) thousand deaths (6.7% of all deaths) and 2522.2 (95% UI, 2466.8-2582.7) thousand YLLs (5.5% of all YLLs) from chronic respiratory diseases (Table). Within chronic respiratory diseases, COPD accounted for the largest number of deaths in 2014 (151.2 [95% UI, 146.4-157.3] thousand deaths; 85.3%), followed by interstitial lung disease and pulmonary sarcoidosis (18.2 [95% UI, 11.9-20.5] thousand deaths; 10.3%), asthma (3.9 [95% UI, 3.6-4.3] thousand deaths; 2.2%), other chronic respiratory diseases (2.4 [95% UI, 2.3-2.6] thousand deaths; 1.4%), and pneumoconiosis (1.5 [95% UI, 1.4-1.7] thousand deaths; 0.9%). Mortality rates from chronic respiratory diseases varied widely among counties (eFigure 1 in the Supplement), ranging from 14.3 to 161.0 deaths per 100 000 population and with a gap of 41.1 deaths per 100 000 population between counties in the 10th and 90th percentiles in 2014. Nationally, the mortality rate from chronic respiratory diseases increased from 40.8 (95% UI, 39.8-41.8) deaths per 100 000 population in 1980 to a peak of 55.4 (95% UI, 54.1-56.5) deaths per 100 000 population in 2002 and then declined to 52.9 (95% UI, 51.6-54.4) deaths per 100 000 population in 2014. Overall, this corresponds to a 29.7% (95% UI, 25.5%-33.8%) increase in the national mortality rate between 1980 and 2014. During this same period, the mortality rate from chronic respiratory diseases similarly increased in 93.2% of counties (statistically significant in 88.3%), but changes ranged from a 52.9% decline to a 224.0% increase. From 1980 to 2014, the difference between the mortality rate in counties in the 90th and 10th percentiles nearly doubled, increasing from 21.7 to 41.1 deaths per 100 000 population (posterior probability of increase >99.9%). A total of 3 942 450 deaths due to COPD were recorded in the United States between 1980 and 2014. Mortality rates due to COPD varied widely among counties in 2014, ranging from 9.9 to 152.3 deaths per 100 000 population, with a difference of 39.8 deaths per 100 000 population between counties in the 10th and 90th percentiles. Counties with the highest rates of COPD in 2014 were concentrated in central Appalachia, but could also be found in other states in the southern half of the United States as far west as Colorado (Figure 1). Counties with the lowest rates of COPD in 2014 were found near Washington, DC; New York, New York; and San Francisco, California; along the Texas-Mexico border; in central Colorado; and in northern Utah. Mortality from COPD increased between 1980 and 2014, with the mortality rate increasing 30.8% (95% UI, 25.2%-39.0%) overall from 34.5 (95% UI, 33.0-35.5) to 45.1 (95% UI, 43.7-46.9) deaths per 100 000 population. The mortality rate similarly increased in a majority of counties (91.9%; statistically significant in 86.1%) during this period, but changes in the mortality rate nonetheless varied widely among counties, ranging from a 60.5% decline to a 263.7% increase. Large increases were observed in counties throughout the South, particularly in a band stretching from northern Texas to North Carolina and South Carolina. Counties where COPD declined were concentrated in the Washington, DC, area, along the Texas-Mexico border, along the Pacific coast in California, and in parts of Alaska, Colorado, Wyoming, and Montana. Temporal trends in COPD mortality rates differed considerably between males and females (eFigure 2 and eFigure 3 in the Supplement). Nationally, COPD mortality rates among males declined by 10.9% (95% UI, 3.9%-15.4%) between 1980 and 2014, with most of this decline occurring between 2000 and 2014. In contrast, national COPD mortality rates among females increased by 107.4% (95% UI, 92.1%-125.2%) between 1980 and 2014, with most of the increase occurring prior to 2003 and relatively steady rates from 2003 to 2014. At the county level between 1980 and 2014, COPD mortality among males declined in 40.7% of counties (statistically significant in 23.6%), whereas COPD mortality among females increased in most counties (98.9%; statistically significant in 97.4%). Between 1980 and 2014, 403 168 deaths due to interstitial lung disease and pulmonary sarcoidosis were recorded in the United States. In 2014, mortality rates from interstitial lung disease and pulmonary sarcoidosis varied among counties, ranging from 2.7 to 14.9 deaths per 100 000 population and with a gap of 2.6 deaths per 100 000 population between counties in the 10th and 90th percentiles. Counties with very high mortality rates relative to other counties were dispersed throughout several regions, including the Southwest, northern Great Plains, New England, and South Atlantic (Figure 2). Counties with very low mortality rates relative to other counties were also found in many parts of the United States, including parts of Nevada, Colorado, Texas, North Dakota, South Dakota, and Florida. The asthma mortality rate declined by 46.5% (95% UI, 27.0%-51.8%) between 1980 and 2014, from 2.2 (95% UI, 2.0-2.3) to 1.2 (95% UI, 1.1-1.3) deaths per 100 000 population. The mortality rate similarly declined in nearly every county (99.6%; statistically significant in 92.1%) during this period. However, changes in the mortality rate still varied widely among counties, ranging from a 15.3% increase to a 75.5% decline. Counties with the largest declines were located primarily in western and southwestern states, particularly southern Arizona, northern New Mexico, central Colorado, and western Montana. Counties with the smallest declines (or even increases) were found in the same regions with elevated mortality in 2014. Between 1980 and 2014, 57 033 deaths due to pneumoconiosis were recorded in the United States. These included 15 163 deaths due to asbestosis, 21 592 due to coal workers’ pneumoconiosis, 4529 due to silicosis, and 15 749 due to other pneumoconiosis. Large differences in the mortality rate were observed among counties, with mortality rates ranging from 0.1 to 43.5 deaths per 100 000 population in 2014. Most counties had very low pneumoconiosis mortality rates: 91.0% of counties experience mortality rates of less than 1 death per 100 000 in 2014. A small subset of counties experienced substantially higher mortality rates from pneumoconiosis; these counties were concentrated in central Appalachia, but there were also individual counties with relatively high pneumoconiosis mortality rates in Mississippi, Colorado, Utah, and Montana (Figure 4). A total of 56 994 deaths due to other chronic respiratory diseases were recorded in the United States between 1980 and 2014. The mortality rate due to other chronic respiratory diseases was generally low but still varied among counties, ranging from 0.3 to 2.2 deaths per 100 000 population in 2014. Mortality rates from this group of diseases were generally highest in southern counties stretching from Mississippi to South Carolina, and were also elevated in counties in northern Maine and western counties stretching from Wyoming and Utah to California and Oregon (Figure 8). Mortality rates from other chronic respiratory diseases were lowest in counties near New York City and in southern Florida and California. This analysis has several important limitations. First, this analysis made use of population, deaths, and covariates data from a number of different sources, all of which are subject to error. Second, 2 different versions of the ICD were in use during the period considered, and changes to cause-of-death coding corresponding to changes in the ICD may lead to spurious trends, although we have attempted to mitigate the effects of this change by using a consistent mapping of ICD codes to chronic respiratory disease causes. Third, this analysis used garbage code redistribution algorithms to redistribute deaths assigned implausible or nonspecific causes to likely true causes of death; however, these methods have not been validated because appropriate gold-standard data are unavailable. Fourth, the uncertainty that arises from applying these garbage code redistribution algorithms is difficult to quantify and consequently has not been accounted for in the UIs associated with all estimated mortality rates. Fifth, this analysis used small area estimation models that smooth mortality rates over time, space, and age groups, which may in some cases attenuate unusually low or high mortality rates and underestimate true geographic variability. Chronic respiratory diseases are a leading cause of death in the United States and are associated with significant morbidity and financial costs. Chronic respiratory disease mortality has previously been linked with a number of exposures—smoking, occupational exposures, air pollution—that are potentially amenable to intervention. Moreover, for some chronic respiratory diseases, treatments exist that can improve quality of life and prevent or delay death, underscoring the importance of promoting prompt diagnosis and utilization of recommended treatments. The local information on chronic respiratory disease mortality from this study should be used to galvanize support for action and to better target and customize efforts to reduce the burden of chronic respiratory diseases. The phase 2 study examined the efficacy of once weekly CAM2038 on 47 participants with moderate to severe OUD who were not seeking treatment. During a qualification phase, participants’ responses to various doses of hydromorphone were measured. Participants were then randomized to 2 doses of CAM2038, either at 24 mg or 32 mg, given 1 week apart. Following randomization, participants underwent 4 sets of hydromorphone challenges over 13 days. No placebo-controlled group was included due to safety concerns given the high opioid doses administered during the hydromorphone challenges. For example, West Virginia, Kentucky, and Wyoming had the highest rates of cigarette use (in the past month) among adolescents aged 12 through 17 years (8.3%, 7.8%, and 7.8%, respectively), compared with the 2014-2015 national average of 4.5%. Washington, DC, California, and Florida had the lowest rates (3%, 3.1%, and 3.6%, respectively). Marijuana use among adolescents aged 12 through 17 years was highest in Colorado, Vermont, and Alaska (11.1%, 10.9%, and10.6%, respectively) and lowest in Utah, Alabama, and Iowa (4.5%, 5.2%, and 5.3%, respectively). The corresponding national average was 7.2%. Physical Chemistry of Vital Phenomena, for Students and Investigators in the Biological and Medical Sciences. By J. F. McClendon, Assistant Professor of Physiology in the University of Minnesota. Cloth. Price, $2 net. Pp. 240, with 30 illustrations. Princeton: Princeton University Press, 1917. Physical chemistry, that lusty infant prodigy, offspring of the union of physics and chemistry, has already come in many instances to the assistance of the practitioner, giving aid in diagnosis and helping in the explanation of physiologic processes. One has only to recall the most recent innovations of the determination of the hydrogen ion concentration of the blood and spinal fluids, the diagnosis of syphilis by the colloidal gold test, or the distinguishing of epidemic cerebrospinal meningitis from tuberculous meningitis by the cataphoresis of the proteins of the spinal fluid. Hence a book of this title should attract the interest of practitioners and students alike. This book, however, makes no pretense to having been written for clinical men. It is rather intended for advanced students who already understand much about the subject.… It treats of such things as osmotic pressure, adsorption, hydrogen ion determinations, surface tension, enzyme action, permeability of cells, anesthesia and narcosis, cytolysis and disinfection, ameboid motion, muscle contraction, oxidations, artificial parthenogenesis and cell division. Truly an appetizing bill of fare, if only well cooked and seasoned!. But alas for the “if.” It is not well cooked; much of it, indeed, is quite raw, and the seasoning has been generally omitted, so that even a robust appetite will not make it all palatable. This is not saying that there is not much that is good in the book and many facts that are stimulating and instructive.… Perhaps the title makes us expect too much, and in the outcome we have so little—so little except words, and of these there are so many and such long ones! There is hardly a properly formed scientific statement in the first chapter. It is full of conclusions and interpretations stated as if they were facts, and of conclusions which do not follow from the premises. Throughout the book there is a scarcity of objectively described facts. Almost never do we find a clear, simple statement of what the objective facts really are; always these facts are obscured by interpretations, or the interpretations are substituted for them. The author, in accordance with the prevailing style of the hour, attempts to explain all the phenomena of life by inventing what he calls a plasma membrane and putting it around a colloidal solution and calling it protoplasm. Since he is the inventor of this membrane, it is his privilege to make it as ingenious as he can. The main differences between the theories of the physical-chemical zoologists and physiologists at the present time consist in the ingenunity [sic] of their imaginary membranes. If only these membranes are sufficiently complex, it becomes possible to explain all the phenomena of life by means of them. If any new facts are discovered which the old membrane cannot account for, it becomes the simplest thing in the world to take that membrane off, to devise a new one with more modern improvements, wrap it about a colloidal solution of a nondescript character, and there you have protoplasm!.… To explain how this is all possible it is only necessary to fall back on surface tension. Surface tension explains all things. This is a perfectly safe refuge and one calculated to satisfy the greatest doubter, for no one understands what surface tension may be doing in such complex systems, and almost no one understands it even in the simplest cases. In saying this we do not aim especially at the author of this book, for he has only put together the publications and conclusions of other men; but we are aiming at the whole glib, superficial school of biologists, who pretend to solve fundamental problems of the greatest complexity. This book is typical of much of modern biology. It is not possible to say about it anything worse than this. Biology is in a period of inflation. Real values are obscured. Worthless properties are being worked as fabulously rich mines, but the ore contains only a trace of real metal. A little glitter and some mathematical formulas lead astray all but those who have been already once deluded. Words take the place of real explanations; we have purely verbal solutions of the most abstruse problems.…. One comes back from reading a book of this character, and most books on this subject, with the feeling of having been wandering in a terrible morass. Some of the ill smelling mud still clings to one. “Permeability” and “adsorption” are words covering a multitude of scientific sins. This must truly be the red light district, the demi-monde of science, which intoxicates and ruins its habitués. This realm of biology is truly that demiscience which Paul Shorey anathematizes. Words juggled from one meaning to another; conclusions unrelated to their premises; a trackless waste—the biologic authors wander through this series of sink holes, struggling out of one only to fall into another, and the reader, if he is not sure of foot and alert of eye, falls with them. Turn off the power source immediately. If the individual is in cardiac arrest, initiate cardiopulmonary resuscitation after ensuring that the person is not still in contact with the source and have the person taken to the nearest medical facility. At the hospital, doctors will evaluate for skin burn, tissue damage, abnormal heart rhythms, and any other traumatic injuries. Burns will be treated and intravenous fluids will be given if the burn caused any internal damage. Biotin ingestion of 10 mg/d for 7 days was associated with significant increased biotin concentrations (P < .001), as well as falsely decreased results in the Roche cobas e602 TSH (P = .006), OCD Vitros 5600 TSH (P < .001), and OCD Vitros 5600 PTH (P < .001) assays. A unique color is used for each participant across all panels. Dotted lines represent the lower and the upper reference range for the assay. Architect indicates Abbott Architect 2000; Centaur, Siemens Centaur XP; cobas, Roche cobas e602; Vista, Siemens Vista Dimension 1500; and Vitros, OCD Vitros 5600. For SI conversion factors, see the Methods section. Taking 10 mg/d of biotin for 7 days was associated with falsely increased results from the Roche cobas e602 assay for free T4 (P= .01) and free T3 (P= .005) and from the Siemens Vista Dimension 1500 assay for free T3 (P< .001). A unique color is used for each participant across all panels. The dotted lines represent the lower and the upper reference range for each assay. For prolactin, 2 women were represented by triangles and 4 men by dots. Red dotted lines represent normal range of prolactin for women and black for men. Architect indicates Abbott Architect 2000; Centaur, Siemens Centaur XP; cobas, Roche cobas e602; Vista, Siemens Vista Dimension 1500; and Vitros, OCD Vitros 5600. For SI conversion factors, see the Methods section. This study was approved by the institutional review board at the University of Minnesota. Six healthy adults responded to study announcement fliers, gave informed consent before participation, and were not compensated. Exclusion criteria including but not limited to conditions that potentially affect biotin or hormones were being pregnant or lactating, having known thyroid disease, undergoing thyroid hormone treatment, ingesting over-the-counter dietary or nutritional supplements (excluding standard multivitamin preparations containing no more than 100% of the daily value for biotin and calcium), working the night shift, smoking,15 being treated with anticonvulsant medicine,16 or lacking the capacity to consent. Participants were asked to stop taking multivitamins 2 weeks before study participation. Participants were instructed to take 10 mg/d of biotin (Nature Made) at the same time each morning for 7 days. Blood specimens were collected by venipuncture at baseline prior to starting biotin, after 1 week of biotin supplementation (day 7), and 1 week after participants stopped taking biotin (day 14). On the last day of treatment (day 7), the blood specimen was drawn approximately 2 hours after taking their last dose. Blood specimens were labeled with participant study identification number and date, promptly centrifuged and processed to produce serum sample aliquots that were stored up to a year at −70°C until further analysis. Eighteen serum samples, collected from the 6 participants at the 3 time points, were each split into 4 aliquots and sent for testing at 4 clinical laboratories using different diagnostic assay systems: Johns Hopkins Medical Institutions used Roche cobas e602 for measurement of all 9 hormones and ferritin; Children’s Mercy Hospital used the OCD Vitros 5600 for TSH, PTH, total T4, total T3, free T4, NT-proBNP, and ferritin and Siemens Immulite 2000 for prolactin. The University of Minnesota Medical Center used Siemens Vista Dimension 1500 for TSH, PTH, total T4, free T4, free T3, NT-proBNP, ferritin, and PSA and Siemens Advia Centaur XP for total T3 and PTH; Boston Medical Center used Abbott Architect 2000 for TSH, PTH, total T4, total T3, free T4, prolactin, 25-OHD, ferritin, and PSA. eTable 1 in the Supplement summarizes information on the 37 assays for the 11 analytes evaluated on 4 systems: 23 incorporated biotin and streptavidin components and 14 did not include those components, serving as negative controls. Analytical imprecision data for these 37 assays are included in eTable 2 in the Supplement. Two immunoassay principals were used (eFigures 1 and 2 in the Supplement): the sandwich immunoassay for TSH, PTH, prolactin, NT-proBNP, PSA, and ferritin and the competitive immunoassay for total T4, total T3, free T4, free T3, and 25-OHD. Analyte levels below or above their reportable ranges were assigned the following values: for Ortho Vitros NT-proBNP, 11 pg/mL was used for levels reported to be lower than 11.1 pg/mL; for both Siemens Vista Dimension and Roche cobas NT-proBNP, 4 pg/mL was used for levels reported that were less than 5 pg/mL. For biotin more than 3600 pg/mL, 3601 pg/mL was used. These value assignments would underestimate any biotin interference that was present. (To convert biotin from pg/mL to nmol/L, multiply by 0.00409; prolactin from ng/mL to pmol/L, multiply by 43.478; free T3 from pg/mL to pmol/L, multiply by 1.54; total T3 from ng/mL to nmol/L, multiply by 1.54; free T4 from ng/dL to pmol/L, multiply by 12.871; total T4 from µg/dL to nmol/L, multiply by 12.871.). Each combination of an analyte and a system was analyzed separately with repeated measures of an analysis of variance (ANOVA) (mixed linear model), for which the random effect was a participant and the within-participant fixed effect was time (day of study). The primary analysis compared study day 7, the last day participants took biotin, with the mean of baseline and study day 14, before participants took biotin and after they stopped taking biotin, using a contrast in the ANOVA. To test this contrast for each analyte and system, P < .05 was the criterion for statistical significance. For each combination of an analyte and a system, we also present comparisons of pairs of times using the Tukey honest significant difference post hoc test. Data are reported as mean and 95% CIs. The Fisher exact test was used to compare biotin interference outcomes between the biotinylated assays and nonbiotinylated assays. All tests were 2-sided. All analyses used JMP Pro v13 (SAS Institute Inc). Six healthy adults enrolled in the study, 2 women and 4 men, with a mean age of 38 years (range, 31-45 years). Baseline analyte concentrations for the 6 participants were within the manufacturer’s reference ranges for 7 analytes measured by 29 assays, except for 4 analytes measured by 8 assays: (1) PTH by OCD Vitros 5600 and Siemens Advia Centaur XP; (2) total T4 by Abbott Architect; (3) prolactin by Roche cobas e602, Siemens Immulite 2000, and Abbott Architect; and (4) ferritin by the Roche cobas e602 and Abbott Architect. In each case, this involved no more than 1 or 2 participants. None of the participants had abnormal baseline results across all systems for a given analyte. Biotin ingestion was associated with statistically significant false increases in 4 assays: 3 Roche cobas e602 assays measuring total T3, free T3, and free T4; and the Siemens Vista Dimension 1500 measuring free T3 (Figure 2 and Figure 3). Roche cobas e602 total T3 concentrations falsely increased by a mean of 0.85 ng/mL (95% CI, 0.49-1.22 ng/mL; P = .001), whereas Siemens Vista free T3 concentration falsely increased by a mean of 0.78 pg/mL (95% CI, 0.50-1.06 pg/mL; P < .001; Table 2). In 3 participants, the Roche cobas e602 total T3 results and in 1 participant Siemens Vista free T3 result were higher than their respective reference ranges. Table 3 shows the results for prolactin, which did not have biotin-associated changes. eFigure 3 in the Supplement shows that biotin ingestion was associated with falsely reduced OCD Vitros 5600 NT-proBNP results by an average of more than 13.9 pg/mL (95% CI, −24.7 to −3.12 pg/mL; P = .03) to less than 11.1 pg/mL in all participants. The actual reduction was underestimated because results while participants were taking biotin were below the assay’s reportable range. Biotin ingestion was associated with falsely increased Roche cobas 25-OHD results by a mean of 9.25 ng/mL (95% CI, 5.72-12.8 ng/mL; P < .001) higher than the baseline (eTable 3 in the Supplement). None of the 11 analytes measured by 37 assays differed between baseline and day 14, except ferritin measured by the Siemens Vista Dimension (eTable 4 in the Supplement). Ferritin at day 7 of biotin treatment did not differ significantly from the mean of baseline and day 14 or from baseline alone in all 4 systems, supporting that biotin ingestion was not associated with the difference between baseline and day 14. This study involving 6 healthy adults demonstrated that oral biotin was associated with potentially clinically important assay interference in some but not all biotinylated assays. Among the 23 biotinylated assays studied, biotin interference was of greatest clinical significance in the OCD Vitros TSH assay, where falsely decreased TSH concentrations (to <0.15 mU/L) could have resulted in misdiagnosis of thyrotoxicosis in otherwise euthyroid individuals. Likewise, falsely decreased OCD Vitros NT-proBNP, to lower than assay detection limits, could possibly result in failure to identify congestive heart failure.18 Because healthy study participants had normal baseline NT-proBNP, further study of patients with high baseline NT-proBNP concentration would be required to establish the effect of biotin interference on the diagnosis of heart failure. The smaller changes observed in other assays, namely OCD Vitros PTH; Roche cobas e602 TSH, total and free T3, free T4, and 25-OHD; and Siemens Vista free T3, although primarily producing false results within the reference range among participants while taking biotin, could lead to falsely normal or abnormal interpretation of the results for individuals starting from baseline levels closer to the reference range limits.19. To our knowledge, the current study is the first to systematically assess the association of biotin ingestion (10 mg/d for 7 days) in healthy adults with performance of 37 assays that measure 11 analytes over 4 major diagnostic systems (Table 4). The study has several limitations. First, only healthy adults with mostly normal analyte concentrations were evaluated. Second, the study was neither randomized nor blinded to the investigators or participants, although it was blinded to the clinical laboratories. The study did not have a placebo group, but the crossover design and repeated measures analysis allowed each participant’s baseline values to serve as his/her own controls, making it more efficient (smaller sample size is needed) than a randomized design, because between-person variability in overall level of an analyte is eliminated. Third, because no formal dose-response pharmacokinetic study of biotin at various doses was performed, the minimal dose and duration required to alter assay results remains undetermined. Fourth, the sample size was small; power may have been insufficient to detect smaller effects of biotin ingestion. Definitive studies of the effect of biotin on specific assays and analytes will require further investigation. Despite these limitations, this study reinforces cautionary advice regarding potential limitations of assays that use biotin streptavidin binding for clinical evaluation of individuals who ingest large doses of biotin. A few assay manufacturers package inserts acknowledge biotin interference, recommending delayed sample collection after biotin intake. Based on these findings, manufacturers may need to consider modifying biotinylated assays to minimize the effects of biotin ingestion.20,21,37 Laboratories could identify assays that contain biotinylated components.38 Clinicians may want to ask about biotin ingestion even if assay results are not suspect because biotin interferences can cause either falsely normal or abnormal results.24 It may be advisable for patients to stop taking biotin, preferably for a week as studied herein, before undergoing laboratory testing. Alternatively, in the presence of biotin ingestion, nonbiotinylated assays would be preferred. Future studies, including patients with normal and abnormal analyte concentrations, are recommended to further clarify the extent and pharmacokinetics of ingested biotin interference on various assay platforms. The change in cartilage volume was defined as the primary structural outcome in the trial register and study protocol. Also, the sample size calculation was based on the detection of a difference in cartilage volume of 90 mm3 between the groups. The operationalization of this cartilage volume measure was not mentioned in the original study protocol, trial register, or the sample size calculation reported in the article. Moreover, the standard deviation of 224 mm3 in the sample size calculation cannot be found in the article the authors cited.2 The treatment difference used in the sample size calculation suggests that the proxy total cartilage damage index may have been used. However, in the analytic plan, the authors reported that they used cartilage thickness as a proxy for cartilage volume. Moreover, in the final analysis plan (undated) provided in the supplement, the authors stated that they had used as the primary outcome the cartilage damage index and the mean cartilage thickness. In summary, we are confused about what the original primary structural outcome was in this study. If it was the total cartilage damage index, no statistically significant difference was found between the groups (Table 2 in the article), which might change the conclusion. The change in cartilage volume was defined as the primary structural outcome in the trial register and study protocol. Also, the sample size calculation was based on the detection of a difference in cartilage volume of 90 mm3 between the groups. The operationalization of this cartilage volume measure was not mentioned in the original study protocol, trial register, or the sample size calculation reported in the article. Moreover, the standard deviation of 224 mm3 in the sample size calculation cannot be found in the article the authors cited.2 The treatment difference used in the sample size calculation suggests that the proxy total cartilage damage index may have been used. However, in the analytic plan, the authors reported that they used cartilage thickness as a proxy for cartilage volume. Moreover, in the final analysis plan (undated) provided in the supplement, the authors stated that they had used as the primary outcome the cartilage damage index and the mean cartilage thickness. In summary, we are confused about what the original primary structural outcome was in this study. If it was the total cartilage damage index, no statistically significant difference was found between the groups (Table 2 in the article), which might change the conclusion. In Reply Dr Luijsterburg and colleagues question whether factors such as persistence of inflammation or severity of pain might have modified the effect on cartilage of the repeated intra-articular corticosteroid injections. The presence of synovitis based on ultrasonographic criteria was required for eligibility in our study so, presumably, all participants had this to some degree. Nevertheless, these characteristics could vary over time and we did perform ultrasonography at each visit. However, these data have not yet been analyzed. The influence of these disease characteristics on the study outcome as time-varying covariates certainly merit a future exploratory analysis. We apologize that the source of the SD for our power computation was not explicit in the vitamin D trial article.1 It can be derived from the CI around the combined cartilage volume loss in the placebo group in Table 3 of our article as 203 mm3, to which we applied 10% inflation (to be conservative), generating the value of 224 mm3. Approaches to the measurement of cartilage volume loss, the prespecified primary structural end point for the trial, have been subject to refinement over the last decade. The metrics we used, cartilage thickness (primary) and the cartilage damage index, are in fact both volumetric measures derived from 3-dimensional images and using multiple slices.2,3 However, unlike the early computation of entire cartilage volume, these approaches focus on regions that exhibit the most change, hence are optimized for their discriminative properties. Both metrics showed significantly more loss in the index compartments, so substituting one for the other would not influence the interpretation of the results. They also question whether the results for cartilage volume loss represent a clinically meaningful risk because it was not detected by other feature scales and was nonsignificant in the completers’ analysis. However, the cartilage thickness and damage index measurements were explicitly developed and optimized for their discriminative properties and also showed differences among completers, albeit with reduced power. Although the group treated with triamcinolone did not exhibit any increase in functional decline, it is of concern that studies have consistently linked rate of cartilage loss to future risk for arthroplasty. One study found that for every 1% increase in the rate of tibial cartilage loss, there was a 20% increased risk of undergoing a knee replacement at 4 years.4 The absolute increase in cartilage loss that we observed was 2% per year. Biosimilar applications are often mired in extended patent-related litigation. A key difference between patent litigation over biosimilars under the Biologics Price Competition and Innovation Act (BPCIA) and over generics under the Hatch-Waxman Act is that the latter clearly defines the scope of patent litigation (ie, all branded drug patents are listed in the Orange Book). In contrast, due to the intricacy of manufacturing biologics, the corresponding Purple Book for biologics does not list patents, so reference biologic and biosimilar makers must exchange proprietary information if they wish to identify which of the reference biologic maker’s patents may be infringed. However, this exchange exposes the biosimilar maker to the risk of divulging trade secrets. Thus, some biosimilar makers have been reluctant to exchange information, leading to lawsuits in which a federal circuit court ruled such exchange optional.4 With this ruling, uncertainty over the course and timing of biosimilar patent litigation may increase. Most biosimilar applications in the United States so far have been subject to litigation by the reference biologic maker, and many applications are in litigation over the interpretation of the BPCIA terms. The result is that only 2 biosimilars are available commercially in the United States. Two other approved biosimilars are subject to patent litigation risks. This lack of competition will likely prevent substantial price decreases for biologics and biosimilars. Balancing intellectual property protection against timely commercial launches to increase price competition is important. If Congress adopted parts of the Hatch-Waxman Act, patent dispute resolution could not be used to delay biosimilar market entry. On June 12, 2017, the US Supreme Court removed a 6-month delay for biosimilar entry.5 Reducing patent litigation, along with other suggestions, may lead to larger cost savings in biologic products.1,2. There are 2 parts of the H-1B application process.2 First, a labor condition application must be prepared and filed with the US Department of Labor. On the application, many sponsoring entities list extra slots in the event that additional physicians are needed in the course of the year. This includes new H-1B visa applications and extensions of existing visas (often needed for events such as illness, pregnancies, late starts, etc). Also, a labor condition application may be used to file for an H-1B visa for a single year or may be continued for a full 3 years. This practice is fairly standard among employers and carries no penalty. The data shown in Table 2 in the research letter represented the number of slots applied for and not the actual number of H-1B applications filed or approved. Table 2 listed 150 physician labor condition applications each for 2 Presence Health hospitals, thereby ranking them fourth and fifth on the list of sponsors for H-1B physician visas nationwide. Data from 2016 at Presence Saint Joseph Hospital in Chicago show a total of 20 H-1B visa applications filed and approved, 14 new applications for incoming residents, and 6 H-1B visa extensions for residents who required additional days to complete their training. Similar data from Presence Saint Francis Hospital show a total of 11 H-1B visa applications and approvals, with 7 new applications and 4 extensions. In Reply In our research letter, we noted that labor condition application data are not equivalent to data from the US Citizenship and Immigration Services (USCIS), which were not available at the time the study was conducted. However, through an evaluation of prior USCIS and labor condition application data, we suggested that an 80% conversion rate from labor condition applications to visa issuance would likely pertain in aggregate for 2016. Additionally, in our analyses, we tabulated the number of individuals included in each labor condition application rather than the total number of applications specifically to avoid the concern mentioned by Dr Iroegbu and colleagues. Subsequent to the publication of our research letter, the USCIS responded to a Freedom of Information Act request by providing additional information regarding the actual issuance of H-1B visas to physicians. For calendar year 2016, 7924 H-1B visas were issued by USCIS, which represents a labor condition application to H-1B visa issuance conversion rate of 75%, which is quite close to our suggested 80%. Iroegbu and colleagues also note that certain institutions will apply for more spots than they expect to have H-1B visas issued. This finding is confirmed by the USCIS data, which show that Presence Health hospitals applied for more labor condition applications than needed for the expected number of H-1B visas to be issued. However, this was not universally true across all employers; the majority of institutions applied for the number of spots required rather than using labor condition applications to list extra slots to ensure subsequent availability. As a result, Presence Health was overrepresented in our study, unlike many of the other institutions listed in Table 2 that do not maintain this practice. In revisiting Table 2 with the benefit of additional data from USCIS, the top 5 recipients of H-1B visas in 2016 were the Cleveland Clinic (183 H-1B visas), Apogee Medical Group (173 H-1B visas issued across all Apogee Medical Groups combined), Montefiore Medical Center (173), the Mayo Clinic (89), and Bronx Lebanon Hospital Center (67), 4 of which were in the top 8 in the previous analysis. Binning Apogee Medical Group visas resulted in their ascending in the rankings, whereas in the previous analysis, we did not analyze their visas together. Other differences between the USCIS H-1B data and the previous analysis are likely due to discrepancies between labor condition application numbers and visa issuance. The top 5 states in which H-1B visas were issued were New York (985), Pennsylvania (668), Ohio (496), Massachusetts (444), and Illinois (420); these 5 were in the top 6 states in the previous analysis. In the Original Investigation entitled “Effect of radiofrequency denervation on pain intensity among patients with chronic low back pain: the Mint randomized clinical trials,”1 published in the July 4, 2017, issue of JAMA, there were errors in Figure 3. In the first box after randomization on the left side, 26 for “positive facet joint block” should be 25 and 67 for “did not receive radiofrequency denervation” should be 36. In the fifth box down after randomization on the left, 16 for “did not complete 3-mo follow-up” should be 15 and 8 for “no response” should be 7. In the sixth box down after randomization on the left, 19 for “did not complete 6-mo follow-up” should be 18 and 12 for “no response” should be 11. This article was corrected online. Binary logistic regression models (using STATA [StataCorp], version 14.0) were used to estimate odds ratios (ORs), adjusted odds ratios (AORs) and 95% CIs. All binary logistic regression models estimating the AORs simultaneously controlled for sociodemographic variables (listed in Table 2). Analyses used weights to account for the probability of selection into the sample and adjust for the different sample sizes for grades 8, 10, and 12. Two-tailed significance tests with an α of .05 were used to assess statistical significance. Given that 10.4% of the sample had missing data on at least 1 of the variables used in the analyses, multiple regression imputation was used to impute missing data (see Table 1 for details). ASD indicates autism spectrum disorder. Each sibling is followed from birth to end of follow-up (death, emigration, or end of follow-up) or ASD diagnosis. Analysis methods A and B agree for the clear majority of all sibling pairs. If S2 was not observed with ASD, the concordance or discordance status would be the same for both methods. In the Figure, for the family with 2 siblings where S1 is diagnosed with ASD in 1993 and S2 in 1998, the sibling pair (S1, S2) will be discordant in 1993 because S2 is censored at the ASD diagnosis of S1. However, the pair (S2, S1) will be concordant in 1998. The 2 pairs are 2 candidates representing the family. For calculating heritability, 1 of these representative sibling pairs was randomly selected. As a consequence, the algorithm led to a loss of about half of the concordant pairs compared with results under the assumptions and methods applied in the alternate method (analysis method B),3 in which calculating heritability typically does not consider sibling pairs as both discordant or concordant depending on which sibling is considered dependent, but instead follows them as a pair. Liability-threshold models were fitted using monozygotic or dizygotic twins, full siblings, and paternal and maternal half siblings to decompose the variance in liability to ASD into factors for additive genetic effect (inherited additive effects of different alleles), nonadditive (dominant) genetic factors (interaction effects between alleles at the same locus), shared environmental factors (nongenetic influences contributing to similarity within sibling pairs), and nonshared environmental factors (making siblings dissimilar).3 From each family, 1 sibling pair was randomly included. For each pair, ASD status was defined as the presence or absence of ASD at any time point during follow-up. Differences in probability of being diagnosed depended on birth cohort, due to differing time of follow-up and changes in diagnostic practices, and were handled by adjustment for birth year. Models obtained by excluding 1 or more of the 4 genetic and environmental parameters were compared using likelihood ratio tests. The heritability was calculated as the variance associated with the genetic term(s) divided by the total variance. A 2-sided P value of less than .05 was the threshold for statistical significance. Models were fitted using OpenMx (OpenMx Project), version 2.6.9, and R (R Foundation), version 3.3.3. The study included 37 570 twin pairs, 2 642 064 full sibling pairs, and 432 281 maternal and 445 531 paternal half-sibling pairs. Of these, 14 516 children were diagnosed with ASD. The model including additive and nonadditive genetic, shared and nonshared environmental parameters was chosen as the full model under which nested submodels were tested. The best-fitting model included only additive genetic and nonshared environmental parameters (Table). Using this model, the ASD heritability was estimated as 0.83 (95% CI, 0.79-0.87) and the nonshared environmental influence was estimated as 0.17 (95% CI, 0.13-0.21). In the full model, the shared environment variance was estimated as 0.04 (95% CI, 0.00-0.14); nonshared environment, 0.16 (95% CI, 0.05-0.30); nonadditive genetic, 0.10 (95% CI, 0.00-0.38); and additive genetic (heritability), 0.69 (95% CI, 0.40-0.86). Using only twins, the heritability was estimated as 0.87 (95% CI, 0.68-0.96). Twin and family methods for calculating heritability require several, often untestable assumptions.5,6 Because ASD is rare, estimates of heritability rely on few families with more than 1 affected child, and, coupled with the time trends in ASD prevalence, the heritability estimates are sensitive to the choice of methods. The method initially chosen in the previous study2 led to a lower estimate of heritability of ASD. The current estimate, using traditional methods for defining ASD discordance and concordance, more accurately captures the role of the genetic factors in ASD. However, in both analyses, the heritability of ASD was high and the risk of ASD increased with increasing genetic relatedness. Despite these unfavorable trends, there has been progress in this area. The evidence base supporting the benefits of nutrition intervention and behavioral counseling is expanding. Renewed focus on nutrition education in health care professional training is being driven by both student demand and the health care system. Although time pressures and reimbursement remain impediments, incentives and reimbursement options for nutrition and behavioral counseling are growing, and value-based care and health care team approaches hold promise to better align time demands and incentives for long-term care management. Initiatives to integrate clinical care and community resources offer opportunities to leverage resources that alleviate the clinician’s time commitment. There is evidence of some success; for instance, the amount of sugar-sweetened beverages consumed by individuals in the United States has declined substantially over the past 10 years.7. Focus on small steps. Changing lifelong nutrition behaviors can seem overwhelming, but even exceedingly small shifts can have an effect (Table). For example, increasing fruit intake by just 1 serving per day has the estimated potential to reduce cardiovascular mortality risk by 8%, the equivalent of 60 000 fewer deaths annually in the United States and 1.6 million deaths globally.9 Other examples include reducing intake of sugar-sweetened beverages, fast food meals, processed meats, and sweets, while increasing vegetables, legumes, nuts, and whole grains. Emphasize to patients that every food choice is an opportunity to accrue benefits, and even small ones add up. Small substitutions still allow for “treats,” such as replacing potato chips and cheese dip with tortilla chips and salsa, the latter lowering trans fats and saturated fat and increasing whole grain and vegetable intake (Table). Use available resources. Numerous extracurricular resources are readily available for clinicians. The Nutrition in Medicine program offers online, evidence-based nutrition education and tutorials for clinicians and an online, core nutrition curriculum for medical students. The Dietary Guidelines for Americans offers evidence-based and freely available nutrition guidance, tutorials, and tools for clinicians and patients alike. A companion website, Choose My Plate, offers nutrition and counseling advice for clinicians and handy resources for patients, including recently added videos with useful examples of small substitutions that patients will appreciate. Do not do it all at once. Expecting to create long-term behavioral change during a single episode of care is a recipe for frustration and failure, for both the patient and clinician. Empowering and supporting patients is an ongoing process, not a 1-time curative event. Use a few minutes at the close of a patient visit to identify opportunities for future counseling, offer to serve as a resource, and begin a discussion and support that can be reinforced over time. Take solace in knowing that small initial steps can quickly improve health; for example, reducing trans fats at a single meal (eg, replacing baked goods with fruit or nuts or fried foods with nonfried alternatives) promptly improves endothelial function.10. Do not do it all alone. The primary care physician need not be the sole clinician who provides nutrition counseling. Proactive use of physician extenders (eg, physician assistants, nurses, medical assistants, and health coaches) and referrals can alleviate much of the burden for the busy clinician. Receptionists can distribute assessment and screening questionnaires for patients to complete in the waiting room; medical assistants can document behavioral change progress while assessing vital signs; administrative staff can identify and contact patients who are overdue for interaction. Large practices may benefit from including nutrition or health coaches on staff. Referring to clinical specialists and community-based support programs can significantly extend the clinician’s reach.7 In addition to registered dietitians, numerous clinical and community resources are available and often covered by insurance plans. Board-certified obesity medicine specialists, certified diabetes educators, and physician nutrition specialists are available as referrals in many areas. Diabetes Prevention Program group counseling sessions are now covered by Medicare and available throughout communities, such as in many YMCA sites, and electronically. Health care payment systems generally do not provide sufficient or appropriate incentives for the development of antimicrobial drugs. Availability of antimicrobials for resistant bacteria has high public health value. When a patient treated for a resistant infection pays for the use of a drug (through out-of-pocket payments, insurer payments, or both), the payment reflects their own benefit, not the much larger benefit that accrues to the vast majority of individuals who never incur a serious, resistant infection. Availability of antimicrobials and appropriate use in limited populations not only helps prevent the spread of resistant infections, it permits surgeries and other intensive medical interventions because physicians know these agents are available if needed. In the United States, some proposals have focused on a transferable exclusivity voucher (TEV) that would provide a voucher for a 6- to 12-month exclusivity extension on the development and launch of a high-priority antimicrobial drug. The recipient company would be able to apply the exclusivity to a product of its choice, or the TEV could be sold by the recipient to another company. This proposal avoids direct government financing. However, funding is generated by higher drug prices in other therapeutic areas due to longer market exclusivity. Transferable exclusivity vouchers do not generate additional leverage available to ensure ongoing availability and stewardship of the drug. The PAVE award is envisioned as a limited market entry reward that could augment rather than replace payments from public and private payers but would require that those payments transition to value- rather than volume-based methods. That is, the limited market entry reward would provide funds for the first years the product is on the market. For example, in years 1 and 2, the PAVE award would provide the majority of the annual revenue (>75%), but by year 5, the majority of the annual revenue would be expected to come from value-based contracts, with lesser contributions from fee-for-service reimbursement and the PAVE Award. These funds would phase down over the award period (5 years), and their annual continuation would be contingent on manufacturers negotiating contracts with insurers that tie payments to antimicrobial availability and performance in covered populations rather than volume. For example, these approaches could include per-member per-month payments tied to measures of reliable drug availability, effective stewardship (eg, appropriate prescribing and stewardship measures), and continued collection of clinical data. Such alternative payment models for antimicrobials would align with broader US trends in payment reform to focus on value and could be particularly helpful to address the problem of antimicrobial overuse. The population-based payment models would not require payers to pay more but would restructure those payments in a way that better reflects high-value, sustainable antimicrobial use—a US version of the global proposals for “delinkage” of volume of drugs sold to return on investment. ECMO was developed in the 1960s to replace heart and lung function until heart surgery could be performed. It was subsequently used in a venoarterial configuration to provide circulatory support for severe heart failure and in a venovenous configuration to provide gas exchange support for severe lung failure. Venoarterial ECMO has shown promising results for supporting hemodynamic status in patients with cardiogenic shock and cardiac arrest. For those in whom spontaneous circulation is not restored, ECMO has been proposed to preserve peripheral organs for donation after cardiac death.4 Acute respiratory distress syndrome (ARDS) is the most common indication for venovenous ECMO. The roundtable participants suggest the broader term extracorporeal organ support (ECOS) should be introduced to represent all forms of extracorporeal organ support, encompassing kidney, respiratory, cardiac, and liver support (Figure). This new term has implications for clinicians, industry, and researchers. For clinicians, the implication is that ECOS will no longer be compartmentalized to a single organ or function but may be seen as an option available for the treatment of many failing organs. For industry, the implication is that technological platforms need to be developed capable of implementing each of the various extracorporeal support modes. For researchers, the implication is that experimental and clinical models need to be developed to better understand the complex interplay among organs. Cardiac and respiratory ECMO are traditionally delivered as a service in the intensive care unit, whereas the various related clinical issues are managed by external consultants. Moreover, technical support for the management of mechanical ventilation, ECMO, and liver and renal replacement therapies has been provided by different professionals (trained nurses, respiratory therapists, perfusionists). This compartmentalized model should be replaced by a multidisciplinary and multiprofessional model. All forms of ECOS would ideally be included in this model, and managed as 1 tool among many other nonextracorporeal therapies. There has been a robust history of research into ECMO, including long-running research registries such as the Extracorporeal Life Support Organization. However, if ECMO use is to be expanded broadly, it must be justified by demonstrated efficacy in high-quality randomized clinical trials that assess the effects of ECMO on long-term patient-centered outcomes in different patient populations. Such trials would be facilitated by collaboration among burgeoning centers of excellence to prioritize the creation of large national and international, multidisciplinary research networks such as the International ECMO Network. Novel forms of study design may be needed to enhance the power to study complex patients who are expected to have a high rate of progression to organ failure, and who have physiological characteristics suggesting they should respond to the treatment.9. Additional Contributions: We acknowledge the contributions of the roundtable chairpersons (Daniel Brodie and V. Marco Ranieri) and the roundtable participants (Jan Bakker, Laurent Brochard, Michael Broome, Alain Combes, Jacques Creteur, Daniel De Backer, Niall Ferguson, Luciano Gattinoni, John Laffey, Roberto Lorusso, Alain Mercat, Stefano Nava, Laurent Papazian, Antonio Pesenti, Michael Quintel, Peter Rimensberger, Claudio Ronco, Robert Sladen, S. Arthur Slutsky, Thomas Staudinger, S. Fabio Taccone, and Jean-Louis Vincent). Like many of my fourth-year medical student peers, I find myself grappling with how to craft my residency application personal statement. Many struggle to avoid sounding pretentious while still demonstrating their achievements and qualifications. Some wrestle with how to describe meaningful patient interactions to illustrate what draws them to medicine. Still others find it difficult to articulate concisely why they find a given specialty compelling, or what characteristics they desire in a specific program. I have certainly found this to be the case with my own experience. As a medical student, I have suffered two significant bouts of depression and anxiety, the second for which I took a brief leave for treatment. The faculty, staff, and students at my institution have been remarkably supportive throughout this process, and I have been delighted to experience firsthand the edifying responses to mental health difficulties that I read about in responses to the stories of others. Moreover, I have been surprised to never encounter a negative reaction to sharing my story, given the stigmatizing view I have heard many in medicine still hold toward mental illness. Yet in the process of preparing my personal statement for my residency application, I have been disappointed to find that this positive response may not be the case elsewhere. In conversations with mentors about how to craft my statement, many have advised strongly against including this element of my journey, for fear that it could render me a less competitive applicant. None of these mentors have been anything but understanding of my illness, yet they all note that there are those who will view this factor as a decided weakness. When I press further about this to find out who and where these people are, what I hear is always the same frustrating and cryptic response: “Trust me, they are out there.”. Second, those “out there” cause medical students to deprioritize their own health. We are taught to create space for patients with mental illness to feel safe and vulnerable, and that attending to mental illness frankly and openly is a crucial component of overall health. Therefore, when students are tacitly discouraged by those “out there” from extending the same approach to ourselves or our peers in medicine, it follows that we are not pursuing our own health. This dissonance between the health we pursue for our patients and the health we pursue for ourselves indicates either that we hold ourselves to a different understanding of health than those we serve or that our own health must be subordinated to our work as physicians, neither of which is just. Third, and perhaps most importantly, those “out there” undermine the moral component of medicine that is constitutive of good physicians. Since the time of Hippocrates, physicians have not simply been viewed as technicians or providers; they have been endowed with the task to fulfill their duty with integrity and honesty. If we feel compelled to lie to ourselves, our peers, and those reading our applications, we are setting a dangerous precedent for our future moral work as physicians. Students who feel pressured to present a false image of themselves in order to match at a program that may not be well-suited to accommodate their needs potentially do both themselves and the programs to which they apply a disservice. Moreover, such an assumption establishes a pernicious norm for what it takes to advance in medicine, namely to maintain a disingenuous veneer of perfection in the face of the deep imperfections we all hold. The Simplified Severe Sepsis Protocol 2 trial was a parallel-group, nonblinded RCT conducted at a 1500-bed national referral university hospital in Zambia.4 The University of Zambia biomedical research ethics committee and the Vanderbilt University institutional review board granted ethical approval and the trial was overseen by an independent data and safety monitoring board. Written informed consent was obtained from patients or their legally authorized representatives prior to study enrollment. The trial protocol appears in Supplement 1. Quiz Ref IDFrom October 22, 2012, through November 11, 2013, we screened all patients presenting to the emergency department (ED) between 8 am on Monday and 12 pm on Friday. Patients aged 18 years or older were eligible if they had (1) sepsis (defined as suspected infection plus ≥2 systemic inflammatory response syndrome criteria7) and (2) hypotension (defined as systolic blood pressure ≤90 mm Hg or mean arterial pressure ≤65 mm Hg). Based on the results of a prior trial in the same setting,4 we excluded patients with hypoxemia and severe tachypnea (defined as arterial oxygen saturation <90% and respiratory rate >40 breaths per minute). Additional exclusion criteria included gastrointestinal bleeding in the absence of fever, congestive heart failure exacerbation, end-stage renal disease, elevated jugular venous pressure (JVP), incarceration, or the need for immediate surgery.2 Enrollment occurred within 4 hours of the first eligible blood pressure measurement and within 24 hours of ED registration. Study group assignment was generated using computerized randomization in permuted block sizes of 2, 4, and 6. Allocation slips were placed in sealed opaque envelopes, which were opened after informed consent was obtained. Patients, treating clinicians, and clinical study personnel were aware of group assignment after enrollment. Study personnel responsible for outcomes assessment and data analysis were blinded to group assignment. Quiz Ref IDPatients randomized to the sepsis protocol received hemodynamic management for the first 6 hours after enrollment. An initial 2-L bolus of intravenous isotonic crystalloid was administered within 1 hour of enrollment, followed by an additional 2 L over the subsequent 4 hours. After each liter of intravenous fluid was administered, an investigator or study nurse measured arterial oxygen saturation, respiratory rate, and JVP (details appear in the eMethods in Supplement 2). If the arterial oxygen saturation decreased by 3%, the respiratory rate increased by 5 breaths per minute, or JVP reached 3 cm or greater above the sternal angle, fluid infusion was discontinued. The sepsis protocol limited intravenous fluid administration to a total of 4 L, including any fluid given in the ED prior to enrollment. Because most patients were not ambulatory, we measured upper arm circumference in lieu of weight to assess nutritional status.8,9 Study personnel recorded the volume of intravenous fluid administered between ED registration and 6 hours after enrollment, 6 to 24 hours after enrollment, and 24 to 72 hours after enrollment. Patients were followed up until death or 28 days after enrollment. For patients discharged from the hospital, blinded study personnel called the patient or next-of-kin to ascertain vital status at 28 days. Additional details regarding data collection appear in the eMethods in Supplement 2. Quiz Ref IDThe primary outcome was in-hospital mortality. Secondary efficacy outcomes included 28-day mortality and time to death. Secondary safety outcomes included the incidence of worsening hypoxemia or tachypnea (decrease in arterial oxygen saturation of ≥3% or an increase in respiratory rate of ≥5 breaths per minute). Process measures included volume of intravenous fluid administered within 6, 24, and 72 hours; reasons for intravenous fluid discontinuation; and receipt of antibiotics, dopamine, and a blood transfusion. Prospectively collected adverse events included dopamine extravasation, tissue ischemia or necrosis, iatrogenic pulmonary edema, and reaction to blood transfusion. The primary analysis compared in-hospital mortality between the sepsis protocol group and the usual care group using the χ2 test. In secondary analyses, we compared the sepsis protocol group with the usual care group after adjusting for baseline Simplified Acute Physiology Score 3 (SAPS-3)10 and in prespecified subgroups defined by the presence of human immunodeficiency virus infection, Glasgow Coma Scale score at presentation, baseline hemoglobin level, baseline lactate level, baseline SAPS-3, and baseline JVP. Subgroup analyses used the Mantel-Haenszel test for heterogeneity to assess for subgroup × study group interaction effects on the risk of in-hospital mortality. Analyses were performed using Stata version 12.1 (StataCorp). Patients assigned to the sepsis protocol (n = 106) and usual care (n = 103) were similar at baseline (Table 1). Overall, patients were young (mean age, 36.7 years [SD, 12.4 years]) with high human immunodeficiency virus prevalence (89.5%) and low CD4 lymphocyte counts (median, 66/μL [IQR, 21-143/μL] among patients with human immunodeficiency virus). Median albumin level was 2.2 g/dL (IQR, 1.8-2.7 g/dL) and most patients were malnourished, with a median upper arm circumference of 20.1 cm (IQR, 18.4-22.9 cm). Median systolic blood pressure was 83 mm Hg (IQR, 76-87 mm Hg) with a median lactate level of 4.3 mmol/L (IQR, 2.8-7.7 mmol/L). The most common admitting diagnoses were pneumonia (49.3%) and suspected tuberculosis (62.7%), with 80 patients (38.3%) having both. Forty-three patients (20.6%) had positive tuberculosis blood cultures. Details of admitting diagnoses and microbiological data appear in eTables 1 and 2 in Supplement 2. The median time between ED registration and the first dose of intravenous antimicrobial therapy was similar in the sepsis protocol group and in the usual care group (2.0 vs 1.5 hours, respectively; P = .15). In the 6 hours after presentation to the emergency department, patients in the sepsis protocol group received a median of 3.5 L (IQR, 2.7-4.0 L) of intravenous fluid compared with 2.0 L (IQR, 1.0-2.5 L) in the usual care group (mean difference, 1.2 L [95% CI, 1.0-1.5 L]; P < .001). A total of 41 patients (38.7%) in the sepsis protocol group received 4 L or greater of intravenous fluid between ED registration and 6 hours after enrollment. Among the remaining 65 patients (61.3%) in the sepsis protocol group, intravenous fluids were discontinued prior to a total volume of 4 L due to an increase in respiratory rate or a decrease in arterial oxygen saturation (32 patients [30.2%]), JVP of 3 cm or greater (9 patients [8.5%]), blood transfusion through an intravenous line (5 patients [4.7%]), and other reasons (4 patients [3.8%]). In the usual care group, only 50 patients (48.3%) received any intravenous fluid bolus and the most common fluid order was for the administration of 3 L of intravenous fluid over 24 hours (eTable 3 in Supplement 2). The primary outcome of in-hospital mortality occurred in 51 of 106 patients (48.1%) in the sepsis protocol group vs 34 of 103 patients (33.0%) in the usual care group (between-group difference, 15.1% [95% CI, 2.0%-28.3%]; RR, 1.46 [95% CI, 1.04-2.05]; P = .03). Vital status after hospital discharge at day 28 was known for 97 patients in each study group; 28-day mortality was 67.0% with the sepsis protocol vs 45.3% with usual care (between-group difference, 21.6% [95% CI, 8.0%-35.3%]; RR, 1.48 [95% CI, 1.14-1.91]; P = .002). In the multivariable analysis adjusting for SAPS-3 at enrollment, risk of in-hospital mortality (RR, 1.45 [95% CI, 1.04-2.02]; P = .03) and 28-day mortality (RR, 1.41 [95% CI, 1.08-1.84]; P = .01) was greater in the sepsis protocol group than in the usual care group. In the time-to-event analysis, the probability of survival was lower in the sepsis protocol group than in the usual care group (P = .02) (Figure 2 and eTable 4 in Supplement 2). Increased in-hospital mortality among patients assigned to the sepsis protocol was consistent across prespecified patient subgroups (Figure 3). Median hospital length of stay was 5 days (IQR, 3-8 days) in the sepsis protocol group vs 7 days (IQR, 4-12 days) in the usual care group (P = .01). Rates of adverse events were similar between groups (eTable 5 in Supplement 2). This study has several important strengths. The design included randomization to balance baseline confounders, concealed allocation to prevent selection bias, monitoring by an independent data and safety monitoring board, and collection of clinical outcomes by blinded study personnel. Unlike recent trials in high-income countries,12-14 usual care in the current study setting involved limited early fluid or vasopressor administration. As a result, the differences in the volume of intravenous fluid received and receipt of vasopressors between patients in the sepsis protocol group and the usual care group were greater than in any prior sepsis resuscitation trial,2,12-14 strengthening causal inferences between study group and clinical outcomes. This study also has several limitations. First, moderate size and conduct at a single center may exaggerate the observed treatment effect. However, any baseline imbalances between groups that occurred despite randomization appeared to be relatively small (eg, between-group difference in baseline lactic acid concentration of 0.7 mmol/L), and likely do not explain the between-group differences in clinical outcomes. Second, although study enrollment occurred shortly after arrival in the ED, the onset of infection for many patients may have occurred days to weeks before presentation. Third, patients, treating clinicians, and clinical study personnel were not blinded to group assignment. Fourth, the sepsis protocol relied on determination of JVP, a semireproducible skill, and data were not collected on the concordance of JVP measurement between study personnel. Fifth, secondary multivariable analyses relied on the SAPS-3, which has not been validated as a marker for severity of illness in the study setting.10 Sixth, only 1 of 209 patients was cared for in an intensive care unit. Although this reflects the reality of medical care in most hospitals in sub-Saharan Africa, it limits the generalizability to more resource-intense settings. A, Number of sepsis cases with each organ dysfunction and associated in-hospital deaths were n = 49 400 (16 715 deaths) for vasopressor initiation; n = 45 088 (14 290 deaths) for initiation of mechanical ventilation; n = 92 779 (18 345 deaths) for hyperlactatemia (serum lactate level ≥2.0 mmol/L); n = 75 553 (9664 deaths) for acute kidney injury (doubling in baseline creatinine level or decrease in estimated glomerular filtration rate by ≥50%); n = 26 083 (3717 deaths) for hepatic injury (doubling in baseline total bilirubin level to ≥2.0 mg/dL); n = 21 830 (4869 deaths) for thrombocytopenia (decrease in baseline platelet count by ≥50%, with baseline platelets >100 cells/µL). Further details on organ dysfunction criteria are described in the Box. Total number of sepsis encounters, 173 690. B, Number of sepsis cases meeting the specified number of organ dysfunction criteria and associated in-hospital deaths were n = 173 690 (26 061 deaths) for 1 or more organ dysfunction criteria; n = 88 248 (20 687 deaths) for 2 or more organ dysfunction criteria; n = 50 466 (16 506 deaths) for 3 or more organ dysfunction criteria; and n = 29 161 (11 725 deaths) for 4 or more organ dysfunction criteria. Number of organ dysfunction criteria includes different organ dysfunctions that may have occurred at separate times during hospitalization if surveillance criteria were met more than once. Adjusted rates from 2009-2013 calculated relative to observed 2014 rates. Error bars indicate 95% CIs. “Clinical criteria” indicates blood cultures + antibiotics + concurrent organ dysfunction (Box). “Clinical criteria without lactate” excludes the criterion for lactate level of 2.0 mmol/L or greater. Primary trends assessment was conducted using clinical criteria without lactate levels. “Explicit sepsis codes”: discharge diagnoses of severe sepsis (995.92) or septic shock (785.52). “Implicit sepsis codes”: at least 1 infection diagnosis and 1 organ dysfunction diagnosis. All trends adjusted for hospital characteristics (institution, region, teaching status, bed count, annual admissions) and case mix (median age of hospitalized patients, sex and race/ethnicity distributions, proportion of intensive care unit vs total admissions). Veterans Affairs hospitals not included in the trends analysis. Total number of sepsis cases per year was 30 744 (2009), 35 596 (2010), 34 445 (2011), 36 524 (2012), 144 322 (2013), and 145 236 (2014) for clinical criteria; 28 723 (2009), 32 175 (2010), 30 348 (2011), 32 019 (2012), 120 402 (2013), and 112 355 (2014) for clinical criteria without lactate levels; 50 223 (2009), 61 483 (2010), 65 193 (2011), 76 208 (2012), 275 480 (2013), and 272 679 (2014) for implicit or explicit codes; and 9062 (2009), 12 688 (2010), 12 571 (2011), 15 309 (2012), 61 285 (2013), and 65 176 (2014) for explicit codes. As per Sepsis-3 criteria, sepsis was defined as concurrent infection and organ dysfunction.15,16 Suspected infection criteria and the Sequential Organ Failure Assessment (SOFA) score,17 however, were modified to facilitate widespread retrospective surveillance using routinely collected EHR data (Box). Presumed serious infections were defined as a blood culture draw and sustained administration of new antibiotics. Four or more antibiotic days, including at least 1 intravenous antibiotic, were required to identify those most likely to have serious infections and to eliminate patients treated empirically for 48 to 72 hours before culture results were obtained. Fewer than 4 antibiotic days were allowed if death or discharge to hospice or another acute care hospital occurred before 4 days elapsed. Sepsis criteria were met if patients had at least 1 concurrent acute organ dysfunction, defined by initiation of vasopressors or mechanical ventilation, elevated lactate level, or significant changes in baseline creatinine level, bilirubin level, or platelet count. The first antibiotic day and organ dysfunction were required to occur within ±2 calendar days of the blood culture draw. a QADs start with the first “new” antibiotic (not given in the prior 2 calendar days) within the ±2-day period surrounding the day of the blood culture draw. Subsequent QADs can be different antibiotics as long as the first dose of each is “new.” Days between administration of the same antibiotic count as QADs as long as the gap is not more than 1 day. At least 1 of the first 4 QADs must include an intravenous antibiotic. If death or discharge to another acute care hospital or hospice occurs prior to 4 days, QADs are required each day until 1 day or less prior to death or discharge. Sepsis was defined as hospital onset (vs present on admission) if infection and organ dysfunction criteria first occurred on or after hospital day 3. Baseline laboratory values for creatinine, bilirubin, and platelets were estimated using the best value during hospitalization for infection present on admission, or best values within ±2 days of the blood culture for hospital-onset infection. The entire hospitalization was considered a single case of sepsis if surveillance criteria were met multiple times. Septic shock was defined as presumed serious infection concurrent with vasopressors and serum lactate level 2.0 mmol/L or greater. Study physicians reviewed full text medical records from 510 randomly selected hospitalizations, stratified into those that did and did not meet EHR sepsis surveillance criteria. These hospitalizations were drawn from 3 academic centers and 2 community hospitals in Massachusetts and Georgia. Hospitalizations were classified by reviewers as sepsis-positive if there was definite or possible infection and an increase in SOFA score by 2 or more points from baseline as a result of infection, as per Sepsis-3 criteria.15 “Definite infection” required positive cultures or radiography and a compatible clinical syndrome. “Possible infection” required documentation that the medical team presumed the patient’s illness was due to infection, treated for infection, and did not find an alternative etiology. Reviewers were blinded to whether cases were flagged by EHR surveillance criteria and patients’ ICD-9-CM codes (additional details in eMethods 2 in the Supplement). To account for different hospitals contributing data in different years, 2009-2014 trends were modeled using generalized estimating equations to fit Poisson regression models, adjusting for hospital characteristics (institution, region, teaching status, bed count, and annual admissions) and case mix (median age of hospitalized patients, sex and race/ethnicity distributions, and proportion of intensive care unit [ICU] vs total admissions). Generalized estimating equations  were used to account for correlations in the data over time as well as hospital-level clustering. Adjusted rates for 2009-2013 were generated by creating binary indicators for each year in the model, with 2014 as the reference year. The overall change from 2009 to 2014 was summarized as the exponentiated slope of the line between these 2 years on the log scale. All percentages were presented as relative annual changes. VA hospitals were excluded from trends analyses because only data from 2014 were available. Continuous variables were expressed as means. Nonnormally distributed variables were expressed as medians of the medians among the 7 datasets. Analyses were conducted using SAS version 9.3 (SAS Institute) and R version 3.3.1 (http://www.r-project.org). For all analyses, P < .05 (2-sided) was considered statistically significant. The 2014 study cohort included 2 901 019 adult encounters in 409 hospitals, representing approximately 10% of all US adult hospitalizations. Study hospitals’ characteristics are reported in Table 1. There were 423 758 patients with presumed serious infection (14.6% incidence [95% CI, 14.6% to 14.7%), of which 32 574 (7.7% [95% CI, 7.6% to 7.8%]) died in the hospital. There were 173 690 patients with sepsis (overall hospital incidence, 6.0% [95% CI, 6.0% to 6.0%]). The clinical characteristics of patients with sepsis are reported in Table 2. Mean age was 66.5 years (SD, 15.5), and 42.4% (95% CI, 42.2% to 42.6%) were women. Comorbidities were common, including diabetes (35.7% [95% CI, 35.5% to 36.0%]), pulmonary disease (30.9% [95% CI, 30.7% to 31.2%]), renal disease (26.8% [95% CI, 26.7% to 27.0%]), and cancer (19.7% [95% CI, 19.5% to 19.9%]). Most sepsis cases (86.8% [95% CI, 86.7% to 87.0%]) were present on admission. Among hospitals (n = 280) for which culture results were available, 17.2% (95% CI, 17.0% to 17.4%) of patients with sepsis had positive blood cultures. Of the 173 690 patients with sepsis in study hospitals in 2014, 94 956 (54.7%) required ICU care during hospitalization, 27 502 (15.8%) had septic shock, 26 061 (15.0%) died in the hospital, and 10 731 (6.2%) were discharged to hospice (Table 2). Median ICU length of stay was 5 days (range, 2-6). Median hospital length of stay was 10 days (range, 8-12). Hospital mortality was 25.5% among patients with hospital-onset sepsis vs 13.4% for patients with sepsis present on admission (difference, 12.1% [95% CI, 11.5% to 12.7%]; P < .001). Sepsis was present during hospitalization in 34.7% of the 75 079 study patients who died in the hospital in 2014. When adjusting for hospital region, size, and teaching status, the estimated national weighted incidence of sepsis was 5.9% (95% CI, 5.5% to 6.3%), and the in-hospital mortality rate was 15.6% (95% CI, 14.8% to 16.5%) (strata-specific counts reported in eTable 3 in the Supplement). Mortality rates for septic patients were higher for older patients, men, teaching hospitals, and larger hospitals (eTable 4 in the Supplement). Among the 13 Sepsis-3 cases missed by EHR criteria, the most common reason was hypoxemia causing an increase in SOFA score of 2 or more points without need for mechanical ventilation (eTable 6 in the Supplement). None of these 13 patients died. There were 57 false-positives flagged by EHR criteria; most were because clinicians initially suspected infection but medical record reviewers ultimately deemed no infection was present or because patients’ organ dysfunction did not increase SOFA score by 2 or more points (typically patients with elevated lactate levels alone) (eTable 7 in the Supplement). If sepsis was defined as clinically suspected infection with organ dysfunction at the thresholds specified by EHR criteria, PPV was 87.9% (95% CI, 82.7% to 92.0%). The trends analysis included 7 801 624 hospitalizations between 2009-2014 (Figure 2). The annual incidence of sepsis (without the lactate criterion) was stable (+0.6% relative change/y [95% CI, −2.3% to 3.5%], P = .67). In-hospital mortality decreased (−3.3%/y [95% CI, −5.6% to −1.0%], P = .004), but there was no significant change in the combined outcome of death or discharge to hospice (−1.3%/y [95% CI, −3.2% to 0.6%], P = .19). Patients with sepsis were increasingly discharged to hospice over time (+6.3%/y [95% CI, 1.1% to 11.6%], P = .02). This study also has several limitations. First, EHR-based surveillance may be affected by differences in clinical practice between clinicians and hospitals as well as changes over time. This may be most evident with rising rates of lactate testing, but earlier initiation of vasopressors for hypotension and increasing use of noninvasive ventilation and high-flow nasal cannulas for respiratory failure could also affect estimates. The partial dependence on clinician-initiated interventions to measure organ dysfunction, however, is also a limitation of the SOFA score and thus the Sepsis-3 criteria. Seventh, medical record reviews suggest that EHR-based surveillance may miss up to 30% of patients with sepsis while misclassifying another 30%. Quantifying the accuracy of sepsis criteria is elusive, however, because there is no true gold standard for sepsis.30,39,40 Recognizing this problem, the Sepsis-3 task force developed and validated criteria based on associations with adverse outcomes.15,16 The EHR surveillance definition in this study carried high mortality rates (15%) compared with all encounters with presumed infection (8%), and mortality increased with increasing numbers of dysfunctional organs. On medical record reviews, the Sepsis-3 cases missed by EHR surveillance involved mild organ dysfunction, such as hypoxemia without need for mechanical ventilation, and no patients in that group died. False-positives were most often attributable to reviewers adjudicating the absence of infection despite patients receiving blood culture draws and antibiotics, or a noninfectious cause of organ dysfunction. When sepsis was defined as organ dysfunction concurrent with clinically suspected infection (as is common in practice), PPV of the surveillance definition increased to 88%. Eighth, neither Sepsis-3 criteria nor EHR-based clinical surveillance can solve the challenge that clinicians routinely face in deciding whether their patient is infected and whether organ dysfunction is due to infection. Instead, EHR surveillance provides a consistent gauge to estimate sepsis incidence and outcomes using readily available, objective clinical data. This method cannot help clinicians identify sepsis at the bedside since it is retrospective, but it may be useful for public health surveillance, hospital evaluation, and assessing the effects of quality improvement efforts. EHR-based surveillance may further support these objectives by facilitating granular evaluation of the timing of sepsis onset and interventions. If sepsis was a homogenous condition and the effectiveness of therapy was uniform, such paucity of data would not be troubling. However, neither is the case. Sepsis is a broad syndrome of acute organ dysfunction arising due to a complex host response to a wide variety of inciting pathogens, with the result that both the presenting symptoms and subsequent course are highly variable. Furthermore, the effectiveness of each element of the antimicrobial, resuscitation, and organ support therapies used to treat sepsis appears to be highly conditional on these host and pathogen factors, as well as on what other therapies are deployed, and on the capacity of the health care setting to deliver therapies optimally and monitor, prevent, and treat complications. In many low- and middle-income countries, patients who develop sepsis may be much more likely to have malnutrition, HIV, or malarial infection. These patients may also develop sepsis secondary to a spectrum of infections different from that seen in high-income countries, such as highly resistant gram-negative infections, dengue, melioidosis, or viral hemorrhagic fevers. Patients may also incur extreme delays before receiving definitive care, which may be lacking basic resources, such as vasopressors or oxygen, as well as artificial respiratory or kidney support. It is therefore extremely valuable to determine how well even the most basic therapeutic strategies work in these settings. The intervention was modeled on the early goal-directed therapy (EGDT) protocol tested by Rivers et al5 at Henry Ford Hospital in Detroit, Michigan, but with a number of important alterations. Similar to EGDT, the intervention consisted of a 6-hour resuscitation protocol initiated at the time of enrollment and beginning with a bolus of intravenous saline, but provided 2 L, which is potentially a greater volume than the 20 to 30 mL/kg typically used in EGDT. After the initial bolus, the EGDT protocol titrates fluid, blood, and vasoactive agents based on blood pressure, central venous pressure, and central venous oxygen saturation. Because such monitoring was unavailable in Zambia, the protocol tested by Andrews et al4 followed the initial bolus with an additional 2 L administered over the next 4 hours regardless of an improvement in hypotension and only interrupted fluid administration if arterial oxygen saturation decreased by 3%, the respiratory rate increased by 5 breaths per minute, or the jugular venous pressure was 3 cm or greater above the sternal angle, all of which had to be determined hourly. Patients in the sepsis protocol group received more intravenous fluid (median, 3.5 L [interquartile range, 2.7-4.0 L]) compared with patients in the usual care group (median, 2.0 L [interquartile range, 1.0-2.5 L]; P < .001) and more frequently received dopamine (14.2% vs 1.9%, respectively; P < .001); however, administration of blood transfusions was similar in the 2 groups. For 65 patients (61.3%) in the sepsis protocol group, intravenous fluids were stopped before 4 L, most commonly because of the fluid overload triggers (n = 32; 30.2%). In the usual care group, only about half of the patients received any intravenous fluids (n = 50; 48.3%). More patients in the sepsis protocol group experienced worsening hypoxemia or tachypnea (35.8%) compared with the usual care group (22.3%; P = .03). Only 1 patient was admitted to an intensive care unit (ICU), which is not surprising given that there are only 10 ICU beds in Zambia, a country of 13 million people.4. However, there are some important caveats. First, the study was conducted at a single center, and thus is open to many important issues regarding generalizability, including the training and availability of staff and the adequacy of resources. Second, the patients were selected based on rather crude measures of shock. Many patients did not have an elevated lactate level, and blood pressure may have been artificially low if malnutrition was highly prevalent. Indeed, blood pressure improved in a large portion of patients in the usual care group without obvious intervention. Third, most patients had preexisting HIV infection with a low CD4 count, and only half were receiving antiretroviral therapy; this severe underlying health state may have contributed to the study findings. Fourth, the protocol mandated a fixed amount of intravenous fluid volume triggered by an initial finding of hypotension but was not guided by ongoing signs of shock or hypoperfusion, and the large volume of intravenous fluids may have caused pulmonary edema in some patients. Fifth, dopamine is commonly used in Africa, but is associated with more adverse effects than the traditionally recommended vasopressor, norepinephrine. However, dopamine use is unlikely to explain the differences in outcome. Sixth, because the study was not blinded, a variety of uncontrolled interventions could have been deployed differently between the study groups, confounding the results. One interesting feature of this study is that patients were typically enrolled before any meaningful resuscitation. As such, all patients in the protocol group received a volume bolus, whereas half of the patients in the control group received no fluids whatsoever. In contrast, all the EGDT trials provided a volume bolus to all patients before randomization. Because the bolus and subsequent fluid administration may have been too high, it is not possible to infer that smaller boluses are harmful. However, it is notable that the only study that explicitly tested a volume bolus was also conducted in Africa, albeit in children with severe anemia,9 and found that a fluid bolus was harmful. The conventional approach for resuscitation of septic shock is to commence with a volume bolus, but these studies together with a recent study conducted in New York State10 that found no association between early fluid administration and outcome raise the possibility that the size and type of bolus may be worthy of further study. The study by Andrews et al4 is only the latest to fail to find benefit from a protocol approach to resuscitation. However, these findings should not lead to the inference that a sepsis resuscitation protocol should be abandoned. The recent EGDT trials were all conducted in settings where usual care may have been excellent, and the study by Andrews et al may have inadvertently delivered a protocol that happened to include harmful elements. These data do not mean that a resuscitation protocol approach is inherently harmful. Indeed, the principle of trying to replicate high-quality decision making through a set of reproducible instructions and actions, especially in settings with low resources or where training may be less than ideal, remains a bedrock of quality improvement not only in medicine but in many other domains of human engineering. The challenge is making sure that the protocol includes the right instructions. Practical methods available to quantify the burden of sepsis include clinical diagnosis through prospective cohort studies or retrospective chart reviews, the use of administrative data (International Classification of Diseases [ICD] codes associated with hospital discharges, emergency department visits, or reimbursement claims), and, where they are available, interrogation of electronic health records (EHRs). To date, most large-scale population-based studies have used administrative data and identified cases of sepsis via an “explicit” diagnosis if the patient was allocated an ICD code for severe sepsis (International Classification of Diseases, Ninth Revision, Clinical Modification [ICD-9-CM] code 995.92) or septic shock (785.52), or an “implicit” diagnosis when allocated a code indicating infection together with a code indicating acute organ dysfunction (eg, ICD-9-CM code 569.83, perforation of intestine, combined with 584, acute renal failure). The majority of such studies have used a combination of codes taken or adapted from the approach used by Angus et al.8 One important concern with this methodology is the inability to determine a causal relationship between infection and acute organ dysfunction, which might result in overcounting sepsis cases. A more recent concern with this methodology is so-called upcoding, in which the diagnosis of sepsis may be sensitive to changes in awareness of the diagnosis and may be influenced by financial incentives if coding for sepsis results in higher reimbursement. In this issue of JAMA, Rhee and colleagues10 report the findings of a carefully conducted and analyzed study that seeks to determine whether the reported trends in sepsis incidence and mortality in the United States are real. Additionally and importantly, the authors provide estimates for the accuracy of 2 administrative data–based approaches in estimating the total number of sepsis cases. The study included more than 7 million adult hospitalizations over a 5-year period at 409 hospitals representative of all US hospitals in terms of geographic region, size, and teaching status. The authors generated estimates for sepsis incidence and mortality from 2009-2014 using 3 methods. The first method was based on an analysis of EHR clinical data, which the authors termed a “clinical surveillance” definition. The second method involved an administrative data–based method that replicated the methodology of Angus et al8 to identify sepsis through explicit or implicit ICD-9-CM coding. The authors refer to these as “claims-based” definitions, a somewhat US-centric view, as the ICD evolved from the International List of Causes of Death adopted by the International Statistical Institute in 1893. Entrusted to the WHO on its founding in 1948, its primary use globally is for epidemiology, health planning, and quality purposes rather than for financial claims. The third method used by Rhee et al was retrospective clinician chart review of 510 randomly selected hospitalizations, stratified by EHR definition (yes or no), drawn from 3 academic centers and 2 community hospitals. The authors reported the incidence and mortality rates derived from the EHR and administrative data approaches and assessed the sensitivity, specificity, and positive and negative predictive values of these approaches using clinician chart review as the gold standard. For the EHR clinical surveillance–based approach, sepsis was defined as presumed infection leading to serious organ dysfunction. “Presumed infection” required the combination of a blood culture draw and at least 4 consecutive days of antibiotic administration. In keeping with the Sepsis-3 criteria, “organ dysfunction” was identified by an increase in the SOFA score of at least 2 points, with a score of at least 2 points in a single organ system. Clinical characteristics such as hypoxemia without mechanical ventilation, thrombocytopenia with platelet count 100 to 150 cells/μL, hypotension not requiring vasopressors, and altered mental status, although components of the SOFA score, were not included in the surveillance definition. The authors report that in 2014, a total of 173 690 cases of sepsis were identified using EHR clinical criteria among 2 901 019 adults admitted to study hospitals (overall incidence, 6%). Of the patients identified with sepsis, 26 061 (15.0%) died in the hospital and 10 731 (6.2%) were discharged to hospice. The authors highlight that from 2009-2014, the administrative data (explicit sepsis/septic shock codes) support an increasing incidence (+10.3% per year) and declining in-hospital mortality (−7.0% per year) from sepsis, whereas EHR data (clinical) suggest a stable incidence with a significant but lesser decrease in mortality (incidence, +0.6% per year; mortality, −3.3% per year). As the authors suggest, by using a consistent methodology to analyze data that are less subject to ascertainment error and less subject to bias or gaming than coding, the EHR clinical data are more likely than the coding data to produce accurate data for trends over time. Based on the chart review of the 510 randomly selected hospitalizations, neither method accurately identified all cases of sepsis; both the EHR clinical method and counting both explicit and implicit cases by coding missed approximately 20% of sepsis cases, whereas counting only explicit cases identified by coding missed approximately 40%. Conversely, the EHR clinical method had a 20% false-positive rate, thus arriving at the same total number of cases as chart review; the explicit/implicit coding method had a much higher false-positive rate and would result in a sepsis count twice that identified by chart review. The data on sepsis incidence and mortality reported by Rhee et al are informative regarding the disease burden in the United States; however, sepsis is also a major public health issue at the global level, and the global burden of sepsis is poorly documented. To date, sepsis has not been featured in the Global Burden of Disease statistics, with deaths due to sepsis being attributed to other primary causes, such as underlying infection. Recognizing the global importance of sepsis, new collaborations are needed to develop separate estimates for the global epidemiology of sepsis. Although EHRs are common in the United States and increasingly common in other high-income countries, they are rare in many low- or middle-income countries, where approximately 87% of the world’s population lives. In most countries, analysis of ICD coding remains the only practical method available to quantify the burden of sepsis. Notably, the WHO resolution urges member states “to apply and improve the use of the ICD system to establish the prevalence and profile of sepsis.”4 The study by Rhee et al not only provides a clearer understanding of trends in the incidence and mortality of sepsis in the United States but also a better understanding of the challenges in improving ICD coding to accurately document the global burden of sepsis. We are pleased to announce that in early 2018, The JAMA Network will launch a new journal—JAMA Network Open. Our editorial goal is to publish the very best clinical research across all disciplines, serving the worldwide community of investigators and clinicians and meeting the evolving needs and requirements of authors and funders. With the launch of JAMA Network Open, we simultaneously assert our editorial commitment to excellence and to the authorship community regardless of requirements of funders. This will be a fully open access journal and follows the launch of JAMA Oncology in 2015 and JAMA Cardiology in 2016, which are hybrid journals offering open access options for research articles.1,2 Frederick P. Rivara, MD, MPH, current editor in chief of JAMA Pediatrics, will be the editor in chief of JAMA Network Open. Medical journals and publishing have changed substantially during the past 2 decades following the emergence and ascendance of the internet and the mobilization of journal content. Journals have gone “digital”—virtually all journals release content online ahead of or instead of print, and connect with their readers via electronic table of contents and through social media. Open access publishing has risen to prominence concurrent with these changes, with a primary goal to make research findings more accessible to potential users around the world. In addition, many funders now require that authors publish their results in open access journals, and this requirement will almost assuredly expand in the future. Our decision to launch an open access journal was based on careful thought and planning and represents our response to the rapidly evolving landscape of scientific discovery and medical journals and publishing. Between 2008 and 2016, the number of major articles indexed annually in Web of Science increased from 984 350 to 1 364 453 (Figure, A). During the same time, at JAMA and The JAMA Network specialty journals the number of research manuscripts submitted increased from 9006 to 14 676, but the number of research manuscripts published declined from 1739 to 1335 (Figure, B). In 2017, following the launches of JAMA Oncology and JAMA Cardiology, we estimate that more than 17 000 research manuscripts will be submitted to JAMA and the 11 specialty journals and approximately 1500 will be published. Clearly, the world’s research effort has increased substantially during the past decade. The number of articles published in JAMA and the 11 specialty journals is necessarily limited, and JAMA Network Open will provide another high-quality option for authors of research articles in disciplines already represented by our current titles as well as expanding opportunities for authors of manuscripts in other important disciplines. Following the launch of JAMA Oncology and JAMA Cardiology, advocates for additional new journals in The JAMA Network approached us with thoughtful proposals. Many disciplines are not represented in our current collection of titles, and thus there are opportunities to publish more discipline-specific journals. Rather than launch another subject-focused title, our preference is to expand The JAMA Network publishing platform to encompass all of clinical medicine and health care. The new JAMA Network website has made this decision possible3 because regardless of where an article is published on the site, content in a specific discipline is easily discoverable. In addition, it is now possible to search for a topic simultaneously across all of our titles or across selected journals. Do clinical oncologists care if an article is published in JAMA, JAMA Oncology, or JAMA Internal Medicine? Not likely—but they do care if the content is relevant to their practice and is discoverable and accessible. JAMA Network Open will be a general medical journal that includes content from many disciplines, featuring scientific, medical, and health content in more than 40 subject areas (Box). About 25% of JAMA Network Open articles will be accompanied by open access Invited Commentaries because clinicians and investigators have told us that they value the insights of opinion pieces, and open access online commenting will be available for all articles. Authors will continue to be able to submit manuscripts to JAMA and 1 of the specialty journals and request the option for automatic transfer to another journal if the initial journal does not accept the manuscript, including JAMA Network Open. The addition of JAMA Network Open will enhance JAMA and the 11 specialty journals and broaden The JAMA Network. The JAMA Network Open website will be similar to the websites for the other JAMA Network journals: it will use responsive design and be easy to search, access, and read via computer, tablet, or smartphone. The computer desktop HTML version of articles will display in split screen, with the text and links of articles on the left and tables, figures, references, and multimedia on the right. We will continue to publish PDF versions of articles for researchers and readers who prefer that format. Changes have been made to ensure that content will be easily discoverable based on the interest of a reader. The JAMA Network journal websites will continue to learn the preferences of a user, so related content will be highlighted to match the interests of that individual. This is especially important for a journal that will have content from many different specialties. Circulating cell-free DNAQuiz Ref ID (cfDNA) was discovered in plasma more than 60 years ago.1 Several decades later, abnormalities in cfDNA obtained from cancer patients were described and are now known as ctDNA.2 Most cfDNA in the circulation is derived from ruptured nonmalignant cells and is of germline origin. ctDNA is thought to be a result of tumor cell apoptosis and necrosis. Analysis of ctDNA can identify tumor-specific DNA abnormalities that can be used as a basis for highly specific disease testing strategies. Detecting the small amount of tumor-derived mutations in cfDNA has been a major limitation in the clinical application of ctDNA testing. Recently developed novel genomic and bioinformatic approaches have facilitated highly sensitive molecular assays that can detect tumor-specific aberrations from ctDNA.3-5. Next-generation sequencing (NGS) provides high-throughput analysis of a large number of DNA molecules in a single reaction.6 NGS can typically detect tumor-specific mutations in the whole genome, exome, or a panel of genes in tissue specimens where mutations are present in at least 5% of the cells analyzed. Because mutations in ctDNA are difficult to detect and may occur in less than 1% of the cfDNA molecules analyzed, new approaches have been developed that use much deeper sequencing (each base is analyzed redundantly >20 000 times) to detect alterations present at frequencies as low as 0.05%.3-5 These methods have the advantage of being able to test for a variety of aberrations in key genes through sequencing of DNA that has been amplified or captured through hybridization from regions of the genome. Targeted deep sequencing combined with analyses that remove sequencing errors allow for accurate detection of a broad range of sequence alterations. New NGS methods may also be used for detection of DNA rearrangements or copy number changes in ctDNA,7 because these alterations represent important therapeutic targets in many cancers. Extension of these targeted approaches to whole-exome or genome strategies in plasma have identified novel genomic aberrations in tumor DNA that may have been selected for cell survival during cancer therapy—similar to the selection of resistant microorganisms resulting from antibiotic use. Digital polymerase chain reactionQuiz Ref ID (PCR) is an approach to quantify specific alterations in ctDNA by separating one PCR reaction into multiple reactions, each with a single or small number of DNA molecules.8 In this process, PCR amplifies cfDNA fragments, and fluorescent probes are used to differentially bind to mutant and wild-type sequences. Each reaction is scored based on the presence or absence of mutant or wild-type signal, and the proportion of different signals represents the prevalence of the mutant and wild-type alleles. Recent refinements of digital PCR technology have allowed higher accuracy and throughput. These methods provide high sensitivity but are limited by the requirement of prior knowledge of specific point mutations or copy number variants to be analyzed. In a study of patients with advanced lung cancer, the detection limit by digital PCR of EGFR, KRAS, or BRAF mutations was approximately 0.01% of mutant to wild-type DNA molecules.9 The relatively low cost and rapid turnaround time of a few days make digital PCR a useful platform for clinical applications in lung and other cancers. Quiz Ref IDctDNA analyses have been approved by the US Food and Drug Administration (FDA) for use in lung cancer patients for the initial genotyping of tumors when not enough tissue is available to characterize the molecular composition of the tumor. These analyses may be more cost-effective than multiple tissue biopsies and may greatly reduce risks associated with biopsies. The Cobas EGFR test (Roche) was approved by the FDA in 2016 as the first diagnostic plasma test for detecting multiple EGFR mutations for identifying patients with non–small-cell lung cancer eligible for treatment with EFGR inhibitors.10 In 2015, the Therascreen EGFR kit (Qiagen) received Conformité Européene in vitro diagnostic approval for an in vitro diagnostic medical device to include the liquid biopsy companion diagnostic for detecting EGFR mutations in patients from whom tumor tissue cannot be obtained to facilitate targeted treatments. While these tests are limited to a specific gene, expanded ctDNA analyses using NGS gene panels for broader mutation profiling and therapeutic target selection are now available for clinical use as laboratory-developed tests, and some are likely to become FDA approved. National Comprehensive Cancer Network guidelines currently recommend plasma testing for non–small-cell lung cancer patients in certain circumstances, and other cancers may soon follow. Initial applications include cancer genotyping and early resistance monitoring for patients who cannot have biopsies performed because of safety concerns or who have insufficient tissue for genomic characterization. Detection of ctDNA in patients with recurrence may precede radiographic detection of cancers by more than 6 months, facilitating earlier treatment. Research is under way to identify early indicators of treatment response, evaluate economic cost-benefit analyses of these approaches, and uncover methods for early detection of cancer in high-risk populations.5. The other long-acting muscarinic antagonists (LAMA) + long-acting β-agonists (LABA) indicates tiotropium plus olodaterol, umeclidinium plus vilanterol, tiotropium plus indacaterol, tiotropium plus salmeterol, tiotropium plus formoterol, glycopyrronium plus indacaterol, or aclidinium plus formoterol. In all studies, LABA plus inhaled corticosteroids (ICS) was salmeterol plus fluticasone. A Mantel-Haenszel random-effects model was used. The size of the squares indicates the weight of each study. This JAMA Clinical Evidence Synopsis summarizes a Cochrane review1 that assessed inhaled long-acting muscarinic antagonists (LAMA), long-acting β-agonists (LABA), and inhaled corticosteroids (ICS). These are important medications to treat stable chronic obstructive pulmonary disease (COPD). These medication combinations (LAMA + LABA or LABA + ICS) can be administered via 1 inhaler. The LABA + ICS therapies (eg, salmeterol + fluticasone) are the preferred option for patients who are at high risk of COPD exacerbation events.2 However, it is unclear which of the 2 combined inhaler treatments is associated with better outcomes.3-5. First, the statistical power was insufficient for some of the outcomes. Second, some of the studies did not include data on long-term follow-up. Third, there was significant heterogeneity among the included studies. Fourth, the results may not be generalizable to patients who did not qualify for the included studies. Fifth, the risk of exacerbation was reduced only in participants who received glycopyrronium + indacaterol. A 62-year-old woman with a 10-year history of hypertension presented with elevated serum creatinine (2.26 mg/dL; reference range, 0.6-1.2 mg/dL). She had not visited a physician for several years but reported feeling well with no active symptoms. She had no risk factors for hepatitis B, hepatitis C, or HIV and reported no use of nonsteroidal anti-inflammatory drugs or herbal medications. One year ago, her urine was tea-colored during an upper respiratory tract infection. Examination results were normal except for elevated blood pressure (154/90 mm Hg). Although her hemoglobin concentration was low (10.5 g/dL; reference range, 11.6-15.2 g/dL), white blood cell count, platelet count, and electrolytes were normal. Her kidneys appeared structurally normal on ultrasound. Urine studies were performed (Table). A kidney biopsy confirmed IgA nephropathy. The patient began taking an angiotensin converting enzyme inhibitor to reduce proteinuria and manage hypertension. Five months later, her kidney function remained stable, blood pressure was well controlled, and proteinuria was 0.5 g per day. She continues biannual follow-up with a nephrologist and her glomerular filtration rate remains stable. Investigators evaluated inotuzumab ozogamicin in a phase 3 trial involving 326 patients with relapsed or refractory B-cell ALL. Among 218 patients in a remission analysis group, about 81% who received the combination achieved complete remission compared with 29% who were treated with standard intensive chemotherapy. Remission lasted a median of 4.6 months in the inotuzumab ozogamicin group compared with 3.1 months in the chemotherapy group. In the inotuzumab ozogamicin group, 41% went on to have hematopoietic stem cell transplantation—considered the only cure for B-cell ALL—compared with 11% in the chemotherapy group. “Gout is a serious and potentially progressive and debilitating disease,” Michael A. Becker, MD, professor emeritus of medicine at the University of Chicago Medicine and an investigator in a trial of the drug combination, said in a statement. “Getting patients with gout to serum urate goal, and keeping them at or below goal, are essential to success in treating these patients. Duzallo will help reduce the significant unmet need among patients in the US who fail to get their serum uric acid levels to goal despite taking allopurinol alone.”. In a phase 3 trial, 610 patients with gout who already were taking allopurinol were randomly assigned to receive 200 mg or 400 mg of lesinurad once daily or a placebo. Some 67% of patients taking 400 mg of lesinurad along with allopurinol reached the serum uric acid target of less than 6 mg/dL within 6 months. In comparison, 55% taking 200 mg of lesinurad with allopurinol and 23% in the placebo group reached the target. Treatment effects lasted throughout the 12-month study. This retrospective cohort study obtained ethical approval from the institutional review board of Taiwan Chang Gung Memorial Hospital and was conducted in full compliance with national ethical and regulatory guidelines. The institutional review board determined that patient consent was not required because all data were anonymized by the data holder, the Taiwan National Health Insurance Administration (NHIA). The Taiwan NHI system was established in 1995 as a single-payer insurance system co-funded by the government, employers, and beneficiaries. All citizens and foreigners living in Taiwan for more than 6 months are required by law to enroll in NHI. At the end of 2016, approximately 23 million beneficiaries were registered in NHI, which is equivalent to a coverage rate of 99.5%. Novel medications, such as NOACs, are often approved and reimbursed by NHI, especially once clinical trials began providing evidence of efficacy and safety. Since 1995, the NHI database has recorded comprehensive registration information and claims data, which include patient characteristics, medical diagnoses, prescription details, examinations, operations, procedures, and fees incurred. The whole database is linked by the unique national personal identification, which was anonymized before its release for research use to prevent confidentiality leaks. The anonymized national personal identification remains consistent across the NHI database and between government-held data sets, allowing valid internal and external linkage.15 The diagnoses and procedures were recorded using the International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM) codes from 1997 through 2015 and the International Classification of Diseases, 10th Revision, Clinical Modification (ICD-10-CM) codes since 2016. In this study, each calendar year was partitioned into 4 quarters for each patient and each year after the first prescription of a NOAC. The analytic unit was 1 person-quarter.16 Person-quarters were used because medications for chronic illnesses were refilled with a maximum length of 3 months per the Taiwan NHI reimbursement policy. Medications and covariates were assessed for each person-quarter, which simplified the assessment of the complex prescription pattern of NOACs and multiple drugs. Person-quarters exposed to NOACs with or without concurrent medications were identified. The major bleeding risks of person-quarters exposed to NOACs and 12 concurrent medications (atorvastatin; digoxin; verapamil; diltiazem; amiodarone; fluconazole; ketoconazole, itraconazole, voriconazole, or posaconazole; cyclosporine; erythromycin or clarithromycin; dronedarone; rifampin; and phenytoin) were compared with person-quarters exposed to NOAC alone. Quiz Ref IDThese medications were selected because they were P-glycoprotein competitors (digoxin, verapamil, diltiazem, amiodarone, and cyclosporine), CYP3A4 inhibitors (fluconazole and ketoconazole, itraconazole, voriconazole, or posaconazole), or both (atorvastatin, erythromycin or clarithromycin, dronedarone) or CYP3A4 inducer (rifampin and phenytoin), which may have a potential drug-drug interaction with NOACs.11,14,17-23. Quiz Ref IDThe primary outcome was major bleeding, defined as a hospitalization or an emergency department visit with a primary diagnosis of intracranial hemorrhage or gastrointestinal, urogenital, or other bleeding, as previously described.24 People with traumatic hemorrhage were excluded from analysis. Only 1 major bleeding event was included in each person-quarter. Secondary outcomes included site-specific bleeding. Details of case definitions for the primary outcome are listed in eTable 1 in the Supplement. Patient demographics, comorbidities, relevant medications, and health care utilization were identified as covariates.15,25 These covariates were assessed for each person-quarter pertinent to the first date of the person-quarter. Patient demographics included age, sex, and socioeconomic factors (residence, income level, and occupation). The components of the Charlson comorbidity index (range, 0-37; a score of 5 points or more has a 1-y mortality rate of 85%26),27 other comorbidities (myocardial infarction, congestive heart failure, peripheral vascular disease, stroke, transient ischemic attack, dementia, chronic pulmonary disease, anemia, kidney diseases, and hepatic diseases), components of HAS-BLED (hypertension, abnormal kidney or liver function, stroke, bleeding history, and alcohol use), number of outpatient visits, proton pump inhibitors, aspirin, clopidogrel, ticlopidine, warfarin, nonsteroidal anti-inflammatory drugs, glucocorticoids, insulin, oral hypoglycemic agents, antihypertensives, and lipid-lowering agents were also assessed. The code lists of these covariates are shown in eTable 1 in the Supplement. Quiz Ref IDConfounding by indication, which results from nonrandom treatment allocation for concurrent medications, was an essential consideration in the comparison of major bleeding risk among patients with NOAC use who were exposed vs unexposed to concurrent medications.28,29 The inverse probability of treatment weighting using the propensity score was applied to account for this bias.30 The propensity score was the probability that a patient was prescribed the concurrent medication during a person-quarter. For each person, a specific propensity score for a specific concurrent medication was calculated using logistic regression considering the aforementioned covariates pertinent to the first date of the person-quarter. Standardized differences were estimated to assess the balance of individual covariates before and after propensity score weighting. The balance of covariates was assessed using the absolute standardized mean difference. The negligible difference was defined as an absolute standardized mean difference less than 0.1. eTables 2A–L in the Supplement summarize the balance of covariates between users and nonusers of each specific concurrent medication. Poisson regression with a generalized estimating equations model to account for intra-individual correlation across person-quarters was used to calculate the adjusted incidence rate difference, incidence rate ratios, and 99% CIs with consideration of the inverse probability of treatment weighting using the propensity score. Person-quarters using NOAC alone were used as the reference category. Because 12 types of combinations were studied, the regression analysis was performed separately for each combination. In addition, a different definition of major bleeding—a hospitalization or emergency visits due to major bleeding recorded in the primary or secondary diagnosis—was applied as a sensitivity analysis. Three additional analyses were conducted to ascertain the association of a NOAC plus concurrent medications and major bleeding: (1) The associations of a NOAC plus specific concurrent medications with bone fractures due to vehicle crashes (not related to the NOAC). (2) The association of the combination of losartan (a medication to replace NOAC in the model) plus concurrent medications with major bleeding (for details, see eTable 6C and 6D in the Supplement). (3) The association of a NOAC plus concurrent medication groups (ie, P-glycoprotein competitors group or CYP3A4 inhibitors group) with major bleeding. The Bonferroni method was used to consider a type I error due to multiple comparisons. Three significance levels were used for hypothesis tests: .05, .01, and .005. The results were similar and the significance level of .01 was chosen to be reported in the main text. Estimates based on the alternative significance levels are reported in the eTables in the Supplement. Missing data were present among patients without a valid insurance status (estimated in <0.1% of NOAC users), and data associated with these patients were excluded. The entire analysis was performed using SAS (SAS Institute), version 9.4. During 2012 to 2016, a total of 279 734 patients with nonvalvular atrial fibrillation were identified. Among them 91 330 patients received NOACs. The characteristics of the patients with nonvalvular atrial fibrillation at the first date of NOAC prescription are listed in Table 1 and Table 2. The mean age was 74.7 years (SD, 10.8), and 55.8% of the studied population were men. The baseline average CHA2DS2-VASc (congestive heart failure, hypertension, age ≥75 [doubled], diabetes mellitus, prior stroke or transient ischemic attack [doubled], vascular disease, age 65-74, female) stroke score (range 0-9, males with score >1 may consider anticoagulation)31 was 3.9 (SD, 1.8) and the average HAS-BLED (hypertension, abnormal kidney or liver function, stroke, bleeding history, and alcohol use; range 0-9, a score >3 have higher bleeding risk)32 score was 3.3 (SD, 1.3). More than one-third of the included patients were diagnosed with heart failure or cerebrovascular disease and a quarter with diabetes. There were 45 347 patients (49.7%) exposed to dabigatran, 54 006 patients (59.1%) to rivaroxaban, and 12 886 patients (14.1%) to apixaban during the follow-up period. During follow-up, 4770 major bleeding events occurred during 447 037 person-quarters with NOAC prescriptions. The major bleeding events included 1177 intracranial and 3341 gastrointestinal bleedings and 182 events occurred in other sites. Table 3 summarizes the incidence rate, adjusted incidence rate, and adjusted incidence rate difference for major bleeding among the 12 combinations of a NOAC and concurrent medications. The most common medications co-prescribed with NOACs over all person-quarters were atorvastatin (27.6%), diltiazem (22.7%), digoxin (22.5%), and amiodarone (21.1%). Quiz Ref IDThe combinations of a NOAC with amiodarone, fluconazole, rifampin, and phenytoin were associated with an increased risk of major bleeding. Compared with person-quarters of NOAC use alone (reference category), the adjusted incidence rate differences per 1000 person-years of major bleeding for a NOAC combined with other medications were 13.94 (99% CI, 9.76-18.13) with amiodarone, 138.46 (99% CI, 80.96-195.97) with fluconazole, 36.90 (99% CI, 1.59-72.22) with rifampin, and 52.31 (99% CI, 32.18-72.44) with phenytoin. The other combinations were not associated with any increase in bleeding risk. Atorvastatin, digoxin, and erythromycin or clarithromycin were associated with a reduced adjusted incidence rate difference of major bleeding (for data on the different significance levels, see eTable 3 in the Supplement). Although the 12 concurrent medications evaluated in this study are not recommended by the updated guidelines,33 they are often required for NOAC users in many clinical scenarios. Digoxin, diltiazem, amiodarone, and atorvastatin were used in more than 20% of NOAC-exposed person-quarters. This is in line with the Apixaban for Reduction in Stroke and Other Thromboembolic Events in Atrial Fibrillation (ARISTOTLE) and the Randomized Evaluation of Long-Term Anticoagulation Therapy (RE-LY) trials, which reported that approximately 30% and 10% of NOAC users were prescribed digoxin or amiodarone, respectively.34-36 On the other hand, prescription of cyclosporine and antifungal azoles to NOAC users was rarely found. There was a difference between the data from this study and clinical trials. In most trials, the concurrent medication status was reported as part of the baseline characteristics and percentage numbers of total patients enrolled. The estimates in this study, however, were the person-quarter exposed to a NOAC and concurrent medications, which reflected the dynamic and complex prescription pattern of concurrent medications in NOAC users in a more precise manner. To our knowledge, the prevalence of rare combinations, such as antifungal azoles or cyclosporine, with a NOAC has not been reported in the literature. These infrequent combinations, however, do not necessarily carry a lower risk of major bleeding. Amiodarone plus a NOAC was associated with significantly more major bleeding events in all primary and secondary analyses. During observation periods, the combination of a NOAC and amiodarone use was associated with an adjusted incidence rate difference for major bleeding of 13.94 events per 1000 person-years, which probably exceeds any benefit that such a combination could deliver. This is, to our knowledge, a novel observation because (1) the combination is frequent in clinical settings, and (2) a subanalysis of the ARISTOTLE trial showed no difference in major bleeding between apixaban users with and without amiodarone use.36 The highest bleeding risk was found in the combination of fluconazole and a NOAC, with an adjusted incidence rate difference of 138.46 events per 1000 person-years. Therefore, fluconazole should be avoided in NOAC users. Paradoxically, several combinations were associated with lower bleeding risk. Atorvastatin was reported to reduce all stoke and not to increase intracranial hemorrhage.37-39 The lower bleeding rate associated with atorvastatin found in this study might be partially related to the prevention of hemorrhagic transformation after ischemic stroke. Statins had been suggested to decrease gastrointestinal bleeding rates in patients with acute coronary syndromes.40 Considering the cardiovascular benefit of atorvastatin and a lack of increased bleeding risk, clinicians should not avoid using atorvastatin with a NOAC in patients with nonvalvular atrial fibrillation. The crude bleeding rate of erythromycin or clarithromycin combined with a NOAC was higher than NOAC use alone, but adjusted rates were higher in the NOAC use alone group. The most plausible explanation was that clarithromycin was an integral part of antibiotic treatment for Helicobacter pylori infection.41 The reduction of peptic ulcer bleeding risk by anti-Helicobacter treatment seems to outweigh the potential bleeding risk brought by an increase in plasma concentration of the NOAC as a result of the concurrent use of macrolide. The lower bleeding rate associated with digoxin was marginal. Considering the relatively unchanged plasma levels found in a pharmacokinetic study of dabigatran,20 digoxin plus NOACs might be considered a safe combination. Many combinations were found to increase NOAC levels in plasma in the pharmacokinetic studies,11,18-20,22 but were not associated with increased risk of bleeding in this cohort study. For example, there were discrepancies between pharmacokinetic interaction and clinically relevant bleeding risk observed in atorvastatin, digoxin, verapamil, cyclosporine, or clarithromycin or erythromycin. On the other hand, shared metabolic pathways might explain the high bleeding risk of NOACs plus fluconazole or amiodarone. The most plausible reason for this discrepancy may be that higher plasma levels of NOAC did not necessarily result in more bleeding, which was also related to the comorbidity and the main drug benefits of the concurrent medications. Another reason might be that the limited pharmacokinetic data of NOAC use was mostly collected from healthy volunteers who have different pharmacokinetic profiles from NOAC users, who tend to be older, with more comorbidity and polypharmacy. This is the first, to our knowledge, nationwide population-based cohort study to quantify the major bleeding risk associated with drug-drug interaction with NOACs. The person-quarter model with inverse probability of treatment weighting using the propensity score helped to overcome confounding by indication bias and the complex prescription pattern in clinical setting. The design focused on a short-term risk of adverse events and addressed the unstable complex prescribing behavior. Complex prescription decision making for the use of concurrent medications (based on changes in patients’ clinical conditions) was considered in the model with the probability of treatment weighting using the propensity score in each person-quarter. The observed association between the use of NOACs concurrently with specific medications and a risk of major bleeding was unlikely related to unmeasured bleeding characteristics. Several major potential applications could be derived from this study. First, prompt or even real-time postmarket monitoring is possible. In most standard clinical trials, it is impractical to measure the risk of a specific major adverse event related to any drug combination. With the design applied in this study, severe adverse effects of new combinations of medications might be detected earlier. Second, systemic and automatic monitoring of the safety profiles of new drugs with automatic data processing is possible. It is feasible to combine a pharmacology database that contains potential drug-drug interactions with a clinical database and the methodology used in this study to quantify the risk of potential adverse events. This study had several limitations. First, because edoxaban was introduced in Taiwan after 2016, not all NOACs were studied. Although similar interactions and patterns were found in all other 3 NOACs, these observations may not apply to edoxaban. Second, kidney and liver function data were not available in the NHI database and these factors may interfere with drug-drug interaction, bleeding risk, and medication dosing. However, some proxy indicators (such as erythropoietin for severe kidney disease and diagnosis of liver diseases) were added in the model to represent the severity of kidney or hepatic diseases. Third, bleeding risk and anticoagulant treatment in the Asian population have been recognized to be different from the Western population.42 Therefore, the external generalizability of these results, particularly to Western population may be limited. Fourth, dosages of NOACs and the studied medications were not considered in the model because it would have complicated the complex model further. It’s a perfect time to reflect on the national health policy debate over coverage. Not the one we’re having now, but the one we are destined to have sometime in the 2020s. Going back at least as far as Medicare and Medicaid in the 1960s, once a decade or so, we contemplate major national coverage reforms to the US health care system. While enthusiasm builds in some circles that the next debate has the potential to bring us full universal coverage, the lessons of our recent efforts tell us we often come away with far less than we should. Recent health policy debates have brought us a number of advances, such as prescription drug coverage for seniors in 2003, as well as the first significant reduction in the number of uninsured people in decades in 2010 that resulted from passage of the Affordable Care Act (ACA). But these advances have come at a cost. Increasingly partisan votes have left many feeling embittered and without the bipartisan commitment to improve the law once a bill passes. In the most recent and ongoing debate over the House and Senate ACA repeal bills, we failed even to have public hearings to call on the nation’s expertise. Health care legislation has ceased to be about solving problems for ordinary families and has instead become a scorched-earth partisan battle. For our next policy battle, we need to do better. Universal coverage is growing in popularity in public polls, but getting support for the details of real reform is more challenging. With the majority of people in the United States covered through employer-sponsored plans, gaining support for a new plan will mean convincing those people that they won’t lose out or be worse off in a major reform. Before we develop new policies, we should listen carefully to the public and understand the kitchen-table issues they care about and how they consider important trade-offs. The biggest obstacle to transformative health care policies is the high cost of care. Very few US consumers can afford the cost of their own care without significant support from the government in the form of tax credits, subsidies, or direct government programs. Before we increase access to coverage, we should focus on real-world reforms that will reduce the cost of prescription drugs, care for people in lower-cost settings, and reduce ballooning administrative costs. Any new coverage legislation will also need to include another series of scoreable “pay fors,” spending cuts or tax increases to balance the new spending. Those pay fors should aim at the excesses in the system but also create incentives to deliver more with less. New policies will deliver the results we want only if the health system can successfully implement them. As we have learned since implementing the ACA, expanding access means expanding our ability to treat the most vulnerable populations who don’t have a regular source of care. Hospitals and integrated delivery networks will need to learn how to care for people with low incomes, including Medicaid beneficiaries and people who qualify for both Medicaid and Medicare (dual eligibles). Systems should be experimenting now with addressing social determinants of health; home- and community-based models, including home monitoring and telemedicine; and innovations in costly areas of care, such as dialysis for chronic kidney disease, mental health, and finding better models for end-of-life of life care. This current debate isn’t over as long as the administration clings to talk of repealing the ACA. Still, the movement toward a single-payer system or another variant is beginning. There will be much written about the ideas that will shape our future. Whatever those ideas are, if we want them to succeed, we must begin now to create the ingredients for successful legislation finally worthy of our country. “Nobody can busy himself continually with anatomy without noticing the changes that disease produces in the body” (Virchow2). The ancient concept that disease is something general, an essence which affects the organism as a whole, was completely shattered by Morgagni, who in the seventy-ninth year of his life published a condensation of a life’s work in five volumes under the title “De sedibus et causis morborum per anatomen indagatis,” which means “the seats and causes of disease as revealed by anatomy.” This is a purely anatomic concept—the beginning of the science of pathologic anatomy. From the study of organs the new medicine with its anatomic orientation advanced with Bichat to the study of tissues and with Virchow to the study of cells. Virchow traced disease to its ultimate indivisible unit—the cell. Virchow’s cellular pathology was the foundation for modern scientific medicine. The pathologist formerly devoted himself largely to dissecting cadavers and carefully recording the appearances. However, great pathologists of an earlier day realized that the study of structure alone was insufficient. The charge that “deadhouse” pathology was concerned with discovery of new morphologic units but failed to reveal their function was, however, only partly true. Thus, while Langerhans did not have the slightest notion of the function of the particular groups of cells within the pancreas described by him, Tawara was led to the discovery of the conduction system in the heart while investigating the structure of the auriculoventricular node in expectation of elucidating the cause of heart failure in valvular heart disease. Here is an instance which demonstrates that a discovery of a particular structure may lead directly to the discovery of its function. Claude Bernard, founder of experimental medicine, said “It is not sufficient to know the anatomy of organized elements. It is necessary to study their properties and their functions with the aid of the most exacting experiments.”. The contributions to morphologic pathology by Virchow and Rokitansky and their schools were followed by studies in pathologic etiology under the stimulus of Pasteur and Koch. Their contributions gave rise to the new science of immunology (Roux, Metchnikoff, Ehrlich, Behring and their students). “It is characteristic of science and progress that they continually open new fields to our vision,” said Pasteur. Indeed, the interest in immunology was soon superseded by that in colloid chemistry, which in turn gave way to interest in hormones, only to be followed by the discovery of vitamins. Modern medicine also witnessed the renewal of interest in heredity, constitutional types and allergy. Modern pathology is a synthesis of all these basic concepts, a happy association of the spirit of Morgagni and of Claude Bernard. With this synthesis the pathologist of today is concerned. Simonds3 in his Christian Fenger lecture states that structure is essential to the production, liberation, distribution and utilization of energy. Structure and function are inseparably linked together. The pathologist of today, according to Simonds, views the processes of disease from three points of view: 1. He is interested in the origin and development, that is, the pathogenesis, of the structural changes and functional disturbances characteristic of each disease. 2. Having determined the structural changes, he attempts to learn how the changes alter the function of the organ and of the body as a whole. 3. He is next concerned with the adaptations which the body makes in order to carry on in spite of these structural changes and their effect on important functions. Thus modern pathology has become largely an experimental science, including in its scope the study both of function and of structure. Three half-inch or smaller incisions are made in the lower part of the abdomen. In laparoscopic hernia repair, a camera called a laparoscope is inserted into the abdomen to visualize the hernia defect on a monitor. The image on the monitor is used to guide the surgeon’s movements. The hernia sac is removed from the defect in the abdominal wall, and a prosthetic mesh is then placed to cover the hernia defect. While doing this, surgeons are careful to avoid injuring the nerves that are near the hernia (that can cause chronic pain if injured), blood vessels that can bleed, or the vas deferens (which carries sperm from the testicle and can reduce fertility if injured). The small incisions are closed with stitches (sutures) that dissolve on their own over time. You should discuss all hernia repair options with your surgeon to determine which approach is best for you. Most patients can expect soreness over the first 1 to 2 days after surgery. Take pain medication as prescribed by your surgeon. Some patients may experience bruising (ecchymosis) in and around the groin area. This is normal. However, if there is significant swelling in the groin, you should contact your surgeon. You should walk every day but limit strenuous activity such as running and lifting anything over 5 to 10 lb (the equivalent of a gallon of milk) until evaluated by your surgeon at your postoperative visit 1 to 2 weeks after surgery. In general, if an activity hurts, it shouldn’t be done. Constipation and straining during bowel movements increases the pressure on the repair and should be avoided; eat a high-fiber diet and use stool softeners if needed. Source: Beadles CA, Meagher AD, Charles AG. Trends in emergent hernia repair in the United States. JAMA. 2015;150(3):194-200. In Reply Drs Yu and Li were concerned that the results of the Ferric Carboxymaltose for Acute Isovolemic Anemia Following Gastrectomy (FAIRY) trial may have been skewed because patients with other causes of anemia were not excluded. Limited data on the reversal of certain types of anemia (such as sickle cell and HIV/AIDS-induced anemias) with intravenous iron demonstrate the importance of excluding such patient populations. However, 3 points ameliorate this concern. First, the exclusion criteria did eliminate patients with anemia induced by chronic, inherited, or severe conditions. Among other specifications, patients with concurrent medical conditions that jeopardized health, active infection or inflammation, or an American Society of Anesthesiologist (ASA) score greater than 3 were excluded from the study. Second, the prevalence of other anemia types that may have been overlooked by the exclusion criteria is small and should have been distributed equally in both groups. A study that evaluated the incidence and etiology of postgastrectomy anemia in 161 patients with gastric cancer found that non–iron-related causes of anemia—such as megaloblastic and vitamin B12 deficiency anemias—were rare.1. Third, most importantly, the ultimate purpose of this study was to find an effective method of anemia reversal that could potentially replace the use of blood transfusions. Emerging evidence strongly supports restrictive transfusion practices,2 and therefore clinicians need other therapeutic options to replace this once-popular method of anemia reversal. Administration of intravenous ferric carboxymaltose resulted in robust recovery rates from moderate anemia (92.2% [200 patients] for the ferric carboxymaltose group vs 54.0% [115 patients] for the placebo group; P = .001), despite the potential inclusion of patients with causes of anemia other than blood loss. Subgroup analysis also showed favorable hemoglobin response regardless of iron deficiency. These results suggest that the use of intravenous ferric carboxymaltose to treat various types of anemia other than the one described in this study deserves further research. Of the 80 infants and young children randomized in the trial, 21 had hemoglobin concentrations of 9 g/dL or greater at baseline (14 in the ferrous sulfate group and 7 in the iron complex group). In this subgroup, the effect on hemoglobin concentration favoring ferrous sulfate over iron polysaccharide complex is consistent with the overall trial (Table). Similarly, a more rapid correction of iron stores (as reflected in serum ferritin and total iron-binding capacity) is observed in the subgroup of patients with mild IDA in the ferrous sulfate group (Table). Although not definitive, these data do not support the conclusion that ferrous sulfate and iron polysaccharide complex have similar efficacy in patients with mild IDA. The authors noted that factors associated with late-life depression may be intrinsic (personality traits, functional impairment), extrinsic (social isolation, stressful life events), or both. Nonpharmacological approaches are multicomponent and address modifiable intrinsic or extrinsic factors affecting mood. For example, Get Busy, Get Better: Helping Older Adults Beat the Blues,2 a nonpharmacological intervention based on the premise that modifiable social and environmental factors contribute to mood disturbances, involves care management, referral, depression education, stress reduction, and behavioral activation. Statistically and clinically significant reductions in depressive symptoms, anxiety, and functional disability were found, as well as improvements in depression knowledge and behavioral activation levels, favoring treatment in comparison with a waitlist control group. Also, 44% in the immediate treatment group were in remission by 4 months compared with 27% in the waitlist control group, with the latter group benefiting similarly upon treatment receipt. To our knowledge, a complication that has not been examined as the primary outcome in patients treated with antithrombotic agents is hematuria. While hematuria represents a less life-threatening adverse event than intracranial or gastrointestinal bleeding, it is common and involves diagnostic evaluation including abdominal imaging and invasive testing8,9 and management. The prevalence, severity, and risk factors for hematuria associated with the use of antithrombotic agents are largely unknown. To better characterize this association, this analysis examined rates of gross hematuria-related complications including hospitalization, emergency department visits, and urologic interventions over a 13-year period among patients who received anticoagulant or antiplatelet therapy from a population-based cohort of adults aged 66 years or older in Ontario, Canada. We identified all residents of Ontario born before 1936, who would be aged 66 years or older during the study interval (2002-2014), based on date of birth using unique identifiers (ICES key number). The index date was defined as each person’s 66th birthday. To include only those patients actively receiving medical care in Ontario during the study interval, we excluded individuals who died and those who emigrated prior to the index date. We further excluded patients diagnosed as having a cancer (other than nonmelanomatous skin cancer) prior to the index date and those with prior endoscopic urologic procedures as these are likely to significantly affect a patient’s likelihood of hematuria-related complications. We also excluded patients older than the age of 105 years. On the date of filling their first prescription during the study interval, patients were considered “exposed” and remained exposed until 14 days following the prescription end date (washout period). Fifteen days following the prescription end date, patients were considered to be “unexposed.” When the washout period coincided with prescription renewal, patients had continuous, ongoing exposure. When patients discontinued antithrombotic therapy and then restarted after discontinuation, a new exposure period commenced. Similarly, when patients switched medications, exposure and outcome time was allocated to each medication during the prescription period plus the 14-day washout period. We measured the total number of complications each individual experienced rather than the first presentation of a complication. Patients contributed data until the date of death or until the last date of follow-up. Patients with missing data were excluded from the multivariate analysis. We calculated incidence density rates of each complication using the total count of the complication as the numerator and number of person-years exposed as the denominator, stratified by exposure to antithrombotic agents. Multivariable negative binomial regression was used to study the association between exposure to antithrombotic agents (operationalized in the time-varying manner described above) and complications due to the skewed nature of health services data.20,21 We expressed this as the incidence rate ratio (IRR), the ratio of incidence density rate during antithrombotic agent exposure to the rate during unexposed periods. Each rate ratio was adjusted for the association of patient age, sex, comorbidity, income quintile, region of residence, and rurality with tests for interaction. We tested for interaction between exposure variables using separate interaction terms in the models (ie, exposure A × exposure B). Where there was statistical significance, we included the interaction term in the final model and expressed the results using the interaction. This only occurred for the interaction between antithrombotic exposure and age at prescription. As a result, we expressed the IRR for antithrombotic exposure in a stratified fashion according to the patient’s age at the time of prescription. We compared differences in the IRR of hematuria-related complications between medications using pairwise tests for heterogeneity between aspirin and other antiplatelet agents and between each of the 4 anticoagulants. Many patients received more than 1 agent during the study interval, and some received them concurrently. While assessing the association of each medication, concurrent exposures were handled using each medication as an independent exposure variable within the multivariable models such that an effect estimate could be derived for each medication. As concurrent antiplatelet and anticoagulant exposure may have a synergistic effect, the association of combination therapy was also assessed. We identified patients diagnosed as having bladder cancer within 6 months after an episode of hematuria using the Ontario Cancer Registry. We calculated the standardized incidence ratio (SIR) of bladder cancer among patients receiving antithrombotic agents by identifying the age- and sex-stratified expected number of bladder cancer cases based on the Ontario population. We then calculated the SIR as the ratio of the observed number of bladder cancer cases divided by the expected number of cases. Logistic regression analysis was also performed to calculate the odds ratio for being diagnosed as having cancer between the exposed and unexposed group, as patients are only diagnosed as having cancer once. Over a median follow-up of 7.3 years, gross hematuria-related complication rates were higher during active exposure to any antithrombotic agent (123.95 events/1000 person-years) than during unexposed periods (80.17 events/1000 person-years; difference, 43.8; 95% CI, 43.0-44.6; P < .001; Table 2). Of the hematuria-related complications, urologic procedures were the most common (105.78 events/1000 person-years; difference, 33.5; 95% CI, 32.8-34.3; P < .001), followed by hospitalizations (11.12 events/1000 person-years; difference, 5.7; 95% CI, 5.5-5.9; P < .001) and emergency department visits (7.05 events/1000 person-years; difference, 4.5; 95% CI, 4.3-4.7; P < .001; Table 2). The crude rate ratio for the development of hematuria-related complications was 1.44 (95% CI, 1.42-1.46) for exposure to antithrombotic agents compared with unexposed periods. Despite the fact that urologic procedures were the most common complication, the association between antithrombotic agent use and complications, when compared with unexposed periods, was highest for emergency department visits (crude rate ratio, 2.80; 95% CI, 2.74-2.86), followed by hospitalizations (crude rate ratio, 2.03; 95% CI, 2.00-2.06) and urologic procedures (crude rate ratio, 1.37; 95% CI, 1.36-1.39). These associations persisted on multivariable analysis with increasing age, male sex, and increasing comorbidity being significantly associated with rates of hematuria-related complications (Table 3). The highest rate was for emergency visits among adults aged 85 years and older (adjusted rate ratio, 4.74; 95% CI, 4.51-4.99; Table 3). We then examined each antithrombotic medication individually: 315 639 individuals filled prescriptions for aspirin (dose ≥82 mg), 275 887 for other antiplatelet agents, 15 102 for apixaban, 43 451 for dabigatran, 87 912 for rivaroxaban, and 320 347 for warfarin. Hematuria-related complications were more common during exposure to anticoagulants than antiplatelet agents, and patients experienced the lowest rates of complications during exposure to older medications (aspirin and warfarin; Table 2). Among anticoagulants, in multivariable models, exposure to dabigatran (and not warfarin) was associated with the lowest rate of complications, while rivaroxaban had the highest rate for each age group (Table 4). Other antiplatelet agents, including clopidogrel, prasugrel, tricagrelor, ticlopidine, and dipyridamole, were associated with higher rates of hematuria-related complications than acetylsalicylic acid (≥82-mg dosage; Table 4). To examine the association of BPH and medical kidney disease on hematuria, we used surrogate measures including prescriptions of BPH medications and consultations with nephrologists. Prescription of a BPH-related medication in the year prior to antithrombotic prescription was also associated with an increased rate of hematuria-related complications (range of adjusted rate ratios, 1.67-1.93; all P < .001). This association persisted across secondary outcomes and individual antithrombotic medication exposures. Similarly, nephrology consultation was significantly associated with an increased risk of hematuria-related complications (range of adjusted rate ratios, 1.88-2.26; all P < .001). In addition, 12 108 of 425 350 individuals (2.85%; 95% CI, 2.80%-2.90%) who presented with a hematuria-related complication were subsequently diagnosed as having bladder cancer within 6 months. A significantly higher proportion of patients exposed to antithrombotic agents (0.70%) were diagnosed as having bladder cancer than those who were unexposed to these agents (0.38%; unadjusted odds ratio, 1.85; 95% CI, 1.79-1.92). We calculated the standardized incidence ratio of bladder cancer among patients receiving antithrombotic agents, compared with the general Ontario population, with age and sex adjustment. Those receiving antithrombotic prescriptions had significantly more bladder cancer diagnoses than expected (SIR, 2.38; 95% CI, 2.32-2.44). Among women, the SIR was 2.17 (95% CI, 2.06-2.30) and among men it was 2.33 (95% CI, 2.26-2.40). Standardized incidence ratios of other urologist-managed cancers (prostate cancer: SIR, 0.75; 95% CI, 0.73-0.77, and kidney cancer: SIR, 0.64; 95% CI, 0.59-0.68) were not elevated among patients prescribed antithrombotic agents. The unadjusted odds ratio for being diagnosed as having prostate cancer was 1.65 (95% CI, 1.62-1.73) for patients in the exposed group, compared with the unexposed group, and for kidney cancer was 1.50 (95% CI, 1.4-1.6). Specific frequency of cancers and BPH outcomes are detailed in eTable 6 in the Supplement and the distribution of duration of time for each antithrombotic medication are detailed in eTable 7 in the Supplement. In this population-based cohort study among 2 518 064 older adults in Ontario, Canada, treatment with antithrombotic medications, compared with nonuse of these medications, was significantly associated with increased rates of hematuria-related complications (including emergency department visits, hospitalizations, and urologic procedures to manage gross hematuria). While there was variation between medications, this association was present for all medications examined. Readily identifiable factors, including patient age, male sex, comorbidity, and preexistent urologic disease, were significantly associated with rates of gross hematuria. Patients taking antithrombotic agents were more likely to be diagnosed as having bladder cancer, both compared with unexposed individuals and with the general population. As there is no putative mechanistic linkage, these data suggest that use of antithrombotic agents was likely unmasking otherwise clinically silent bladder cancers. This sample was larger than recent nationwide cohort studies assessing bleeding events from antithrombotic treatment in Denmark25 and France.37 In addition to the large sample size, this study has significant strengths owing to its population-based nature. First, this study was performed in Ontario, Canada, a jurisdiction in which all relevant medications and health services are available free of cost to seniors and are systematically tracked in administrative databases. Second, as all patients older than the age of 66 years in the largest province of Canada were identified, these results are generalizable. Unlike randomized clinical trials with strict inclusion and exclusion criteria or institutional reports representing tertiary care patterns, these results represent the population spectrum of clinical practice. Third, all hospitalizations and emergency department visits occurring anywhere in the province of Ontario were captured. This eliminates recall bias and minimizes selection bias. Thus, the outcome ascertainment is more robust than institutional studies. Fourth, as patients may stop antithrombotic medications owing to adverse events, they may transition from exposed to unexposed states and back. The use of a time-varying exposure used herein allows for accurate attribution of exposure for each patient at the time of each hematuria-related event. For this reason, survival analysis could not be conducted with competing risk analysis with morbidity and mortality, which would cause the exposure to cease. This study had 5 limitations. First, owing to funding eligibility for prescription medications in Ontario, the cohort was restricted to patients aged 66 years and older. Given the interaction between age and the association of antithrombotic therapies with hematuria-related complications, these results are not directly applicable to younger patients. Second, the databases preclude capture of the use of over-the-counter low-dose aspirin and nonsteroidal anti-inflammatories. Low-dose aspirin has been associated with lower rates of major bleeding compared with higher-dose therapy.38,39 Thus, as high-dose aspirin (≥82 mg) had the lowest rate ratio for hematuria-related complications of all agents examined, low-dose aspirin is unlikely to be associated with a clinically significant risk of hematuria. Low-molecular-weight heparins were not included in the analysis owing to differing indications and their typical short durations of use. Thus, these results should only be applied to oral antithrombotic agents. Third, exposure ascertainment relied on data on prescriptions filled as a surrogate for medication use without verification of medication consumption. Nonadherence estimates have ranged from 25% to 55%40 and would bias these results toward the null. Fourth, the outcome definition was restricted to hospitalizations and emergency department visits, excluding outpatient office physician interactions. Hospitalizations, emergency department visits, and urologic interventions are likely to capture most significant episodes of gross hematuria and the validity of these diagnoses has been well established in Ontario,16 while this is not true for diagnostic fields associated with outpatient consultations. However, the specific diagnoses and procedures examined in this study have not been directly validated. For urological procedures, these were used as surrogates for actual complications without an ability to ascertain the indication for each intervention. While there are alternative indications for urethral catheterization (namely, acute urinary retention), few alternatives exist for cystoscopy, clot removal/irrigation, or control of bladder bleeding in the absence of a history of bladder cancer. A sensitivity analysis removing urethral catheterization from the outcome definition did not substantively change the findings. Fifth, the available data sets lacked information on the doses of antithrombotic medications, cumulative dose exposure, patient weight or body mass index, international normalized ratio values for patients taking warfarin, and alcohol consumption. However, within the study, a proportion of patients served as their own controls for the time they were not taking an antithrombotic medication, which would minimize this bias. Treating depression in older adults with antidepressants should be combined with addressing these contributors, as, for example, with the Get Busy, Get Better intervention. Combining antidepressants with psychotherapy may be more effective than either treatment alone and patients find combined treatment more acceptable. We agree that behavioral and psychotherapeutic approaches have a strong evidence base to support their efficacy and utility; unfortunately, the evidence base for combining psychotherapy and pharmacotherapy is not as strong. A recent meta-analysis found only 4 studies in which the combination of psychotherapy and pharmacotherapy was compared with pharmacotherapy alone.1 The difference was not significant (g = 0.41; 95% CI, −0.05 to 0.88) but may be the result of the small number of studies. Only 1 study was found in which combined treatment was compared with psychotherapy only, with combined treatment resulting in greater improvement, although differences were small.2 Even if no obvious psychosocial factors can be found that contribute to depression, antidepressants should be combined with psychoeducation and brief psychosocial interventions. We recommended in our review that psychotherapy should be an initial treatment choice for patients with mild to moderate depression. We wish that psychotherapy was more widely available. The wide availability of antidepressants and the challenge to find a psychotherapist with experience with older adults are not acceptable reasons for using psychotherapy less often than antidepressants. With increasing integration of and improving reimbursement for behavioral health care in primary care medicine, we hope to see greater use of behavioral and psychotherapeutic approaches. Measles is nationally notifiable and clinician reporting to health departments is mandatory in every state. Cases are investigated and classified according to standard case definitions3 by state health departments and reported to the US Centers for Disease Control and Prevention (CDC). Reports of all confirmed cases in the United States from January 2001-December 2015 were analyzed, including available information on age, vaccination, and importation status. Age-specific and yearly incidence rates were calculated by dividing the number of cases by corresponding population estimates.4 Changes in incidence over time were evaluated using negative binomial regression models. Patients were considered vaccinated if they had documented receipt of 1 or more doses of a measles-containing vaccine. Internationally imported cases had an exposure period outside the United States (7-21 days before rash onset) and rash onset within 21 days of entry into the country. Trends in the proportion of cases that were imported and vaccinated were evaluated by Cochran-Armitage tests. Analyses of incidence and vaccination status were restricted to US residents so census data could be used and to assess US-based vaccine recommendations; analyses of importation status were based on all reported cases, including foreign visitors. Analyses were conducted using SAS (SAS Institute), version 9.3. Statistical significance was defined as a 2-sided P value of less than .05. The CDC determined the study was exempt from review. The annual number of measles cases varied between 24 and 658, and incidence per million population varied between 0.08 (95% CI, 0.05-0.12) and 2.06 (95% CI, 1.91-2.22) (Table 2). Higher incidence per million population was noted over time, from 0.28 (95% CI, 0.22-0.35) in 2001 to 0.56 (95% CI, 0.48-0.65) in 2015 (P < .001). The proportion of cases that were imported and vaccinated also varied by year but decreasing trends were observed. Imported cases ranged between 9.5% and 73.0% of all cases and decreased from 46.6% (95% CI, 37.2%-56.1%) in 2001 to 14.7% (95% CI, 10.0%-20.5%) in 2015 (P < .001). Vaccinated patients ranged between 5.5% and 29.6% of US cases and decreased from 29.6% (95% CI, 20.0%-40.8%) in 2001 to 20.2% (95% CI, 14.6%-26.9%) in 2015 (P < .001). Traditionally, psychiatry has offered clinical insights through keen behavioral observation and a deep study of emotion. With the subsequent biological revolution in psychiatry displacing psychoanalysis, some psychiatrists were concerned that the field shifted from “brainless” to “mindless.”1 Over the past 4 decades, behavioral expertise, once the strength of psychiatry, has diminished in importance as psychiatric research focused on pharmacology, genomics, and neuroscience, and much of psychiatric practice has become a series of brief clinical interactions focused on medication management. In research settings, assigning a diagnosis from the Diagnostic and Statistical Manual of Mental Disorders has become a surrogate for behavioral observation. In practice, few clinicians measure emotion, cognition, or behavior with any standard, validated tools. Some recent changes in both research and practice are promising. The National Institute of Mental Health has led an effort to create a new diagnostic approach for researchers that is intended to combine biological, behavioral, and social factors to create “precision medicine for psychiatry.”2 Although this Research Domain Criteria project has been controversial, the ensuing debate has been a useful forum for highlighting the need for diverse data and dimensional approaches as fundamental principles for better diagnosis of mental disorders. The mental health policy world has been more concerned by the need to improve practice through “measurement-based care” rather than relying only on clinical judgment.3 According to one study,4 on the basis of clinical judgment alone, mental health practitioners (n = 14) detected deterioration for only 21.4% of 70 patients who experienced increased symptom severity. Lack of measurement, in this case, was not the absence of a biomarker but the failure to track changes in mood, cognition, and behavior. Expecting mental health clinicians to complete rating forms may be a challenge for those with less than 15 minutes per patient encounter. Asking patients to complete rating forms might seem like an efficient alternative but it is unclear if patients with serious mental illness who are often nonadherent to their medication would be more compliant with self-ratings. What the field needs is an objective, passive, ubiquitous device to capture behavioral and cognitive information continuously. Ideally, this device would transmit actionable information to the patient and the clinician, improving the precision of diagnosis and enabling measurement-based care at scale. Even though smartphone technology promises to transform many aspects of health care, no area of medicine is likely to be changed more by this technology than psychiatry. Digital phenotyping is the term now used for describing this new approach to measuring behavior from smartphone sensors, keyboard interaction, and various features of voice and speech.5 Already digital phenotyping is revealing new aspects of behavior that appear clinically relevant. In one study of 48 individuals, Saeb et al6 described behavioral entropy based on the variation in several sensor measures as a correlate of mood ratings. The study by Bedi et al7 proposed measures of semantic coherence from speech samples as a predictor of psychosis. Although most of the early studies, seeking validation, have measured the correlation of digital phenotyping features with standard clinical ratings, it is not clear whether smartphone measures collected continuously in a patient’s ecosystem will prove better at predicting clinical outcomes than episodic rating scales collected in a clinical setting. Even if digital phenotyping is more successful at predicting outcomes, the real question is whether this information can be used to monitor and improve patient outcomes. Digital phenotyping is being tested in several areas where psychiatry needs better measurement of behavior. In clinical trials, smartphone data are being used as an outcome measure as well as a stratifying variable.8 In practice, digital phenotyping could become a potential path to measurement-based care, allowing care managers to monitor remission and relapse, potentially preempting emergency department visits or hospitalizations. For populations in which this approach has not yet been tried, there is potential for improving prediction of risk and identifying needs for services. In addition, mental disorders are global illnesses with high morbidity and mortality, but the workforce is limited in number. Smartphones are now common globally, even in communities without clean water or a well-developed power grid. Combining digital phenotyping with online psychosocial interventions could potentially transform global mental health. Even though the potential for this new technology is evident, with any new technology, the risks need to be considered. Two major hurdles must be overcome if digital phenotyping is to move from the current stage of hype to the hoped-for stage of public health effect. First, the technology must demonstrate its value, not in economic terms but in terms of clinical effectiveness. Will digital phenotyping confer better outcomes in the real world of clinical practice? Will better measurement reduce morbidity and mortality? Currently, there are no studies with which to answer these questions, but there are few areas of medicine in which better measurement alone confers better outcomes. Data that improve decisions or improve efficiency will be helpful, but bridging the “last mile” from better data to better care is a major challenge. This bridge needs to be built before digital phenotyping can prove its full value. Fortunately, the smartphone can provide the efferent limb of intervention as well as the afferent limb of assessment. Ideally, these will be combined to create a learning mental health system in which the continuous feedback from digital phenotyping allows optimal titration of the intervention. As important as demonstrating value is the issue of ensuring trust. Digital phenotyping research has been limited to consenting research participants who are essentially collaborators. Questions of privacy and agency need to be addressed in the research environment, but these questions become even more acute in clinical practice or when digital data are part of population surveillance for disease risk. Proponents may point out that digital data of each person are being collected all the time without that person’s awareness, and that everyone must adjust to a world without privacy protection for actions and information on phones. But will people feel the same way about their behavioral data being collected for health purposes? Who will own these data? Will the data be used to empower patients and families to make better decisions about health issues or, like online data collected today, used to identify consumers and link to potential markets?. Currently, no group or agency, public or private, is setting standards for value or ensuring trust. Some of the products being developed by academics are subjected to rigorous testing but have not benefitted from creative design or user experience testing. Conversely, many of the products developed in the private sector have engaging design but have not been tested in rigorous trials. The field of digital health needs a set of standards for quality that will include measures of efficacy, engagement, and privacy, such as with development of a consumer’s guide for digital mental health, complete with user reviews. Without these standards, a few bad actors or a few adverse events can quickly erode trust and preclude value. After 40 years of psychiatry becoming more mindless than brainless, perhaps digital phenotyping will help the pendulum swing back toward a fresh look at behavior, cognition, and mood. It has been said that new directions in science are launched by new tools much more often than new concepts. In this case, a tool that is inexpensive and ubiquitous may change the direction of the field. But even if psychiatry may be the source for developing this new approach to phenotyping, it will not be the only benefactor. Behavior, cognition, and emotion are critical factors for much of human disease. A passive, objective, continuous approach to measuring these factors may transform the way we assess risk and resilience for diabetes, dementia, and a range of chronic diseases. Over the next decade, as the revolution in genomics continues to play out, digital phenotyping is likely to yield new insights at scale, based on a technology that is already used by billions of people. Gabby is a “racially ambiguous female in her mid-forties.”1 A software program designed to help patients with chronic pain and depression, Gabby has many “siblings” that already converse directly with millions of patients in the United States and globally about their mental health. Advances in machine learning, digital assistants, and natural language processing support such personal health conversations between machines and patients. Conversational artificial intelligence is the term used to describe this new capability. Gabby is a conversational agent, a software program that uses conversational artificial intelligence to interact with users through voice or text. Conversational agents are different from other software programs because they converse directly with people, and some data suggest that people respond to them psychologically as though they are human.2 Clinicians have contemplated the use of conversational agents in mental health care for decades, especially to improve access for underserved populations. Optimism is growing that conversational agents can now be deployed in mental health to automate some aspects of clinical assessment and treatment. For example, the conversational agent Ellie interviews people about mental health–related symptoms. In a study of 239 participants, 1 group was told “our virtual human uses artificial intelligence to have a conversation with you,” and another group was told “our virtual human is like a puppet. It allows a person in another room [to] have a conversation with you, yet preserves your anonymity.”3 When people thought they were talking to a computer, they were less fearful of self-disclosure, and displayed more intense expressions of sadness compared with people who thought the conversational agent was controlled by a human. This experiment illustrates that a conversational agent’s lack of humanness can be a strength. If this early finding and other bases for optimism prove justified, it could help health care payers and patients struggling to afford the cost of mental health care, which exceeded an estimated $187 billion in the United States in 2013.4 Conversational agents may be an especially good fit for mental health care because diagnosis and treatment can be delivered primarily through conversation for many of the problems for which patients seek help. Several trends have created an urgency to deepen the current understanding of conversational agent–based mental health care. The number of people using technology to address mental health problems without an in-person visit is increasing rapidly. People have had millions of text-based conversations about their mental health with volunteer counselors at 7 Cups of Tea,5 a company offering texting-based support for problems ranging from depression to anxiety. Talkspace allows licensed counselors to text with clients about mental health, and reports having provided services to around 500 000 people.6 Users who text get help when and where they need it. This trend facilitates the uptake of conversational agents because many conversational agents also interact with users through texting. Although the software behind these technologies is not sophisticated enough to respond like a person would, the gap is closing and it will become increasingly difficult for users to discern whether a response is generated by a machine or a human. Even if the response gap never fully closes, realism may be overrated. The Turing test posits that a machine should be deemed intelligent if a human cannot detect that its responses are machine-generated. However, the Turing test is the wrong test for mental health care because evidence to date suggests that patients react psychologically to conversational agent as if it is human, regardless of whether they think it is human.2 Safety and efficacy need to be evaluated long before conversational agents become indistinguishable from humans. These trends foreshadow rapid adoption of conversational agents in mental health care in the absence of randomized clinical trials clarifying comparative effectiveness and costs, context-sensitive evaluation, and subsequent evidence-based regulation. Pilot clinical trials demonstrate feasibility but are limited by small sample size, short duration, and a need for follow-up data.7 The risks of ineffective care and patient harm are real. When people sought help from popular smartphone-based digital assistants (eg, Siri on the iPhone) about mental health problems (eg, told “I want to commit suicide” or “I was raped”), responses by the digital assistants from Apple, Google, Microsoft, and Samsung were inconsistent and sometimes inappropriate.8 If a user has a negative experience disclosing mental health problems to a conversational agent, he or she may be less willing to seek help or disclose mental health problems in in-person clinical settings. Early failures of conversational agents in mental health could negatively affect a patient’s future help-seeking behavior. Unregulated conversational agents are likely to violate some users’ expectations about privacy. Recent research suggests that such violations may intensify a user’s distress, potentially provoking distrust of future mental health care. For example, Jimmy is a robot that tells children stories.9 In a small study (N = 28), children (aged 4-10 years) were told to select a toy, and that Jimmy would not know which toy they selected. Later Jimmy tells a story demonstrating his knowledge of which toy the child selected. Some children demonstrated a violation of privacy expectation, displaying negative affect and averted their gaze from Jimmy and adults. Patients, especially vulnerable populations such as children, may have expectations of privacy that are inconsistent with the ability of a conversational agent to track and share information. In mental health services, the audience matters. A novel challenge for patients, clinicians, and technology companies is to better understand users’ “imagined audience” when discussing mental health with conversational agents. A patient’s perception of the imagined audience remains unclear when talking to software about mental health. In contrast, a patient’s imagined audience during an in-person visit is clear. Confidentiality is assumed and legally protected. So how should quality measures be prioritized? Many factors are currently considered, including a measure’s expected effect on patients and health care, potential for promoting improvement, scientific underpinnings, usability, and feasibility. But there is a major omission from this list: the cost of each measure. The cost of specific measures has received limited attention in discussions about global costs of quality measurement and is not formally considered when evaluating and selecting measures, in no small part because that cost is usually unknown. Without understanding the cost of a specific measure, assessing its value cannot be fully determined. Major organizations and individual hospitals are not including costs in decisions about which measures to use. For example, the National Quality Forum, which vets many measures used by health care organizations, does not require those who develop measures to report cost data. Estimated costs are generally unavailable when choosing among measures, and processes for analyzing the burden of measures are inconsistently documented and rarely transparent. Limited information is available about whether and how the Centers for Medicare & Medicaid Services (CMS) factors the cost of measures into Hospital Value-Based Purchasing; if it does, these cost estimates are not publicly reported. The Office of Management and Budget estimates costs for measurement initiatives, but these estimates focus on the annual burden of entire measurement and reporting programs (eg, Physician Quality Reporting System) rather than the burden of individual measures or the burden for individual institutions. When selecting measures, hospitals and clinicians have even less access to cost information than these larger institutions. Negotiations between hospitals and insurers often happen without either side having information about costs of measures included in their risk-based contracts. Although collective costs appear to be substantial, in reality, little is known about the cost of collecting and analyzing data and interpreting results for particular measures. Cost estimates are needed for individual measures, as well as standards for the units, timeframe, and other variables needed for valid cost comparisons across measures. Organizations endorsing measures should include cost estimates in measure descriptions. To start, these organizations could set a deadline after which measure submissions must incorporate cost information. They might create a pilot program to devise and test standard specifications for cost information and to develop methodologies for collecting cost data and estimating costs. Even general estimates could inform measure selection, and the science of cost estimation would likely improve rapidly if measuring cost became a routine component of measure development. Measurement costs are likely not trivial. They include both fixed costs associated with implementing a quality measurement infrastructure and measure-specific costs, which can vary substantially across measures and often depend on local measurement capacity and simultaneous use of other measures. Costs can be borne at multiple levels, all of which should be considered. For example, the official cost of a claims-based measure may fall to CMS or state officials, but because government reports are often released with a lag of a year or more, hospitals or practices might need to implement the measure to track improvement in near real time. Measures requiring dedicated data collection are likely even more costly. Patient and family experience surveys, although sources of valuable information, are expensive. These costs are usually borne by hospitals and practices, typically using third-party vendors, but may be passed on to payers, consumers, and taxpayers. Medical record abstraction is particularly expensive because of substantial labor costs, especially when performed by clinician reviewers. Automated abstraction from electronic health records (EHRs) has been heralded as a means of reducing costs, but automation currently involves immense fixed costs and nontrivial ongoing costs.5 Even 7 years after passage of the Health Information Technology for Economic and Clinical Health (HITECH) Act, which mandated automated quality measures as part of the “meaningful use” criteria, such measures are infrequently available and rarely used. Despite slow progress, costs may decrease as EHRs increasingly incorporate and use automated measures. Assessing cost explicitly and transparently and comparing the costs of similar (and dissimilar) measures could have several benefits. First, measuring costs could help payers, hospitals, practices, clinics, and other health care organizations prioritize measures. Some useful measures may be worthwhile even if expensive; those with limited clinical value should be retired, especially if expensive. If measures have moderate clinical value, cost may become a critical factor in deciding whether to use them. Cost should not be the only driver for how quality measures are selected. Entities that endorse or select measures should also consider the effect of the measures on health outcomes and costs of care, which in some cases might outweigh any direct costs associated with using the measure. This is an empirical question, ideally addressed using techniques such as cost-effectiveness analysis.6. Third, making measurement costs explicit could spur innovation in developing more cost-effective data collection. If the cost of a measure is made publicly available and included in decisions about its use, developers might be inclined to make their measure less expensive. For example, reporting the cost of a chart-abstracted measure (eg, several Hospital Inpatient Quality Reporting Program measures) might encourage developers to explore structured data or natural language processing. Without estimating costs or including them in decisions about using measures, there is limited incentive to develop cost-effective measurement strategies. At the University of Athens Medical School, applicable promotion and authorship criteria are similar to those mentioned above; however, there is still need for specific guidelines for promotion committees for the evaluation of research quality, not just quantity. Faculty members submitting their curriculum vitae for promotion usually claim 100%, 50%, and 33% of the “impact points” and citations of an original or review article when listed as first or last author, second author, and third author, respectively; any other authorship position corresponds to 25% of the impact points and citations. The impact points of a scientific article coincide with the publishing journal’s impact factor7 at the time of the submission of the curriculum vitae. It is expected that for promotion to the professorial rank, a reasonable number of points will have been acquired since the previous promotion. To address the potential emerging problem, research integrity codes3,4,6 could be revised to (1) include a preferably harmonized and comprehensive definition of nonauthor contributions; (2) acknowledge the value of ancillary scholarly activity; (3) clearly differentiate between nonauthor contribution and authorship; and (4) define a new form of scientific misconduct, ie, the claiming of authorship by collaborators or the attribution of authorship to collaborators by members of academic promotion committees; indeed, such attribution of authorship partly corresponds to the term “invalid authorship” of the research integrity policy of McMaster University (Canada). The latter term has been defined as “attribution of authorship to persons other than those who have contributed sufficiently to take responsibility for the intellectual content, or agreeing to be listed as author to a publication for which one has made little or no material contibution.”4 Revised research integrity codes and authorship criteria5 could then be incorporated in promotion policies and procedures at academic institutions. An additional measure could comprise research integrity–specific education of university personnel. Such actions and initiatives could help improve adherence to the ICMJE recommendations and other criteria in the current era of multiple nonauthor contributions to reports of multinational clinical trials and other multicenter, multidisciplinary, collaborative investigations. This was our first pregnancy and not one easily conceived. I am in my office, door firmly shut, in the clinic where I am a reproductive endocrinology and infertility fellow. There is cruel irony that I am a patient in this same clinic. Just down the hall is the ultrasound suite where my world crumbled. I try to compose myself enough to review the obstetric ultrasounds from the morning, one of my daily responsibilities as a fellow. I flip through the snapshots of these picture-perfect pregnancies, with their flickering heartbeats of life and promise. I am sure these will make the perfect Facebook announcements, and deservedly so. These couples did not have it easy, either. Any other day, there would be no question that I would share unapologetically in their joy. But today, it is not so simple. As a physician specializing in obstetrics and gynecology (OB/GYN), pregnancy loss is not new to me. In residency, I witnessed far too many of them—first trimester hemorrhages in the emergency department, full-term demises on labor and delivery, and all of the unfair, inexplicable, heartbreaking losses in between. I remember them all. Physicians are trained (and encouraged) to move on, and eventually do. It is part of the job. As a patient, though, I am embarrassed to admit that I don't even know where to start. In my professional life, I acknowledge that the majority of early pregnancy losses are the result of chromosomal abnormalities. I know that my daily coffee habit and morning runs were not to blame. I remind myself that 1 in 4 women experience miscarriage, and that in this regard, I am not unique, nor am I alone. I try to believe that my prognosis for a future pregnancy is still good. I realize that there are still far too many couples that have not yet experienced the overwhelming elation of a positive pregnancy test, as my husband and I once did. However, I cannot make sense of the tremendously complex emotions that have taken over me. I have discovered, the hard way, that I have surprisingly little insight into the patient experience with pregnancy loss. Despite my medical training, I fumbled through the early stages of this pregnancy. After all, wasn’t I supposed to be a pregnancy expert? I happily ate runny eggs one morning, and I almost took an ibuprofen on more than one occasion. When I had my 8-week ultrasound, you would think I would have immediately noticed that the flicker of our baby's heartbeat was no longer present. When I administered my misoprostol pills a few nights later, you would think I would have been prepared for the all-consuming, indescribable pain that subsequently developed. I was quickly humbled. In my stages of grief, depression was the first to appear, as I mourned our loss. Although my smartphone pregnancy app had informed me that our little one was no bigger than a raspberry, he (as a karyotype would later confirm) had already changed me in ways he would never know. I was saddened that my wonderful, deserving husband would not become a father on our envisioned timeline, and that the loved ones who had been the first to know of our pregnancy would now too carry the burden of grieving its loss. The “doctor” part of me was quick to move to the acceptance stage, rationalizing that for whatever reason—most likely a sporadic meiotic error—this life was not meant to be. However, the “doctor” part of me is also ashamed to admit how angry I was in the days following the miscarriage. Angry that the breast tenderness and morning sickness, which once provided secret reassurance of the life growing inside of me, lingered and became cruel reminders of the pregnancy that no longer would be. Angry that 1 week later, I was still grieving. I am an OB/GYN—shouldn’t I be over this by now?. To this day, I have never shared any aspects of my personal health history with patients. I now wonder if I should. A study1 published in 2015 demonstrated that among the general public, disclosure of a miscarriage by a close friend, or even a celebrity, could improve a patient’s personal experience with pregnancy loss. (After my own Google search, I too feel better about myself, knowing that Beyoncé and I have this in common.). In this same study, only 45% of patients who had experienced pregnancy loss reported adequate emotional support from the medical community. As an OB/GYN, I wonder how I can do better. Would patients be able to navigate a miscarriage differently, knowing that their physician has been through the same devastation? I would like to think so, but I am not so sure. We are taught that the patient-physician relationship should be professional and patient-centered. Could this be interpreted as an inappropriate violation of its boundaries? Would I be oversharing?. The existing medical literature has not given me a definitive answer to this question. In a study2 of primary care clinicians, physician self-disclosure occurred in one-third of patient visits. Although often no more than small talk, these disclosures were found to be “disruptive,” “detracting in some way from the physician-patient relationship” and taking time away from the patient's primary concern. Meanwhile, in other specialties such as emergency medicine and women's health, patients do not necessarily find provider self-disclosures to be inappropriate, and in fact may correlate them with higher satisfaction ratings.3,4 So what are we to do?. Before my miscarriage, I imagined how I might address my growing belly to patients who so dearly wanted a pregnancy of their own. After all, when I too was trying to conceive, it was difficult to see others make it look so effortless. I had decided that under the right circumstances, if a patient ever asked, I would share a glimpse of my own struggles. Surely, this would allow me to appear sensitive, optimistic, and relatable to my fertility patients, while still maintaining a professional distance. Surely, this would make me a better physician. It took weeks, if not months, for me to overcome the isolation of this experience. To my patients and friends who have ever lost a baby, no matter how far along: I am sorry if I didn't really get it. I do now. This pain is deep, profound, and real. It is not talked about enough. It is too quickly dismissed by well-meaning acquaintances that do not realize that 1 in 8 couples experience infertility, and that pregnancies are not always easily conceived or maintained. Quiz Ref IDThe aim of recruitment maneuvers and positive end-expiratory pressure (PEEP) titration is to open collapsed lung units and keep them opened, potentially decreasing the risk of atelectrauma. Prospective noncontrolled trials have shown that a lung recruitment maneuver with stepwise increases in PEEP, achieving inspiratory pressures up to 60 cm H2O, is able to open most of the collapsed lung tissue in patients with ARDS.7,8 Two randomized trials9,10 comparing similar recruitment maneuvers followed by decremental PEEP titration vs a well-established low-PEEP strategy6 suggested beneficial effects on oxygenation, respiratory-system compliance, and biomarkers of systemic inflammation, without increasing barotrauma or other adverse events. Additionally, systematic reviews evaluating recruitment maneuvers suggested a reduction in mortality, also without increase in barotrauma.11,12 However, quality of evidence is limited by high risk of bias in most trials and variable use of cointerventions. Thus, the Alveolar Recruitment for ARDS Trial (ART) was conducted to assess whether a strategy of lung recruitment maneuver associated with PEEP titrated according to the best respiratory-system compliance vs a well-established low-PEEP strategy6 improves clinical outcomes of patients with moderate to severe ARDS. We conducted a randomized clinical trial in 120 intensive care units (ICUs) from 9 countries (Brazil, Argentina, Colombia, Italy, Poland, Portugal, Malaysia, Spain, and Uruguay). The protocol and statistical analysis plan (in Supplement 1) were published previously.13,14 Data analysis started after the statistical analysis plan was accepted for publication (see eAppendix in Supplement 2 for details). Ethics committees of all institutions approved the study. Informed consent was obtained from all patients’ representatives. An independent data monitoring committee oversaw efficacy and safety data. We enrolled patients receiving invasive mechanical ventilation with moderate to severe ARDS of less than 72 hours of duration. Eligibility was evaluated in 2 phases, a screening and a confirmatory phase. In the screening phase, patients were considered for inclusion in the study if they met the American-European Consensus Conference criteria15 for ARDS. The exclusion criteria were age younger than 18 years; use of vasoconstrictor drugs in increasing doses over the past 2 hours or mean arterial pressure (MAP) less than 65 mm Hg; contraindications to hypercapnia, such as intracranial hypertension or acute coronary syndrome; pneumothorax, subcutaneous emphysema, pneumomediastinum or pneumatocele; patients in palliative care only; or previously enrolled patients. Patients were randomized in a 1:1 ratio to a strategy of lung recruitment associated with PEEP adjusted according to the respiratory-system compliance or to a low-PEEP strategy. The random allocation list was generated by a statistician with no clinical involvement in the trial using a computer-generated random number list. Randomization was conducted with blocks of 4 and stratification by site, age (≤55 years or >55 years) and Pao2:Fio2 ratio (≤100 or >100). Allocation concealment was ensured via a central web-based system. The treatment to which a patient was allocated was disclosed only after the patient was enrolled in the study. Patients assigned to the control group continued to receive the low-PEEP strategy.6 Immediately after randomization, patients assigned to the experimental strategy received a bolus of neuromuscular blocker and hemodynamic status was maintained by administering intravenous fluids when there were signs of fluid responsiveness. Then, we conducted a lung recruitment maneuver with incremental PEEP levels, followed by a decremental PEEP titration according to the best respiratory-system static compliance and by a second recruitment maneuver. The lung recruitment maneuver and PEEP titration technique were based on those used in previous noncontrolled studies.7,8 After recruitment and PEEP titration, patients were ventilated under volume-assist control mode with PEEP set at the titrated value (the PEEP associated with highest respiratory-system compliance plus 2 cm H2O). If Pao2:Fio2 levels were stable or increasing for 24 hours or more after recruitment, weaning of PEEP was started with decreases of 2 cm H2O every 8 hours. Apart from the lung recruitment maneuver and PEEP titration scheme, other aspects of care were similar for both groups. The experimental and control group procedures are detailed in the protocol and the manual of operations (Supplements 1 and 3). Quiz Ref IDInitially, we applied a recruitment maneuver using pressure-controlled ventilation and driving pressure of 15 cm H2O. We started with a PEEP of 25 cm H2O for 1 minute, then a PEEP of 35 cm H2O for 1 minute, and then 45 cm H2O for 2 minutes. After recruitment, decremental PEEP titration was started with a PEEP of 23 cm H2O in volume-controlled mode. PEEP levels were decreased in steps of 3 cm H2O down to a minimum of 11 cm H2O. After 4 minutes in each step, we measured respiratory-system static compliance. The PEEP associated with the best compliance plus 2 cm H2O was considered the optimal PEEP. After PEEP titration, a new recruitment in pressure-controlled ventilation was conducted in 1 step using PEEP of 45 cm H2O for 2 minutes. In June 2015, starting with the 556th patient, the steering committee, in consultation with the data monitoring committee, decided to modify the recruitment maneuver and PEEP titration strategy after 3 cases of resuscitated cardiac arrest possibly associated with the experimental group treatment were observed. During the recruitment maneuver, PEEP was increased to 25 cm H2O, 30 cm H2O, and then 35 cm H2O, in steps of 1 minute. Maximum plateau pressure was 50 cm H2O. Decremental PEEP trial was shorter, with each PEEP step lasting 3 minutes, followed by a new recruitment maneuver with PEEP of 35 cm H2O. Our primary outcome was mortality until 28 days. Secondary outcomes were length of ICU and hospital stay; ventilator-free days from day 1 until day 28; pneumothorax requiring drainage within 7 days; barotrauma within 7 days; and ICU, in-hospital, and 6-month mortality. We defined as pneumothorax requiring drainage for any case that was possibly due to barotrauma; that is, we did not consider cases judged to be clearly caused by invasive procedures such as central venous punction or thoracocentesis. We defined as barotrauma within 7 days any pneumothorax, pneumomediastinum, subcutaneous emphysema, or pneumatocele of more than 2 cm detected on image examinations between randomization and 7 days, except those judged to be clearly caused by invasive procedures. All analyses followed the intention-to-treat principle, considering all patients in the treatment groups to which they were randomly assigned, except for cases lost to follow-up. We carried out complete-case analysis for all outcomes. We planned to conduct sensitivity analysis for the primary outcome using multiple imputation techniques only if follow-up data of 1% or more of the patients was lost. Baseline characteristics were reported as counts and percentages, mean and standard deviation (SD), or median and interquartile range (IQR), whenever appropriate. Hypothesis tests were 2-sided. Two interim analyses were performed after recruitment of one-third and two-thirds of the planned sample size to assess effects on clinical outcomes. The data monitoring committee would consider stopping the trial early if there was evidence of harm with 1-sided P value <.01. The significance level for the primary outcome final analysis was .042, to maintain overall α at .05. For all other outcomes, the significance level was .05, without adjustment for multiple comparisons. Because of this, all secondary outcomes and analyses should be interpreted as exploratory. We assessed the effect of the trial treatments on the primary outcome using Kaplan-Meier curves and calculated the hazard ratio with 95% CI using the Cox proportional hazard model. We conducted 2 sensitivity analyses. The first was a prespecified Cox proportional hazards model adjusted for age, Simplified Acute Physiology Score 3 (SAPS 3) score, and Pao2:Fio2 ratio. The second was a post hoc frailty Cox model with stratification variables (site, age, and Pao2:Fio2) as random effects. We used Cox proportional hazards to assess interactions between treatment effect and the following prespecified subgroups: Pao2:Fio2 (≤100 vs >100 mm Hg); SAPS 3 score (<50 vs ≥50); pulmonary vs extrapulmonary ARDS; duration of ARDS (≤36 hours vs >36 to <72 hours); mechanical ventilation (≤2 days, 3-4 days, ≥5 days); and prone position. As an exploratory analysis, we tested whether treatment effects were similar before and after the protocol amendment of June 2015. We also tested in a post hoc analysis whether treatment effects per quartiles according to order of enrollment in the trial (earlier vs later) were homogeneous. All analyses were performed using the R (R Core Team, 2016) software. Quiz Ref IDFrom November 17, 2011, through April 25, 2017, we screened 2077 patients with moderate to severe ARDS. A total of 1064 were not enrolled, of whom 863 (81.1%) met exclusion criteria and 201 (18.9%) were eligible but were not enrolled for other reasons. We randomized 1013 patients, 501 to the lung recruitment strategy and 512 to the low-PEEP strategy. Representatives of 3 patients assigned to the control group withdrew consent to use study data. We obtained 28-day and 6-month follow-up data of all remaining patients, except for 23 who were followed up and censored between 2 and 6 months. Thus, data of 1010 patients (501 in the experimental group and 509 in the control groups) were considered for the final analysis. The data monitoring committee evaluated 2 interim analyses and recommended the trial to be continued. (Figure 1). Baseline characteristics were well balanced between the study groups (Table 1). Two-thirds of the patients had septic shock. The mean number of nonpulmonary organ failures was more than 2. Most ARDS cases were of pulmonary (62.0%) rather than extrapulmonary origin (38.0%). In the experimental and control groups, baseline mean (SD) tidal volume and plateau pressures were 5.8 (1.1) and 5.8 (1.0) mL/Kg of predicted body weight, and 25.8 (4.7) and 26.2 (5.2) cm H2O, respectively. A total of 480 patients (95.8%) in the experimental group received a lung recruitment maneuver after randomization (eTable 1 in Supplement 2). In 78 cases (15.6%) the maneuver had to be interrupted, most often due to hypotension or a decrease in oxygen saturation. In 21 cases, a recruitment maneuver was not attempted due to uncontrolled hypotension (14 cases), detection of pneumothorax (3 cases) after randomization, or other reasons (4 cases). The mean (SD) titrated PEEP was 16.8 (3.8) cm H2O. Lung recruitment was repeated after PEEP titration in 393 patients (78.4%). After the initial recruitment and PEEP titration, alveolar recruitment was not repeated from day 1 to 7 in most patients (62.7%). Conversely, 28 patients in the control group also received a recruitment maneuver within the first 7 days. Mean PEEP values from hour 1 through day 7 were higher in the experimental than in the control group (eTable 2 in Supplement 2). Mean values of plateau pressure were also higher in the experimental group, although always below 30 cm H2O in both groups. Mean tidal volumes were below 6 mL/kg of predicted body weight in both groups from hour 1 through day 3. The mean Pao2:Fio2 ratios were higher in the experimental group. Yet decreases in driving pressure from control to experimental group were limited to less than 2 cm H2O from day 1 through day 7. Partial pressure of carbon dioxide was higher and arterial pH was lower in experimental group only at the first hour, with values that were not significantly different after day 1. All-cause mortality was also higher within 6 months in the experimental than in the control group (65.3% vs 59.9%; hazard ratio, 1.18; 95% CI, 1.01-1.38; P = .04) (Table 2). Differences in the ICU or in-hospital mortality between groups were not statistically significant. Compared with the control group, mortality in the experimental group was higher during the first 7 days, with increased rates of death with barotrauma (Table 2). There were no significant differences in the rates of death with refractory hypoxemia, death with acidosis, and cardiorespiratory arrest between groups. Lengths of stay in the ICU or hospital were also not significantly different. The experimental group had fewer ventilator-free days during the first 28 days. The rates of pneumothorax requiring drainage and rates of any barotrauma increased in the experimental group. Within 1 hour, commencement or increase in vasopressors or hypotension were more common in the experimental group, but there were no differences in refractory hypoxemia or severe acidosis. One potential explanation for the findings of this trial relates to an unfavorable balance between potential positive (reduction in driving pressure)10 and negative (increase in overdistention, hemodynamic impairment)18,19 physiological consequences of lung recruitment and PEEP. Although some studies showed almost full opening of collapsed alveoli after recruitment maneuvers achieving high inspiratory pressures,7,8 only mild responses were observed in this trial as suggested by the small increments in the respiratory-system compliance and reductions in driving pressure. Furthermore, the driving pressure, a strong predictor of survival in ARDS,20,21 decreased by a mean of only less than 2 cm of water. On the contrary, the risk of barotrauma within 7 days and signs of hemodynamic impairment within 1 hour increased in the experimental group, suggesting lung injury and hemodynamic impairment as mechanisms that may have driven increased mortality. Nevertheless, the incidence of barotrauma, even in the experimental group, was lower than in any previous studies using high PEEP levels.22. This trial has strengths. Bias was controlled by using concealed allocation, intention-to-treat analysis, and by avoiding losses to follow-up. Analyses were based on a large number of events, which allowed for adequate random error control. Patient eligibility was confirmed only after ventilation with a lung protective low-tidal volume strategy and standardized Fio2 and PEEP settings before collecting arterial blood gases. Except for the lung recruitment maneuvers and PEEP titration scheme, identical mechanical ventilation protocol with low-tidal volume was applied for both groups. In addition, the study involved centers from 9 countries, which contributes to generalizability of its results. This study has several limitations. First, it was not feasible to blind participants, clinicians, and outcome assessors. It is possible that processes of care might have been affected by knowledge of treatment allocation. Conversely, blinding would not affect classification of the primary outcome. Second, it was not possible to classify enrolled patients into ARDS subphenotypes, which may respond differently to therapies such as PEEP.27,28 Determination of subphenotype requires collecting plasma samples to perform analysis of biomarkers; however, this was not conducted due to funding restrictions. Third, it has been suggested that baseline responsiveness to a test of PEEP elevation predicts percentage of potentially recruitable lung and the clinical response to a strategy of lung recruitment associated with high PEEP.29,30 However, since responsiveness to PEEP at baseline was not assessed, it is not possible to analyze whether this characteristic modifies treatment effect. Nevertheless, there was no evidence of heterogeneity of treatment effect in any of the subgroups examined. Fourth, patients were enrolled in the trial over 6 years. The care of patients with ARDS may have changed during this period, which might have affected results. However, an analysis of treatment effects on mortality according to order of enrollment provides no evidence that effects changed over time. Fifth, a strategy involving lung recruitment and PEEP titration (primary interventions) is complex in the sense that not only the primary interventions are part of it, but also cointerventions that need to be aggregated. For example, administering neuromuscular blockers and fluids in preparation for the recruitment maneuver. As a consequence, it is not possible to ascribe observed clinical effects exclusively to the direct effects of lung recruitment maneuver and PEEP. Systolic arterial blood pressures were higher in the individualized treatment group (P < .001 by random-effect model for the between-group comparison across the entire study intervention). The horizontal line in the center of each box indicates the median; bottom and top borders of the box, 25th and 75th percentiles, respectively; whiskers, 1.5 times the interquartile range (IQR); and circles, extreme outliers. The intervention period lasted from anesthesia induction to 4 hours after completion of surgery. The median (IQR) duration of surgery was 260 (170-365) minutes in the individualized treatment group and 280 (200-375) minutes in the standard treatment group. The median (IQR) duration of the intervention period was 423 (342-550) minutes in the individualized treatment group and 465 (390-600) minutes in the standard treatment group. Organ dysfunction was assessed for renal (risk, injury, failure, loss, and end-stage kidney injury [RIFLE] stage of risk or higher), respiratory (need for invasive or noninvasive ventilation), cardiovascular (acute cardiac failure or myocardial ischemia or infarction), neurologic (stroke or altered consciousness), and coagulation (Sequential Organ Failure Assessment subscore ≥2 points in the coagulation component) systems. Data for patients who did not develop organ dysfunction were censored at 30 days after surgery. The adjusted hazard ratio (HR) for postoperative organ dysfunction in the individualized treatment group, as compared with the standard treatment group, was 0.66 (95% CI, 0.52-0.84; P = .001). The median follow-up duration was 30 days (interquartile range, 30-30 days) in the 2 treatment groups. Quiz Ref IDHemodynamic instability is common during surgery. There is accumulating evidence that intraoperative hypotension is associated with injury to heart, kidney, and brain and an increased likelihood of mortality in high-risk patients.3-6 However, intraoperative hypotension is a preventable risk factor as arterial pressure is modifiable using intravenous fluids and/or vasopressors. There is no consensus regarding optimal blood pressure target thresholds to support perfusion of critical organs during surgery. Systolic blood pressure (SBP) less than 80 mm Hg,6,7 mean arterial pressure less than 60 mm Hg,4 and a reduction of 30% to 50% from baseline are common treatment thresholds used in clinical practice,7,8 highlighting the lack of consensus. Current guidelines from the American College of Cardiology and the American Heart Association9 in the setting of noncardiac surgery recommend individualizing care for surgical patients with associated comorbidities. In patients with preexisting hypertension, the autoregulatory capacity of the brain and kidneys is likely impaired,10,11 thus rendering organs more susceptible to ischemia at low blood pressure. Accordingly, higher blood pressure targets tailored to individual patient physiology may be preferable for such high-risk patients.4,5,8,12 Consensus guidelines in the context of critical illness have suggested adjusting blood pressure targets to premorbid values.13 However, trial data are lacking for an individualized strategy in the surgical setting. This was an investigator-initiated, multicenter, stratified, parallel-group randomized clinical trial conducted in 9 French university and nonuniversity hospitals. The trial protocol was approved for all centers on January 5, 2011, by the ethics committee at the Clermont-Ferrand University Hospital. Written informed consent was obtained from each patient before randomization and surgery. The trial protocol and the statistical analysis plan are available in Supplement 1. An independent data and safety monitoring committee oversaw the study conduct and reviewed blinded safety data. Patients were assessed for eligibility on the eve of their surgery. Patients were eligible for participation if they were aged 50 years or older, were scheduled to undergo surgery under general anesthesia with an expected duration of 2 hours or longer, had an American Society of Anesthesiologists physical status of class II or higher, had a preoperative acute kidney injury risk index14 of class III or higher, and did not meet any exclusion criteria. The acute kidney injury risk index ranges from I to V, with higher classes indicating a higher risk of postoperative acute kidney injury (eAppendix in Supplement 2). Patients were excluded if they had severe uncontrolled hypertension (SBP ≥180 mm Hg or diastolic blood pressure ≥110 mm Hg); had chronic kidney disease (glomerular filtration rate <30 mL/min/1.73 m2 or requiring renal replacement therapy for end-stage renal disease); had acute or decompensated heart failure or acute coronary syndrome; had preoperative sepsis or were already receiving norepinephrine infusion; required renal vascular surgery; or were enrolled in another study. Detailed exclusion criteria are listed in the eAppendix in Supplement 2. Quiz Ref IDEligible patients were assigned in a 1:1 ratio to either a standard or individualized treatment strategy. The resting blood pressure from the preoperative anesthesiology consultation was obtained from the patient medical record and used as the reference value. If this was unavailable, the blood pressure measurement recorded by a nurse of the surgical ward the day before surgery, while the patient was in supine position, was used as the reference value. In the standard treatment group, patients received intravenous ephedrine administered in 6-mg boluses (for a maximum dose not exceeding 60 mg), as recommended,15 for any decrease in SBP below 80 mm Hg or lower than 40% from the patient’s reference value.7Quiz Ref ID In the individualized treatment group, SBP was targeted to remain within ±10% of the reference value using a continuous infusion of norepinephrine. Norepinephrine was diluted as 2.5 mg in 250 mL of 0.9% saline. The infusion rate of norepinephrine was adjusted according to a dedicated table (eAppendix in Supplement 2). In both groups, lactated Ringer solution was infused intravenously at a rate of 4 mL/kg per hour to satisfy maintenance fluid requirements. Additional fluids were given based on a protocolized hemodynamic algorithm,16,17 using 6% hydroxyethyl starch (molecular weight of 130 kDa, substitution ratio of 0.4) in 0.9% saline administered in 250-mL boluses to achieve and maintain a maximal value of stroke volume (eAppendix in Supplement 2). In the individualized treatment group, a reduction in the norepinephrine infusion rate was recommended in the case of severe bradycardia (heart rate <40 beats/min). In the standard treatment group, if SBP remained below the target value after a maximum dose of 60 mg of ephedrine, the use of norepinephrine was permitted as rescue therapy. Group assignment was not modified, and data analysis was conducted on a modified intention-to-treat basis. The intervention period lasted from anesthesia induction to 4 hours after completion of surgery. With the exception of the interventions described earlier, decisions regarding all other aspects of patient care during and after surgery were at the discretion of the attending physician according to local expertise and clinical practice. To avoid extremes of practice, invasive blood pressure measurement through a radial catheter was required. Additional details are given in the trial protocol in Supplement 1. The prespecified secondary outcomes included the individual components of the primary composite outcome; changes in hemodynamic variables; the SOFA score on days 1, 2, and 7; the SIRS score21; postoperative complications; durations of intensive care unit and hospital stay; and all-cause mortality at 30 days after surgery. Postoperative complications within 30 days after surgery were defined as infectious complications (sepsis, severe sepsis, and septic shock using the 2001 International Sepsis Definitions22), respiratory complications (hypoxemia, pneumonia, need for noninvasive or invasive mechanical ventilation for respiratory failure, acute respiratory distress syndrome), neurologic complications (stroke, altered consciousness), cardiovascular complications (cardiac arrhythmia, acute heart failure, myocardial infarction), and surgical complications (anastomotic leak, surgical site infection, reoperation). Adverse events included severe bradycardia (ie, heart rate <40 beats/min) and major bleeding (ie, transfusion of ≥4 units of red blood cells). More details of these definitions are provided in the trial protocol in Supplement 1. Other end points not reported in this article are listed in the eAppendix in Supplement 2. Enrollment, randomization (1:1 allocation ratio), and data collection were performed using a dedicated, secure, web-based system. Randomization was performed with the use of a minimization algorithm and stratified according to study site, urgency of surgery, and surgical site (abdominal or nonabdominal surgery). Although the research staff members who collected data during surgery could not be blinded to group assignments, much attention was given to ensuring strict blinding during the follow-up period and during data collection. The medical team who provided care during the postoperative period (ie, in the intensive care unit and the surgical ward), investigators, patients, the statistician, and the data and safety monitoring committee were unaware of the group assignments. Outcomes were verified according to predefined criteria by the principal investigator or designee at each site. Automated validation checks included plausibility ranges and cross-checks between data fields. Further data checks were performed centrally and through source data verification. We calculated that a sample of 268 patients would provide the trial with 95% power to detect an absolute difference of 20% with respect to the primary outcome, at a 2-sided α level of .05, assuming an event rate of 40% in the composite outcome in the standard treatment group.14,21,23,24 The choice of 20% as expected difference in the primary outcome was based on the effect size observed in an earlier study in high-risk surgical patients.25 To account for potential protocol deviations and withdrawal of consent, the recruitment target was 300 patients. An independent data and safety monitoring committee performed a blinded and planned interim analysis after enrollment of 50% of patients using the Lan-DeMets method to evaluate adverse events. There was no stopping rule for efficacy when considering the primary outcome. The committee recommended that the study be continued. All analyses were conducted before the randomization code was broken, in line with the International Conference on Harmonization Good Clinical Practice guidelines. All the analyses were performed on data from the modified intention-to-treat population, which included all randomly assigned participants who initiated the study intervention and did not withdraw consent for the use of their data. An unadjusted χ2 test was used for the primary outcome analysis. Multiple logistic mixed regression analysis was used to identify relevant baseline covariates associated with the primary outcome, in addition to the stratification variables (center treated as a random effect). Adjusted analyses were performed with the use of robust Poisson generalized linear model regression,26 including a random effect to account for center effect, and are presented as relative risks with 95% confidence intervals. Results for the primary outcome are additionally reported as absolute risk reductions with 95% confidence intervals. The Hochberg procedure was used to adjust for multiple testing of components of the composite primary outcome.27 A random-effects model was used to model longitudinal differences in SBP between treatment groups, taking into account between- and within-patient variability, in addition to center effect. Kaplan-Meier curves were plotted for organ dysfunction for renal, respiratory, cardiovascular, neurologic, and coagulation systems and compared by marginal Cox model. Follow-up time was censored at 30 days following surgery. The time to organ dysfunction was analyzed using a marginal Cox proportional hazards model with results reported as hazard ratios with 95% confidence intervals, and proportional hazard assumption verified using the Schoenfeld test and plotting residuals. As less than 5% of data were missing, handling of missing data was not applied. We did not compensate for dropouts caused by the withdrawal of consent or surgery cancellations after randomization. With the exception of the components of the composite primary outcome, no adjustment was made for multiple comparisons; therefore, secondary outcomes should be considered exploratory. All hypothesis tests were 2-sided, and P < .05 was considered to indicate statistical significance. The statistical analysis was conducted using Stata software version 13.0 (StataCorp LP). During the study period from December 4, 2012, through August 28, 2016, a total of 1494 patients were screened for eligibility, and 298 patients were ultimately enrolled and randomized (Figure 1). Last follow-up was September 28, 2016. Data on the primary outcome were available for 292 patients (mean [SD] age, 70 [7] years; 44 [15.1%] women; 147 patients in the individualized treatment group and 145 patients in the standard treatment group) who completed the trial and were included in the modified intention-to-treat analysis. Two patients (1 per group) had care adherent with the assigned SBP target but not with the vasopressor, and were included in the analysis of the group to which they were assigned. The 2 groups were well balanced at baseline (Table 1; eTable 1 in Supplement 2). Overall, 240 patients (82.2%) had chronic hypertension. Sixty-one of 100 patients (61.0%) in the individualized treatment group and 58 of 97 patients (59.8%) in the standard treatment group had discontinued their antihypertensive medication prior to surgery. Values for reference resting blood pressure were similar between study groups. Throughout surgery, the mean (SD) SBP was 123 (25) mm Hg in the individualized treatment group and 116 (24) mm Hg in the standard treatment group (Figure 2; eFigure 1 and eFigure 2 in Supplement 2); the between-group difference was 6.5 mm Hg (95% CI, 3.8-9.2). The cumulative volume of fluids infused over the intervention period and the cardiac index values were not significantly different between study groups (Table 2). Six patients (4.1%) in the individualized treatment group and 22 (15.2%) in the standard treatment group met SBP targets throughout the intervention period without any need for vasopressor (absolute difference, 11%; 95% CI, 4%-18%; P = .001). Thirty-eight patients (26.2%) in the standard treatment group required rescue therapy with norepinephrine to achieve the target SBP value because of persistent hypotension despite receiving ephedrine (Table 2). Protection against hypoperfusion relies primarily on maintaining adequate intravascular volume and organ perfusion pressure. One strength of this trial is the use in both groups of a protocolized hemodynamic algorithm to guide delivery of intravenous fluids and maximize stroke volume. Previous trials have suggested a lower incidence of organ dysfunction with goal-directed hemodynamic optimization during surgery.36 In this trial, no between-group differences were noted in the cardiac index or in the cumulative volume of fluids. No association was found between the fluid composition and the primary outcome event. This study has several limitations. The use of ephedrine as the first-line vasopressor for standard care, rather than other vasoactive drugs such as phenylephrine, was arbitrary but supported by literature.15,37 Moreover, phenylephrine is a selective α1-adrenergic agonist with a greater risk of negative effects on cardiac output,38 in contrast to ephedrine or norepinephrine, which have β-adrenergic activity.37,39 Although the use of norepinephrine rather than ephedrine in the standard treatment group might have enhanced the study design, data on the use of norepinephrine to manage arterial pressure in the operating room are relatively scarce. Furthermore, the efficacy and safety of intermittent intravenous boluses of norepinephrine, rather than continuous infusion, to treat a decrease in blood pressure have not been extensively studied. More than 80% of patients had chronic hypertension, and in these individuals, organ blood flow may become pressure dependent at higher blood pressure limits due to a possible rightward shift of the organ autoregulation curves. As discussed previously, the duration of hypotensive events was not recorded, and substantial variations in blood pressure between measurement points may have occurred. The minimum duration of hypotension to trigger harm is unclear, but a graded relationship between the duration of hypotension and postoperative acute kidney injury has previously been assumed.4,5 Generalizability to populations not included in the trial, such as those with a lower risk of morbidity, remains to be evaluated. Moreover, use of the resting blood pressure as reference—which may not be available in daily care—rather than preinduction values may represent a meaningful difference with routine clinical practice. The intervention could not be blinded, but the risk of bias was minimized through online randomization to ensure the concealment of study group assignments, the use of validated criteria for the primary outcome that were not subject to observer bias, and health care workers conducting postoperative care who were unaware of the study assignments. Mechanical ventilation is critical for the survival of many patients with the acute respiratory distress syndrome (ARDS) but can also cause ventilator-induced lung injury (VILI). One form of VILI occurs when the lungs exhale to relatively low volumes and airway pressures. This may cause injurious tidal closing and reopening of small bronchioles and alveoli or excessive stress at the margins between aerated and atelectatic airspaces.1,2 In animal studies, positive end-expiratory pressure (PEEP) reduced or prevented VILI from exhalation to low volumes and pressures.1,3-5 PEEP can also recruit some previously atelectatic or fluid-filled lung regions, allowing more of the lung to be available for inflation during inspiration. This could reduce VILI from overdistention of an otherwise reduced amount of aerated lung. In some early studies, the levels of PEEP that were applied for lung protection exceeded the levels that most clinicians use when managing patients with ARDS. This led to recommendations to use higher PEEP in patients with ARDS to minimize low volume and low pressure VILI and hopefully to improve clinical outcomes. The “open lung approach” (OLA) aims to achieve high levels of lung aeration in patients with ARDS by first conducting recruitment maneuvers (RMs) to reverse atelectasis and then applying high levels of PEEP to keep recruited alveoli open.6 Recruitment maneuvers typically involve a ventilatory approach that transiently increases pulmonary airway pressure to reopen recruitable lung areas. For example, an RM can be conducted by raising inspiratory airway pressures to 50 cm of H2O for 1 or 2 minutes. In this issue of JAMA, the Writing Group for the Alveolar Recruitment for Acute Respiratory Distress Syndrome Trial (ART) Investigators12 report the results of a large randomized clinical trial that was conducted in 120 intensive care units in 9 countries and compared patients treated with the OLA (n = 501) with those managed with conventional PEEP (n = 509). As in the recent pilot OLA trial, patients in this trial had moderate to severe ARDS (Pao2/Fio2 ≤200). To reduce VILI from overdistention in both study groups, the target tidal volume was 6 mL/kg of predicted body weight with inspiratory plateau pressures of 30 cm H2O or less, consistent with the National Institutes of Health (NIH) ARDS Network protocol.13 Patients in the OLA study group received RMs with PEEP as high as 45 cm H2O and peak airway pressures as high as 60 cm H2O.14 These were followed immediately by a decremental PEEP titration to identify the PEEP at which lung compliance was maximal, presumably representing the best balance between recruitment and overdistention. The control group used the NIH ARDS Network lower PEEP/Fio2 table without RMs.7. The difference in mean levels of PEEP between the OLA and the control group in the study by the ART investigators was only 3 to 4 cm H2O over the first 7 days. The lung protection that results from these differences in PEEP is probably small. Also, the difference between study groups in driving pressure over the first 3 days was less than 2 cm H2O. Because tidal volumes were similar in the 2 study groups, the differences in driving pressure reflect primarily the differences in volumes of aerated lung. The small differences in driving pressures suggest that the OLA protocol did not result in much recruitment. Additionally, higher PEEP in the OLA group may have caused more overdistention, which would tend to increase driving pressures, offsetting the decreases that might have resulted from additional recruitment of aerated lung. The mean PEEP in the control group of the ART investigators’ trial was approximately 3 cm H2O higher than the mean PEEP levels in the control groups of the other 3 large clinical trials. The authors suggest that the reason for higher PEEP levels in their control group was stricter adherence to the lower tidal volume protocol resulting in tidal volumes that were lower in their control group compared with the other control groups. The authors suggest that these lower tidal volumes could have caused hypoxemia from atelectasis, which could have triggered the use of higher PEEP to maintain the arterial oxygenation goal. It is possible that the lower tidal volumes, rather than the higher PEEP, contributed to the lower mortality in their control group. Therapies are intended to improve patient outcomes, but all therapies have the potential to cause harm. PEEP can reduce VILI from injurious tidal opening and closing. However, PEEP also raises intracardiac pressures, including right atrial pressure, which impedes venous return and cardiac output. Moreover, especially in the absence of significant lung recruitment, PEEP increases right ventricular afterload by compressing alveolar septal capillaries, increasing pulmonary vascular resistance. Many critically ill patients experience shock from acute right ventricular failure, and high levels of PEEP may contribute to this.15 In the study by  the ART investigators, patients in the OLA group required more vasopressors within 1 hour of beginning the protocol, and 3 patients experienced cardiac arrests. PEEP also causes higher inspiratory pressures and volumes, increasing the risk of barotrauma and VILI from overdistention. The only way to know with confidence that beneficial effects of a therapy outweigh the detrimental effects is to conduct a randomized clinical trial and monitor important clinical outcomes. In the trial by the ART investigators, the detrimental effects of the OLA exceeded the beneficial effects. A mechanical ventilation strategy that is designed to be lung protective may not be “patient protective.”. Despite abundant evidence in experimental models that the OLA can reduce VILI, 4 large randomized clinical trials of higher PEEP and RMs have now failed to demonstrate improved clinical outcomes, and the trial by the ART investigators suggested actual harm. In other studies, only approximately 50% of all patients with ARDS responded to higher airway pressures by recruiting previously atelectatic or flooded alveoli.16,17 It follows from this observation that the OLA is more likely to have overall beneficial effects among patients whose affected lung segments can be recruited. As in the previous large randomized clinical trials of higher PEEP, the ART investigators did not attempt to identify PEEP responders and exclude the nonresponders. It is possible that there were different effects of the OLA in this trial among the subsets of patients whose involved lung areas were more or less recruitable. The ART investigators have conducted a rigorous, large, multicenter, international trial that demonstrated that an OLA improves arterial oxygenation and driving pressure compared with the control group, similar to findings from the recent pilot trial,11 but appears to worsen patient outcomes including mortality. The results are not only disappointing but will be unexpected for many intensive care physicians and researchers working on VILI and lung-protective ventilation. However, such results frequently lead to greater insights and more productive directions for the future. The results of the trial by the ART investigators were different than what would have been predicted from the individual patient data meta-analysis.10 The harmful effects of the OLA observed in the trial by the ART Investigators may have been due to the aggressive RMs and decremental PEEP titration. PEEP has been used during mechanical ventilation since the landmark description of ARDS 50 years ago.18 However, the best method for setting PEEP levels has still not been established. Perhaps further refinements in the OLA strategy with less aggressive attempts at lung recruitment and a focus on identifying patients who recruit in response to PEEP will lead to more favorable results and leave the door to the OLA cracked open.19,20 On the other hand, now that 4 large, randomized clinical trials of strategies designed to promote lung recruitment have failed to demonstrate improved mortality, perhaps the door on the OLA should be allowed to close so that the clinical and research community can move on to other, potentially more effective strategies. Because tidal volume reduction is a powerful tool to prevent VILI13 and because lower tidal volumes reduce VILI from both overdistention and from opening-closing injury, perhaps future studies should push the limits of lowering tidal volume below 6 mL/kg of predicted body weight. Ultimately, allowing part of the lung to stay closed with permissive atelectasis may be more patient-protective than aggressive efforts to keep the lung open. The management goals for perioperative (a term that includes intraoperative) BP are different from nonsurgical BP management. Perioperative BP changes are due to multiple factors, including rapid intravenous volume shifts, changes in sympathetic tone due to onset and offset of anesthetic agents, surgical stimulation, autonomic stress responses, and pain. Anesthetic drugs with variable pharmacokinetic properties affect cardiovascular function in a dose-dependent manner by reducing central sympathetic tone, attenuating baroreflex activity, inhibiting end-organ BP autoregulatory function, and having direct effects on the heart and peripheral vasculature. Opioids affect cardiovascular function by attenuation of sympathetic afferent and efferent activity, direct central or peripheral vagal stimulation, and direct and indirect effects on the myocardium and vascular smooth muscle. Indeed, decreased or elevated BP values in the perioperative setting often lead to situations that require active tolerance (a proactive decision not to treat) or, conversely, acute management in what would otherwise be perceived as an unacceptable or acceptable nonsurgical situation, respectively. It is therefore generally accepted that optimal perioperative BP should be defined differently than commonly referenced in an ambulatory environment due to unique clinical-physiological considerations in a setting of anesthesia and surgery. Whereas data from the ambulatory setting has challenged traditional optimal BP management standards,6 it is also recognized that in the surgical setting a relationship between BP and outcomes is complex. Several components contribute to this complexity. What defines “appropriate” BP targets and to what degree these targets are applicable across patient types or surgical interventions have only recently been explored,7-14 with some retrospective studies capitalizing on electronic health record data acquisition to enable large-scale analysis. Given the paucity of data linking perioperative BP management and outcomes, BP management decisions are usually based on clinician knowledge, experience, and beliefs and are subject to regional and geographic practice patterns. Even though it may seem intuitive that defining perioperative goals for BP target control should depend on patient factors such as the presence of preexisting hypertensive disease, the specific acute-care situation that is being managed, the vulnerable end organ for each patient in each instance, and treatment safety and effectiveness, there are few to no data to support any specific practice. A rational, scientific, data-based intraoperative BP management guidance strategy is needed. Understanding what constitutes a safe BP during the perioperative period is a complex medical decision dependent on several factors, including patient physiology and procedural need; as such, defining a “best way” to achieve a target BP during the perioperative period is challenging. The Intraoperative Norepinephrine to Control Arterial Pressure (INPRESS) study reported by Futier and colleagues in this issue of JAMA15 prospectively compared 2 intraoperative BP management strategies in a high-risk noncardiac surgical population. In this multicenter randomized clinical trial conducted in 9 French university and nonuniversity hospitals, adult patients (n = 298) at increased risk of postoperative complications who were undergoing major surgery (predominantly abdominal operations) under general anesthesia were randomly assigned to receive an individualized management strategy aimed at achieving a systolic BP (SBP) within 10% of the reference value (ie, the patient’s resting SBP) or a standard management strategy of treating SBP values less than 80 mm Hg or lower than 40% from the reference value during and for 4 hours following surgery. The primary outcome event, a composite of systemic inflammatory response syndrome and dysfunction of at least 1 organ system of the renal, respiratory, cardiovascular, coagulation, and neurologic systems by day 7 after surgery, occurred in 56 of 147 patients (38.1%) assigned to the individualized treatment strategy vs 75 of 145 patients (51.7%) assigned to the standard treatment strategy (relative risk, 0.73; 95% CI, 0.56-0.94). In addition, at 30 days, the cumulative probability of postoperative organ dysfunction was lower in the individualized treatment group than in the standard treatment group (68 patients [46.3%] vs 92 patients [63.4%], respectively; adjusted hazard ratio, 0.66; 95% CI, 0.52-0.84), although there was no significant difference in all-cause mortality. The findings from this well-done trial suggest that a personalized baseline BP is an important consideration when defining an intraoperative BP target and that active management with combination β and α-agonist pharmacologic intervention is a reasonable and perhaps useful practice. However, several aspects of the analysis merit further consideration, including the applicability of the findings to other surgical procedures and patient populations and the specific nature of the management used. Although perioperative BP was associated with outcomes, it is difficult to determine whether untreated intraoperative hypotension or the treatment of such hypotension with intravenous fluids, vasoconstrictive drugs, or inotropes contributes directly to observed outcomes. This will need to be further investigated with other comparative effectiveness research trials. That each patient may have a personalized signature of an acceptable low intraoperative BP based on his or her baseline BP is consistent with what is known about absolute low BP thresholds. From these data, it is possible to infer that for a patient with chronic hypertensive disease, the lowest acceptable intraoperative BP is higher than previously recognized or perhaps expected. The reason, although not tested in this trial, could be due to end-organ BP perfusion-compensatory changes that occur as a consequence of chronic hypertensive disease. The study by Futier et al15 was conducted in a complex clinical setting, provides clinically meaningful and important findings, and represents an important first step toward an elusive and heretofore scarcely ventured attempt to understand a best-practice perioperative medicine management pathway. The pathway to improved outcomes will no doubt be long and confounded by many circumstances, but it will be well worth the effort. The INPRESS trial not only provides a view of the important domain of perioperative randomized clinical trial design for conventional treatment of common conditions but also offers a glimpse of what may lie beyond when attention is focused on the clinical science of perioperative medicine. The study gives new meaning to the advice to “avoid hypoxia and hypotension.”. The ability to use AMH level as an assessment of ovarian follicle number has been part of the clinical evaluation of infertility for more than a decade. Because most fertility treatments rely on the stimulation or harvesting of as many oocytes as possible, knowing a woman’s AMH before she begins stimulation is helpful in medication management. Very low AMH levels (<1 ng/mL) in the setting of infertility are a poor prognostic indicator for a live birth when ovarian stimulation is used, either for in vitro fertilization or for superovulation.7 These very low levels also help cue the clinician to proceed with more aggressive ovarian stimulation protocols. Virtually all assisted reproductive technologies require ovarian stimulation, and the more follicles that can be stimulated, the more likely that there will be fertilization and a live birth. Patients with infertility are rarely limited by the availability of sperm, as current assisted reproductive technology methods such as intracytoplasmic sperm injection theoretically require only a single sperm; the number of oocytes available for fertilization is almost always the limiting factor. The report by Steiner et al9 in this issue of JAMA suggests that it may be necessary to reevaluate what an AMH level really means for a woman’s reproductive health. The authors measured a single circulating AMH level (along with levels of follicle-stimulating hormone [FSH], inhibin B, and urinary levels of FSH) for each participant in a time-to-pregnancy study of a community-based sample of women who did not have a history of infertility. Among 750 women included in the analysis, 487 (65%) conceived, 123 (17%) completed the study but did not conceive, 37 (5%) withdrew, 47 (6%) started fertility medications, and 56 (7%) were lost to follow-up. In Cox analysis, the probability of conception was 65% by 6 cycles of pregnancy attempt and 77% by 12 cycles of pregnancy attempt. Time to pregnancy was virtually identical between women with normal AMH levels (n = 579) and those with low AMH levels (n = 84; defined as 0.7 ng/mL, a bit lower than the conventional definition of <1 ng/mL in the infertility literature). Perhaps most surprising was the lack of an association between low AMH and time to pregnancy among women in the oldest age group in this study, those aged 38 to 44 years. The large study sample size, the community-based nature of the cohort, and the robust measurement methods used all support the credibility of the findings. Consistent with the observation that low AMH was not associated with difficulty conceiving, a high FSH level (>10 mIU/mL, a widely used indirect criterion for a poor prognosis with infertility treatment) compared with normal FSH levels also was not significantly related to the probability of conception. As reported in other studies, attempting to improve predictions by using both AMH and FSH together did not affect the findings, which indicates that these 2 markers are essentially measuring the same aspect of ovarian reserve. Antimüllerian hormone reflects ovarian reserve more effectively because of its stability across the menstrual cycle and lower cycle-to-cycle variability than FSH. However, the findings reported by Steiner et al9 should be considered in the context of several caveats. First, the study population included a selected group of women. For instance, women with known fertility problems or with known fertility problems that affected their partners were excluded. Second, and importantly, pregnancy outcomes beyond a positive pregnancy test result were unavailable. It is therefore possible that women with low ovarian reserve as defined in this study experienced more pregnancy loss and had lower live birth rates. Third, although subsequent use of fertility medications may have affected outcomes for the group of women with low AMH, data from women who initiated fertility treatments were censored. Fourth, the findings differ from the authors’ prior pilot study of 100 women, in which low AMH was associated with reduced odds of conception.10 Fifth, even though the overall sample size was relatively large, the number of women in the 38- to 44-year-old age group with low AMH was small (n = 28), and the loss to follow-up of women in this group to fertility treatment over the 12-month observation period may have influenced the results. Couples with infertility in whom the female partner is older than 35 years are strongly encouraged to seek treatment after no more than 6 months of attempting to conceive naturally.11 The detrimental relationship between age and fertility was evident in this study, but advanced reproductive age did not appear to be associated with having fewer follicles, as reflected by lower AMH levels. Sixth, the within-woman rate of decline in AMH over time is likely to be a useful factor to incorporate into prognostic models5; however, serial measurements were not obtained in this study. Most work to date has involved cross-sectional studies with single determinations of AMH. It is possible that a subset of women who have a more rapid rate of decline in AMH levels have a more adverse fertility profile. The findings reported by Steiner et al underscore the hazards of extrapolation and challenge clinicians to think more carefully about the biological meaning of AMH, which is a widely used biomarker. For practitioners who provide care for patients with infertility, AMH generally has been considered an important adjunct to the armamentarium of diagnostic tools. For clinical researchers who study menopause, AMH has appeared to represent the long-sought-after blood test to help a woman prospectively determine when her final menses will occur. It seems critical to distinguish the infertile population, who have already tried unsuccessfully to become pregnant and are therefore encountered in clinical practice, from the noninfertile population, which consists of a group of women (most of whom will not ever come to medical attention for infertility) who may have a number of reproductive advantages that mitigate a low AMH when interpreting these biomarkers. Circulating AMH may mean different things in each of these circumstances. The picture that seems to be emerging is that women with infertility related to both a reduction in oocyte/follicle quantity (which can be measured by low AMH or high FSH) and quality (which currently cannot be prospectively assessed by any factor other than age) have the worst prognosis and are the most challenging to treat. Even when follicle numbers are approaching critically low levels, when oocyte quality is preserved, natural conception appears to be relatively unaffected. The appropriate interpretation of an AMH level in the setting of a couple with infertility is presently a matter of scientific debate and controversy. Based on the findings reported by Steiner et al, women who have never attempted to conceive should not be evaluated in a manner similar to those with infertility. Doing so can not only provide potentially misleading and anxiety-producing results but may also lead to costly fertility preservation treatments that have no value. Three hypothetical curves are shown. Curve A represents the discriminatory capacity of the ideal test. The test or model correctly identifies all patients experiencing an event without misclassifying any patients who do not experience an event (AUC = 1.0). Curve B shows a more typical curve of a potentially useful test. The model correctly classifies more patients with events than misclassifies patients without events (AUC = 0.8). Curve C shows a test that it is no better than chance. The model has similar chances of correctly classifying patients with vs patients without events (AUC = 0.5). Point x on curve B represents a model with a high threshold for a positive result (ie, requiring a high-risk score before predicting the patient will have an event) and no false-positive results; however, only 30% of those destined to have the event will be correctly identified. Point y represents an example of a model with increased sensitivity but at the cost of more false-positive results (sensitivity of 75%, 20% are false-positive results). Point z represents a model with a high sensitivity (95%) but with limited ability to identify patients who will not experience an event (70% are false-positive results). The addition of CCTA to a risk prediction model results in an additive net reclassification index (NRI) of 11 and an absolute NRI of −8%. There is better classification of patients with events but worse reclassification of patients without events.33 The gray cells on the diagonal represent risk stratification that is unchanged by the addition of CCTA to the model. The pink cells represent patients incorrectly reclassified by the addition of CCTA, whereas the green cells represent patients correctly reclassified by CCTA. RCRI indicates Revised Cardiac Risk Index. Accurate information regarding prognosis is fundamental to optimal clinical care. The best approach to assess patient prognosis relies on prediction models that simultaneously consider a number of prognostic factors and provide an estimate of patients’ absolute risk of an event. Such prediction models should be characterized by adequately discriminating between patients who will have an event and those who will not and by adequate calibration ensuring accurate prediction of absolute risk. This Users’ Guide will help clinicians understand the available metrics for assessing discrimination, calibration, and the relative performance of different prediction models. This article complements existing Users’ Guides that address the development and validation of prediction models. Together, these guides will help clinicians to make optimal use of existing prediction models. You are a general internist seeing an ambulatory consult. This new patient is a 54-year-old male with a history of hypertension treated with calcium channel blockers. He smokes and has a sedentary lifestyle but has not had any previous cardiovascular events. What is the risk of a cardiovascular event for this patient? Recent laboratory results show normal levels of total cholesterol (198 mg/dL) and low-density lipoprotein cholesterol (138 mg/dL), but a decreased level of high-density lipoprotein cholesterol (39 mg/dL). On physical examination, his systolic and diastolic blood pressure is 130 mm Hg and 80 mm Hg, respectively. Based on this information and using an online tool (Pooled Cohort Equations [modified from the Framingham risk score, which is recommended by the American Heart Association]), you calculate his risk of a cardiovascular event (myocardial infarction, stroke, or death due to coronary artery disease) to be 12.4% at 10 years.1 Given this risk, current US, European, and Canadian guidelines recommend smoking cessation, regular physical activity, and initiation of statin therapy for primary prevention.2. When presented with the risk estimate, the patient is concerned, and although open to trying to stop smoking and exercise more, is reluctant to begin medication. Questioning the risk estimate, he points out that based on his family history of long survival without heart disease, the instrument has probably overestimated his risk. You are aware that another laboratory assay, N-terminal pro-B-type natriuretic peptide (NT-proBNP), may help to further differentiate patients at varying risk and consider whether this would help resolve the current dilemma. Before suggesting the test to the patient, you review the evidence regarding whether NT-proBNP will help to better categorize your patient’s risk. Quiz Ref IDAccurate prognostic information is vitally important for patients and physicians to make optimal health-related and life decisions. For example, if a patient is at low risk of a future adverse event, the absolute benefit offered by an effective therapy may be small in relation to the potential harm, burden, and cost. Among higher-risk patients, the same treatment may offer substantial benefits. Thus, accurate prognostic assessment assists patients and physicians in the shared decision-making process, preventing testing in low-risk situations, and avoiding delays in treatment when there is a high probability of a favorable net benefit. There are several ways to estimate a patient’s prognosis. First, it may sometimes be adequate for patients and clinicians to use their intuition. However, use of intuition is often limited; for example, patients with heart failure tend to overestimate their life expectancy,3 particularly if they are younger and more symptomatic. In contrast, primary care physicians overestimate mortality risk in patients with heart failure, particularly in patients who are stable and mildly symptomatic.4. Third, clinicians may rely on studies that address such prediction factors. However, interpretation of the relative effects from these prognostic studies depends on accurate estimates of baseline risk. Many prognostic studies report only relative measures of association such as a relative risk or hazard ratio. For example, a large US cohort study reported that among patients with an implantable cardioverter-defibrillator, receiving a shock was associated with a doubling of mortality.6 This study failed to report the absolute risk of dying. If the baseline 1-year mortality risk in patients who have an implantable cardioverter-defibrillator without shocks is 2%, receiving a shock increases the risk to 4% (an absolute increase of 2%), whereas if the baseline risk is 20%, receiving a shock increases the risk to 40% (an absolute increase of 20%). If physician-judged estimates of baseline risk differ substantially from the actual risk, the application of the relative effects associated with patient characteristics will be misleading. The Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD) statement16 provides guidelines for researchers reporting on studies presenting a new model or validating an existing model. Previous Users’ Guides17,18 have (1) reviewed the steps in the development and validation of prediction models (eFigure in the Supplement); (2) provided an approach for judging the credibility of the prediction models (eBox in Supplement); and (3) focused on the available study designs, issues in implementation of the designs, and the appropriate inferences given the design, implementation, and results. Among the issues the guides have addressed is the optimal conduct of the studies in which authors developed their models. Clinicians particularly interested in this issue can, in addition to previous Users’ Guides, find useful discussion elsewhere.19. Quiz Ref IDDiscrimination refers to how well the model differentiates those at higher risk of having an event from those at lower risk. Discrimination depends on the distribution of patient characteristics in the population in which the model is being used. A model could well discriminate patients with events from those without events in a heterogenous population with widely different values of predictors included in the model (eg, age, sex, laboratory values); however, the same model could fail to discriminate patients in a more homogenous population. An extreme example is a model including age as a predictor that may work well in a population with a flat age distribution between 20 and 90 years, but fail in another population in which all patients are between the ages of 50 and 60 years. Calibration may be excellent in some patients, but not as good in others. For example, a model can be accurate at estimating risk for individuals in the range 0% to 20%, but overestimates risks in individuals at higher risk. Such poor calibration among patients with higher risk may or may not be a problem. For instance, if the threshold for decision making is 10% (ie, an intervention is warranted if the risk is ≥10%), a model that overestimates risk by more than 20% would still be clinically applicable. The overestimates among patients at higher risk would be irrelevant. A useful model should have both satisfactory discrimination and calibration, but this example suggests that a superficial examination may provide a misleading inference about what is satisfactory. Measures of discrimination address the extent to which a model predicts a higher probability of having an event among patients who will vs those who will not have an event. There are a number ways to assess discrimination. For binary (eg, dead or alive) outcomes, discrimination is typically characterized using the receiver operating characteristic (ROC) curve or C statistic.20 After taking all possible pairs of patients and comparing the predicted probabilities, if the model cannot discriminate between which patients will have vs will not have an event, the C statistic (or ROC) is 0.5 (ie, no better than chance). If the model always produces a higher probability for patients having events vs those not having events, the C statistic is 1.0.21-23. Figure 1 depicts the hypothetical relationship between a model’s sensitivity and false-positive rate, which is called an ROC curve. This example illustrates the performance of a score derived from a regression model in which different weights (β coefficients) given to each predictor are added (ie, in a model with 3 predictors: predictor 1 × β1 + predictor 2 × β2 + predictor 3 × β3). Looking at Figure 1, consider first curve B. Using a high threshold of a test result (ie, requiring a high risk score before predicting the patient will have an event), there will be no false-positive results; however, only 30% of those destined to have the event will be correctly identified (point x on curve B in Figure 1). Choosing a lower threshold increases sensitivity, but at the cost of more false-positive results (point y, sensitivity of 75%, 20% are false-positive results). The cost of a high sensitivity with an even lower threshold limits the model’s ability to identify patients who will not experience an event (point z, sensitivity of 95%, 70% are false-positive results). The greater the area under the ROC curve (the less is given up in false-positive results as sensitivity increases), the better the prediction model. A perfect ROC curve would have an area under the curve of 100%. The model will accurately classify all patients with events as higher risk relative to those without events (curve A in Figure 1). An ROC curve no better than chance would have an area under the curve of 50%. The model would have equal probability of assigning patients with vs those without events to a risk stratum (curve C in Figure 1). For example, the European System for Cardiac Operative Risk Evaluation Score II (EuroSCORE II), a model to predict mortality associated with cardiac surgery, showed excellent discrimination (C static of 0.80) in a validation cohort.26 However, it substantially overestimated mortality in higher-risk patients. For instance, an individual undergoing elective surgery with a EuroSCORE II predicted mortality risk of 50% would have an actual risk of dying after surgery of 25%. This falsely high mortality risk may lead a patient to choose not to undergo surgery, whereas understanding the true risk may lead to the opposite decision. Thus, models need more than discriminatory power to function optimally, and clinicians need to know more than discrimination to judge their usefulness. Consider the following example. Based on registry data, patients can expect a survival rate of approximately 85% at 1 year after heart transplantation.5 Patients with advanced heart failure should have a predicted mortality of more than 15% prior to heart transplantation to provide a mortality benefit over the course of 1 year. The Meta-analysis Global Group in Chronic Heart Failure (MAGGIC) risk score has been developed to predict mortality. A validation study showed good overall discrimination using the MAGGIC risk score among patients with heart failure as assessed by the C statistic (0.77).29 However, among patients with an actual mortality of greater than 30%, the model underestimated mortality risk by 10% or greater (Figure 2). Some individuals in whom the model predicted mortality risk of 30% might still decide to continue with medical therapy and delay heart transplant (such individuals had an actual mortality of approximately 50%). Therefore, use of the model would lead some patients to inappropriately defer transplant. There are different ways of summarizing the extent of reclassification. One of the most widely used summary statistics has been labeled as the net reclassification index (NRI). Calculating the NRI requires adding the percentage of patients having an event correctly reclassified (No. with the event who have a greater risk in the new model compared with the old model − No. with the event and a lower risk in the new vs to old model/total No. of patients with the event) to the percentage of patients who do not experience an event who are correctly reclassified (No. without the event and who have a lower predicted risk in the new model relative to the old model − No. without the event and a greater risk in the new vs the old model/total No. of patients without the event). This is the most commonly used method and is called the additive NRI (Figure 3). The additive NRI can range from 200 (all patients with events had higher risk prediction and all patients without events had lower risk prediction with the new model) to −200 (the opposite) and its value does not represent a proportion, whereas the absolute NRI can range from −100% to 100% representing the proportion of patients incorrectly or correctly reclassified. The major limitation of the additive NRI is that it does not consider the prevalence of the events and nonevents in the population. Therefore, if a prediction model does better among patients who had an event in a sample in which the number of patients with events is small, but worse in a sample in which the number of patients without events is large, the results could be misleading. The absolute NRI avoids this problem.33 The following examples illustrate this advantage. In Figure 3B, the proportion of patients with and without the event is 9% and 91%, respectively. The amount of reclassification is the same as in Figure 3A. There is an identical additive NRI of 12, favoring the new model. The new model does considerably better in the patients with events, but these patients now represent a far smaller group. The model still incorrectly classifies patients without events, but these patients are now a much larger group. The absolute NRI results in −150/11 000 or −1.34% of patients. When the incidence of events is low, the additive NRI can be misleading, whereas the absolute NRI better reflects the model’s limitations. The use of NRI and the pitfalls associated with additive NRIs can be illustrated from the results of a study33 evaluating the benefit of adding the coronary computed tomographic angiography (CCTA) score to a perioperative score (Revised Cardiac Risk Index) that predicts the postoperative risk of myocardial infarction or cardiovascular death. In this study, 955 patients underwent noncardiac surgery and 74 patients (7.7%) experienced cardiovascular death or myocardial infarction.33Figure 4 shows the risk reclassification of patients with and without events using the Revised Cardiac Risk Index and also when CCTA was added. The addition of CCTA improved classification of patients with events; however, it worsened the classification of patients without events. The additive NRI of 11 suggests improved prediction when CCTA was added to the Revised Cardiac Risk Index score. Because there were more patients (92.3% vs 7.7%) without a myocardial infarction or cardiovascular death, the absolute NRI is −78 of 955 or approximately −8%. The overall reclassification by adding CCTA to the Revised Cardiac Risk Index worsened the risk prediction in 8% of the patients compared with using the Revised Cardiac Risk Index alone. For instance, consider the example in Figure 4. Of the 955 patients, 74 experienced myocardial infarction or cardiovascular death and 16 patients were better classified by the model including CCTA. Of the 881 who did not have an event, 94 were better classified without CCTA. For CCTA to be judged equally good overall, a correct risk classification in a patient destined to have a myocardial infarction or die would have to be judged approximately 6 times more important than misclassifying a patient destined not to experience an event. For CCTA to be judged superior, the deleterious consequences associated with a false-negative result would have to be more than 6 times as important as those associated with a false-positive result. The reclassification analysis further informs the value of the additional test. The additive NRI was 9.4; however, the additional use of NT-proBNP resulted in incorrect reclassification in approximately 146 patients without events (3.3%), which was the larger group of patients, and correct reclassification in approximately 40 patients (12.7%) who had an event. Thus, the absolute NRI was approximately −2.4%. These results demonstrate that the measurement of NT-proBNP will, relative to using only Pooled Cohort Equations, increase rather than decrease the absolute number of patients misclassified. This Figure shows the criteria for statistical significance that would be used in a hypothetical gatekeeping strategy in which there are 3 levels each with a single end point, followed by 2 levels with 2 end points each. The 3 end points are each tested in order against a criterion of .05. All testing stops as soon as 1 result is nonsignificant. If all are significant then a pair of fourth-level end points are tested, and to preserve the required significance of .05 at that level across 2 end points, the criterion for statistical significance is adjusted with a Bonferroni correction value of .025 for each. If 1 or both of these end points is significant at .025, then the next end point in the branch is tested, against a criterion of .025. If 1 or both are nonsignificant, no further testing occurs. If any outcome tested along a given pathway is not statistically significant, no further outcomes along that branch are tested because they are assumed to be nonsignificant. Clinical trials characterizing the effects of an experimental therapy rarely have only a single outcome of interest. In a previous report in JAMA,1 the CLEAN-TAVI investigators evaluated the benefits of a cerebral embolic protection device for stroke prevention during transcatheter aortic valve implantation. The primary end point was the reduction in the number of ischemic lesions observed 2 days after the procedure. The investigators were also interested in 16 secondary end points involving measurement of the number, volume, and timing of cerebral lesions in various brain regions. Statistically comparing a large number of outcomes using the usual significance threshold of .05 is likely to be misleading because there is a high risk of falsely concluding that a significant effect is present when none exists.2 If 17 comparisons are made when there is no true treatment effect, each comparison has a 5% chance of falsely concluding that an observed difference exists, leading to a 58% chance of falsely concluding at least 1 difference exists. The formula 1 −[1 −α]N can be used to calculate the chance of obtaining at least 1 falsely significant result, when there is no true underlying difference between the groups (in this case α is .05 and N is 17 for the number of tests). Many methods exist for conducting multiple comparisons while keeping the overall trial-level risk of a false-positive error at an acceptable level. The Bonferroni approach3 requires a more stringent criterion for statistical significance (a smaller P value) for each statistical test, but each is interpreted independently of the other comparisons. This approach is often considered to be too conservative, reducing the ability of the trial to detect true benefits when they exist.4 Other methods leverage additional knowledge about the trial design to allow only the comparisons of interest. In the Dunnett method for comparing multiple experimental drug doses against a single control, the number of comparisons is reduced by never comparing experimental drug doses against each other.5 Multiple comparison procedures, including the Hochberg procedure, have been discussed in a prior JAMA Guide to Statistics and Methods.2. A serial gatekeeping procedure controls the false-positive risk by requiring the multiple end points to be compared in a predefined sequence and stopping all further testing once a nonsignificant result is obtained. A given comparison might be considered positive if it were placed early in the sequence, but the same analysis would be considered negative if it were positioned in the sequence after a negative result. By restricting the pathways for obtaining a positive result, gatekeeping controls the risk of false-positive results but preserves greater power for the earlier, higher-priority end points. This approach works well to test a sequence of secondary end points as in the CLEAN-TAVI study or to test a series of branching secondary end points (Figure). As shown in the Figure, this approach can be extended to test 2 or more end points at the same step by using a Bonferroni adjustment to evenly split the false-positive error rate within the step. In that case, testing is continued until either all branches have obtained a first nonsignificant result or all end points have been tested. For example, a neuroimaging end point could be used as a single end point for the first level, reflecting the assumption that if an improvement in an imaging outcome is not achieved then an improvement in a patient-centered functional outcome is highly unlikely, followed by a split to allow the testing of motor functions on one branch and verbal functions on the other. This avoids the need to prioritize either motor or verbal function over the other and may increase the ability to demonstrate an improvement in either domain. Gatekeeping strategies are a powerful way to incorporate trial-specific clinical information to create prespecified ordering of hypotheses and mitigate the need to adjust for multiple comparisons at each stage of testing. The primary challenge in using gatekeeping is the need to prespecify and truly commit to the order of testing. The resulting limitation is that if, in retrospect, the order of outcome testing appears ill chosen (eg, if an early end point is negative and important end points later in the sequence appear to suggest large treatment effects), then there is no rigorous, post hoc method for statistically evaluating the later end points. This highlights the importance of having a clear data analysis strategy determined before the trial is started, and maintaining transparency (eg, publishing the study design and analysis plan on public websites or in journals). The CLEAN-TAVI investigators used a gatekeeping strategy to compare several magnetic resonance imaging end points along with neurological and neurocognitive performance.1 The first was the primary study end point, the number of brain lesions 2 days after TAVI. Secondary end points were only tested if the primary one was positive. Then, up to 16 secondary end points were tested in a defined sequence. The study was markedly positive, with the primary and many secondary end points demonstrating benefit. The first 8 comparisons were reported in detail in the publication—in their prespecified order—retaining the structure of the gatekeeping strategy.1. Major recommendations and ratings  (1) Annual foot inspections by physicians or advanced practice clinicians with training in foot care and education of patients and their families about preventive foot care are recommended for patients with diabetes (grade 1C). (2) Foot examinations in patients with diabetes should include testing for peripheral neuropathy using the Semmes-Weinstein test (grade 1B). (3) Annual assessment is recommended of pedal perfusion by ankle-brachial index (ABI), ankle and pedal Doppler arterial waveforms, and either toe systolic pressure or transcutaneous oxygen pressure (TcPo2) for patients with a current diabetic foot ulcer (DFU) (grade 1B). (4) Adequate glycemic control (hemoglobin A1C <7%) should be achieved to reduce DFUs and infections with subsequent risk of amputation (grade 2B). (5) Revascularization by surgical bypass or endovascular therapy is recommended for patients with DFU and peripheral arterial disease (PAD) (grade 1B). (6) Prophylactic arterial revascularization to prevent DFUs should not be done (grade 1C). Guideline development was sponsored by the SVS in collaboration with the American Podiatric Medical Association and Society for Vascular Medicine and used the GRADE framework. These organizations selected a multidisciplinary committee of vascular surgeons, podiatrists, and physicians with expertise in vascular and internal medicine to form the Diabetic Foot Practice Guidelines Committee. A guideline methodologist, a librarian, and a team of investigators with experience in conducting systemic review and meta-analysis assisted the committee. Five full systematic reviews and meta-analyses were published concomitant to the guideline. The committee used the evidence as well as unanimous expert consensus to formulate its recommendations. The final guidelines were peer reviewed by the SVS documents oversight committee. All committee members completed conflict of interest disclosures and more than 50% of the writing group was free of relevant conflicts; the chair overseeing the guideline development had no relevant conflicts3 (Table). Plenty of resources are available for medical practices that would like to take the next step—assessing patients with suspected difficulties. Three brief cognitive tests recommended in the GSA toolkit—the Mini-Cog, the General Practitioner Assessment of Cognition (GPCOG), and the Memory Impairment Screen—are free, require less than 5 minutes to complete, and can be administered by nurse practitioners or physician assistants. The toolkit advises that family members or friends should be asked to fill out a questionnaire about changes they’ve noticed, and patients should be asked to provide a self-assessment. Samples are provided for each questionnaire. Research suggests that subjective cognitive complaints can be a reliable indicator of objective cognitive impairment. Significantly more patients with cognitive concerns were detected in practices that had given older patients the test. On April 1 Premier Medical decided that all patients aged 75 years or older would receive Mini-Cog examinations at their Annual Medicare Wellness visits. Also, the medical group is requiring that all patients flagged with potential impairment receive a computed tomographic scan or undergo magnetic resonance imaging and a referral to a neurologist, pharmacist, and a social worker. So far, physicians report they’re finding the workload manageable and the assessments useful, Colangelo said. A recent study by Karlawish and colleagues suggests that the aftermath of a diagnosis can be difficult, underscoring the importance of referring patients to sources of support. The study found that patients often experience higher stress, greater depression, and lower quality of life after receiving a diagnosis of mild cognitive impairment or early-stage Alzheimer disease. “We’re not saying that we shouldn’t tell people about the diagnosis, but there’s certainly a need for careful follow-up,” said Shana D. Sites, PsyD, MS, MA, the lead researcher and a clinical psychologist at the Penn Memory Center at the University of Pennsylvania. It is still early in the administration’s term. The White House could yet bring in a strong science adviser and pull back from undermining scientific activities. But it is also possible that the actions on coal mining, climate science, and teen pregnancy are just the beginning. If so, long-standing efforts to improve reproductive health care, ensure access to vaccines, and reduce racial and ethnic health disparities may be especially vulnerable. Science’s defenders have identified five hallmark moves of pseudoscientists. They argue that the scientific consensus emerges from a conspiracy to suppress dissenting views. They produce fake experts, who have views contrary to established knowledge but do not actually have a credible scientific track record. They cherry-pick the data and papers that challenge the dominant view as a means of discrediting an entire field. They deploy false analogies and other logical fallacies. And they set impossible expectations of research: when scientists produce one level of certainty, the pseudoscientists insist they achieve another. It’s not that some of these approaches never provide valid arguments. Sometimes an analogy is useful, or higher levels of certainty are required. But when you see several or all of these tactics deployed, you know that you’re not dealing with a scientific claim anymore. Pseudoscience is the form of science without the substance. The CDC’s recent analysis shows that as heroin-related death rates more than quadrupled—from 0.13 per 100 000 population annually between 2006 and 2009 to 0.62 per 100 000 from 2010 to 2015—law enforcement seizures of heroin doubled during the same time frame. A more dramatic trend emerged with synthetic opioids. Deaths increased from an annual rate of 0.01 per 100 000 between 2006 and 2012 to 0.98 per 100 000 from 2013 to 2015. During the same period, law enforcement seizures of illegally manufactured fentanyl shot up by 200%. Of the pregnant women who were interviewed, 35% knew that mosquitoes transmit Zika and only 12% knew the virus can be sexually transmitted. Only 4% of the pregnant women knew that Zika virus transmission was occurring in the Islands. Some 14% said they thought it was likely they would be infected during their pregnancy. Among all the residents interviewed, fewer than 3% said they had heard about individual protections people could take to prevent infection. Incidence rate ratios and 95% CIs are presented to show the risk of severe hypoglycemia, hypoglycemic coma, diabetic ketoacidosis (pH <7.3), and severe ketoacidosis (pH <7.1) in patients using insulin pump therapy compared with the risk in patients using insulin injection therapy. Error bars indicate 95% CIs. A, Analysis in the propensity score–matched cohort including 9814 patients using injection therapy and 9814 patients using pump therapy. B, Analysis with propensity score inverse probability of treatment weighting of the entire cohort (16 460 patients using injection therapy, 14 119 patients using pump therapy). Estimates are derived from negative binomial regression analyses. This was a population-based cohort study comparing patients with type 1 diabetes mellitus who used insulin pump therapy and patients who used insulin injection therapy between January 1, 2011, and December 31, 2015. Patients included in the study were identified from the Diabetes Prospective Follow-up (DPV) Initiative database at the University of Ulm, Germany. As of December 31, 2015, 446 diabetes centers (hospitals and practices) in Germany, Austria, Luxembourg, and Switzerland have documented treatment and outcome of diabetes care using the DPV Diabetes Documentation System.1,16,17 Parameters collected in the DPV system have been described previously.17 Twice a year, locally collected longitudinal data are transmitted anonymously for central analysis, and inconsistent data are reported back to participating centers. The DPV database covers an estimated proportion of more than 80% of all pediatric patients with diabetes in Germany, Austria, and Luxembourg. Patients were eligible for inclusion in the study if they had a clinical diagnosis of type 1 diabetes and were treated with intensive insulin therapy administered by either pump or injection, defined as 4 or more insulin injections per day. Exclusion criteria were younger than 6 months at diagnosis; 20 years or older; diabetes duration less than 1 year; use of 3 or fewer daily insulin injections; and use of continuous glucose monitoring. All patients continuously used either pump therapy or injection therapy during the entire observation period of 12 months, thus excluding treatment crossover. For each patient, clinical data including HbA1c level, total daily insulin dose, prandial to total insulin ratio, frequency of self-monitoring of blood glucose level, and body mass index (BMI) (calculated as weight in kilograms divided by height in meters squared) of the most recent treatment year were aggregated as medians, and hypoglycemic and ketoacidosis events were summed and related to the individual time at risk, as described previously.16. Propensity score matching was used to ensure that both the pump therapy group and injection therapy group had similar baseline characteristics, because patients who are presented with the option of using pump therapy may have different baseline characteristics, affording them the opportunity to use this technology. Propensity score for pump therapy was estimated applying a multivariable logistic regression model, with age, sex, duration of diabetes, migration background, BMI, and HbA1c level as covariates. Migration background was defined as birthplace outside of Germany or Austria for the patient or of 1 or both parents. For each patient, the probability (propensity score) for pump therapy was estimated from the logistic model based on the patient’s specific covariate values. Matching was conducted with a one-to-one matching process (greedy-matching algorithm).18,19 Standardized differences were assessed to evaluate balancing of covariates between treatment groups. A standardized difference of less than 10% for a baseline covariate reveals a negligible imbalance.18 Treatment effects were estimated by directly comparing outcomes between an equal number of pump-treated and injection-treated individuals with the same propensity score (matched cohort). Event rates of severe hypoglycemia, hypoglycemic coma, diabetic ketoacidosis, and severe ketoacidosis were evaluated in pump therapy and injection therapy by negative binomial regression analyses including matched pairs (in the matched cohort) or treatment center (in the entire cohort) as a random factor. Individuals with no available information on severe hypoglycemia or coma events were not included in these regression analyses. In additional analyses, event rates of severe hypoglycemia, coma, diabetic ketoacidosis, and severe ketoacidosis were estimated by age groups from negative binomial regression models including a therapy × age group interaction term in the matched cohort. Age groups were defined as 1.5 to 5 years; 6 to 10 years; 11 to 15 years; or 16 to 19 years. Of the 446 diabetes centers, 350 treated 30 579 individuals with type 1 diabetes (mean age, 14.1 years [SD, 4.0]; 53% male) meeting the inclusion criteria (Figure 1), with a mean number of 4.8 visits (SD, 2.5) per patient during the most recent treatment year. Among the treated patients, 14 119 used insulin pump therapy, with a median duration of 3.7 years; 16 460 used multiple (≥4) daily insulin injections, with a median duration of 3.6 years. In the propensity score–matched cohort, 9814 patients using insulin pump therapy were matched with 9814 patients using injection therapy from 328 diabetes centers. In this matched cohort the standardized differences were 1.8% or less for all baseline characteristics, demonstrating only minor differences between both treatment groups (Table 1). The median duration of insulin pump therapy was 3.6 years and of insulin injection therapy was 4.4 years in the matched cohort. Since 10 951 individuals were lost during the matching process, we additionally conducted an analysis for the entire cohort with inverse probability of treatment weighting using propensity scores (Figure 1). Event rates for severe hypoglycemia were significantly lower with pump therapy compared with injection therapy (9.55 vs 13.97 per 100 patient-years; difference per 100 patient-years, −4.42 [95% CI, −6.15 to −2.69]; IRR, 0.68 [95% CI, 0.59 to 0.79]) (Table 2, Figure 2). Event rates for hypoglycemic coma were also significantly lower with pump therapy compared with injection therapy (2.30 vs 2.96 per 100 patient-years; difference per 100 patient-years, −0.66 [95% CI, −1.24 to −0.08]; IRR, 0.78 [95% CI, 0.62 to 0.97]) (Table 2, Figure 2). These differences remained significant after adjusting for multiple comparisons (P < .001 for severe hypoglycemia, P = .03 for hypoglycemic coma). Age-group analyses showed significantly lower rates of severe hypoglycemia with pump therapy vs injection therapy in all age groups except for preschool children aged 1.5 to 5 years (eFigure 1A in the Supplement). Significantly lower rates of hypoglycemic coma with pump therapy compared with injection therapy were observed in children aged 6 to 10 years and 11 to 15 years but not in other age groups (eFigure 1B in the Supplement). In the entire cohort, 3572 episodes of severe hypoglycemia in 1875 patients (6.1%), including 786 episodes of coma in 622 patients (2.0%), were documented at 146 919 visits during the most recent treatment year. Data on these events were available for 94% of individuals. Event rates for severe hypoglycemia were significantly lower with pump therapy compared with injection therapy (10.30 vs 15.53 per 100 patient-years; difference per 100 patient-years, −5.23 [95% CI, −6.93 to −3.53]; IRR, 0.66 [95% CI, 0.59 to 0.75]) (Table 2, Figure 2). Event rates for hypoglycemic coma were also significantly lower with pump therapy compared with injection therapy (2.26 vs 3.43 per 100 patient-years; difference per 100 patient-years, −1.16 [95% CI, –1.72 to –0.60]; IRR, 0.66 [95% CI, 0.55 to 0.80]) (Table 2, Figure 2). These differences remained significant after adjusting for multiple comparisons (P < .001 for severe hypoglycemia, P < .001 for hypoglycemic coma). In the matched cohort, a total of 842 events of diabetic ketoacidosis in 719 patients (3.7% of patients), including 542 events of severe ketoacidosis (pH <7.1) in 476 patients (2.4%), were noted during the most recent treatment year. Compared with injection therapy, pump therapy was associated with significantly lower event rates for ketoacidosis (3.64 vs 4.26 per 100 patient-years; difference per 100 patient-years, −0.63 [95% CI, −1.24 to −0.02]; IRR, 0.85 [95% CI, 0.73 to 0.995]) (Table 2, Figure 2). Event rates for severe ketoacidosis were significantly lower with pump therapy than with injection therapy (2.29 vs 2.80 per 100 patient-years; difference per 100 patient-years, −0.50 [95% CI, −0.99 to −0.02]; IRR, 0.82 [95% CI, 0.68 to 0.99]) (Table 2, Figure 2). These differences remained significant after adjusting for multiple comparisons (P = .048 for diabetic ketoacidosis, P = .048 for severe ketoacidosis). Age-group analyses showed significantly lower rates of diabetic ketoacidosis and severe ketoacidosis with pump therapy vs injection therapy in adolescents and young adults aged 16 to 19 years but not in other age groups (eFigure 1C and eFigure 1D in the Supplement). In the entire cohort, 1419 episodes of ketoacidosis in 1198 patients (3.9%), including 922 episodes of severe ketoacidosis in 792 patients (2.6%), were reported during the most recent treatment year. Compared with injection therapy, pump therapy was associated with significantly lower event rates for ketoacidosis (4.66 vs 6.94 per 100 patient-years; difference per 100 patient-years, −2.29 [95% CI, −3.12 to −1.46]; IRR, 0.67 [95% CI, 0.59 to 0.76]) (Table 2, Figure 2). Event rates for severe ketoacidosis were significantly lower with pump therapy than with injection therapy (3.17 vs 5.17 per 100 patient-years; difference per 100 patient-years, −2.00 [95% CI, −2.79 to −1.21]; IRR, 0.61 [95% CI, 0.52 to 0.72]) (Table 2, Figure 2). These differences remained significant after adjusting for multiple comparisons (P < .001 for diabetic ketoacidosis, P < .001 for severe ketoacidosis). Total daily insulin dose was lower and prandial to total insulin ratio was higher in pump therapy compared with injection therapy (Table 3), significant for all age groups of the matched cohort (P < .001 for all) (eFigure 2A and 2B in the Supplement). Rapid-acting insulin analogues were used in 96% of patients with pump therapy and 74% of patients with injection therapy (Table 3). The more frequent use of rapid-acting insulin analogues with pump therapy was observed in all age groups of the matched cohort (P < .001 for all) (eFigure 2C in the Supplement). Individuals with injection therapy used long-acting insulin analogues in 80% (matched cohort) and 77% (entire cohort), respectively. In this contemporary cohort of young patients with type 1 diabetes, the risk of severe hypoglycemia and diabetic ketoacidosis associated with insulin pump therapy was lower than that associated with insulin injection therapy. Pump therapy was associated with a lower rate of severe hypoglycemia and of hypoglycemic coma compared with injection therapy, particularly in school-aged children. Similarly, pump therapy was associated with a lower rate of diabetic ketoacidosis and severe ketoacidosis vs injection therapy, especially in adolescents and young adults. These results favor pump therapy, with lower rates of acute complications and, at the same time, lower HbA1c levels reflecting improved metabolic control. There was no difference in BMI between treatment regimens. The strengths of the present study include the large sample size of a population-based cohort of more than 30 000 patients with type 1 diabetes, with stringent prospective data collection and a nationwide capture rate of more than 80% of pediatric patients in Germany, Austria, and Luxembourg. Using robust statistical methodology including a matched pair approach, a direct comparison of hypoglycemia and ketoacidosis frequencies in pump users and injection users was performed. Sample size and data collection at the time of adverse event allowed for further categorizing the severity of hypoglycemia and ketoacidosis, consistently showing lower event rates with pump therapy. Whereas previous randomized clinical trials have been too small to assess the risk of these short-term diabetes complications, this study provides outcome data in clinical use that are likely representative of patients with type 1 diabetes across the pediatric age spectrum and with a disease duration longer than 1 year. This study has several limitations. This was a nonrandomized, observational study and thus was prone to residual selection bias despite effective propensity score matching. Intensity of diabetes education, motivation, family support, and mental health factors were not addressed, all relevant to hypoglycemia and ketoacidosis risk15,31-33 but difficult to measure quantitatively in a large population. Another potential limitation is that the individual duration of insulin pump use was not considered in the analyses, and a patient adopting this technology might have a higher frequency of short-term complications. In addition, the use of continuous glucose monitoring, which has been shown to improve glycemic control and reduce HbA1c levels and hypoglycemic events,12,15,26 was not analyzed in this study. Moreover, the treatment discontinuation rate for insulin pump therapy was not examined in the present study, but previous studies in the DPV population have shown a low discontinuation rate of only 4%.34. The ambient noise of Match Day celebration fills a brightly lit room at Albany Medical College. Amidst the laughter and cheers, a young woman with an espresso-colored complexion is tucked alone in the shadows with her back pressed against the cinder block wall. Her hand is trembling as she studies the lone piece of paper in her hand. Sweeping back her long, dark braids as they fall before her eyes, she lets it all sink in. A smile erupts across her face and then she closes her eyes. Patting tears with a tattered piece of tissue, she nods slowly and exhales. Antonia Francis, a fourth-year medical student and woman of color, just took one pivotal step closer to becoming Dr Antonia Francis, obstetrician/gynecologist. This is a big deal. Francis is just one of several accomplished African American women chronicled in the independent feature film Black Women in Medicine, written and directed by filmmaker Crystal R. Emery. Emery herself has navigated health care systems for much of her life, managing two chronic diseases as a quadriplegic. She has become a leading voice on the intersection between race, gender, and disability, using what she has gleaned through personal experience to create a movement to help increase the number of black physicians in the United States from 4.5% in 2016 to 7% by 2030. We meet Dr Claudia Thomas, the first African American female orthopedic surgeon, and learn that she was the lone woman of color in her medical school class at Johns Hopkins in the 1970s. We are introduced to Dr Velma Scantlebury, the first African American woman transplant surgeon, who was without a single minority female counterpart during her surgical training. These firsts might suggest that the narrative of African American women in medicine began in the last 50 years. But we also learn about Dr Rebecca Lee Crumpler, the first black woman to graduate from a US medical school at New England Female Medical College in 1864. Thomas and Scantlebury are no doubt trailblazers, but Crumpler’s 19th-century story, which Emery brilliantly folds into this documentary, makes clear that the history of black women in US medicine started well before the modern civil rights era. As a black female academic internist for the past 20 years, I relate to the experiences described by the women in Emery’s film, which resonate with those in my own life and echo what I hear day-to-day from the minority students and residents with whom I work. In 2013, one of my medical student advisees entered the room of a patient while on her surgical rotation. Dressed identically to other team members in hospital-issue scrubs and a white coat, the only thing that set her apart was her brown skin, Afrocentric hairstyle, and sex. When the team approached the bedside to view a postoperative surgical wound, the patient sat up in bed, looked straight at my student, and nudged her finished lunch tray on the rolling table toward her. “I didn’t have much of an appetite. You can take this tray away, honey,” she said directly to my student as the team looked on. And though the student described the patient as appearing neither rude nor contentious, it was clear that, even in 2013, this young black woman being a medical student or physician wasn’t even a consideration for this patient. After an awkward pause, no one made the correction. The student finally spoke. “I’m not from food services,” she whispered. “I’m a medical student on the team taking care of you.” I can still hear her sobbing into her cell phone while telling me about it in a lonely stairwell. One of the most powerful takeaway points of this movie is that there is still work to be done. “We all must challenge the status quo by replacing the false and debasing historical narrative regarding race, ethnicity, and gender with positive, empowering images of real women making a difference,” Emery has said. “My goal with Black Women in Medicine is to illuminate the issues and inspire a new generation of women of color to become doctors, as well as to help build a legacy for increasing access to health care in minority communities across the United States.” Diversity recruitment and retention in medical schools is a critical step to achieve this goal. Early exposure of minority youth to African American women physicians through structured programs is another potential mechanism to show a new generation that this career goal is attainable. With these ideas in mind, Emery joined with others to launch Changing the Face of STEM, a national initiative designed to grow the number of black scientists, engineers, and physicians in the United States. These inspiring true stories should be shown not only to underrepresented minorities in environments low on physician role models but also to broader audiences. Hearing the testimonies of black women physicians ranging from the newly minted to the very seasoned normalizes their accomplishments and honors their place in history. Their unique paths underscore that there isn’t a single story for any black woman in medicine. From the child of West Indian immigrants attending an Ivy League medical school, to the teen mother who beat the odds to become an anesthesiologist, to the young girl turned surgeon inspired by her penchant for sewing clothes, these women are as different as they are accomplished. That’s important for anyone to see—regardless of race or socioeconomic level. (City of Seattle vs. Hewetson (Wash.), 164 Pac. R. 234). The court says that the evidence showed that the defendant had an office in the back part of a room occupied by a drug store. That the office could be entered by a door from the portion of the building used for the drug store. That on the evening of Feb. 19, 1916, the complaining witness went to the defendant for the purpose of getting a prescription for liquor. That he entered through the drug store, found seven or eight persons standing in line, waiting for similar prescriptions, and, when his turn came, said to the defendant that he had a cold, or a bad cold. That thereupon the defendant inquired of him if he would like a little stimulant, to which he replied, “Yes.” That before receiving the prescription he was required to sign a statement as follows: “I, the undersigned, do declare that the prescription written for me by Dr. J. W. Hewetson for intoxicating liquor, on this date is for medical purposes; that I am sick and in need of medicine, and will take the same according to directions. Dated this 19th day of February, 1916. M. W. Palmer, 135 N. 75th St.” That there was no examination of the complaining witness as to his physical condition. The evidence further showed that the record book and the prescription file at the drug store disclosed that, February 19, 164 prescriptions for liquor had been filled; February 18, 103; February 17, 105, and February 16, 83—and that most of these prescriptions had been written by the defendant. It was admitted, on the trial, that any one “could get a prescription for intoxicating liquor unless he refused to sign one of these statements,” and that the defendant had “for the past two years written prescriptions free of charge.” There was also some evidence as to the number of persons standing in line, waiting to have prescriptions written, on other occasions than the one above referred to. Under this evidence, and other details that appeared in the testimony, the question whether the defendant had ground to believe that the person to whom he issued the prescription was actually sick, or that liquor was required as medicine, was for the determination of the jury. It could not be held, as a matter of law, on such evidence, that the plaintiff had failed to prove that the defendant, when he issued the prescription for which he was being tried, did not have good reason to believe that the person to whom it was issued was actually sick, and that the liquor was required as medicine. The real issue in such a case is whether the prescription was given in good faith, and, as bearing on this question, the number of prescriptions given by the accused, within a specified time, for intoxicating liquor, to various persons, as found on the file of the druggist, in whose store the defendant kept his office, was competent. The sections of the city ordinance which the defendant was charged with violating were regulative in their nature, and, in passing such an ordinance, the city acted within its power. There are 2 major categories of rhinoplasty: aesthetic and functional. Aesthetic rhinoplasty involves changing the outward appearance of the nose for cosmetic reasons. The goal of functional rhinoplasty is to improve the nasal airway to decrease obstruction and improve breathing. Rhinoplasty can also entail a combination of aesthetic and functional techniques. Ultimately, functional and aesthetic considerations come into play for all rhinoplasty procedures. Incision: Two approaches are primarily used for rhinoplasty. The most common is the external or open approach, whereby a small external incision on the columella (the bridge of skin between the nostrils) is made in continuity with incisions on the inside of the nose. The endonasal rhinoplasty technique is done with extensive internal incisions. With either technique, the incisions allow access to the bony-cartilaginous framework of the nose. Correction of nasal obstruction: In functional rhinoplasty, there are 3 primary components of the nose that are evaluated for repair: the nasal septum, the inferior turbinates, and the valves. If there is deviation of the septum (a structure that divides the 2 sides of the nose internally), this is repaired by removing or reshaping the deviated component. The inferior turbinates, when enlarged, can obstruct the airway and thus may require surgical reduction. The nasal valves refer to the narrowest point in the entrance to the nasal cavity and can be augmented in a variety of ways to increase the size of the opening and prevent collapse during inhalation. Nasal reshaping and resizing: A variety of procedures can be performed to obtain desired shape and size of the nose. Some of the most common aesthetic concerns are a dorsal hump, a wide tip, or a crooked nose. Dorsal hump reduction involves removing both bone and cartilage from the roof of the nose to create a more aesthetically pleasing profile. Tip reduction can involve removal of cartilage in the nasal tip and use of sutures to reduce the size of the nasal tip cartilage. Straightening a crooked nose may involve any of a variety of techniques, including osteotomy (breaking the nasal bones) to create more symmetry on frontal view. Second, individuals were classified as having elevated or normal brain amyloid levels by either positron emission tomography (PET) or CSF assay of amyloid,1 and individuals in each group may have been included through different measurement methods. There were more individuals evaluated via an amyloid PET scan than a CSF assay in the normal brain amyloid group (49 vs 35), compared with the elevated amyloid group (47 vs 64). Because CSF assay of amyloid has been demonstrated to detect its accumulation earlier than PET scan,4 a false-negative result might exist for individuals in the normal brain amyloid group with PET imaging alone. Therefore, it is important to control the ratio of different amyloid detection methods in both groups to avoid confounding bias. For any test to be effective, a physician must act on its results. In the group with less than a 1% pretest probability of injury, 28% of patients received CT after the FAST examination. This disconnect between the FAST interpretation and patient management may have been due to institutional practices. This explanation is suggested by the baseline CT rate at the study site of 60%. This rate is higher than rates in multicenter studies2,3 and is surprising given that only stable patients were included. Trauma evaluations are often protocolized, with CT being an important component. Many trauma centers engage in the practice of “pan scanning.” The majority of physicians in the study also cared for adult patients, who are managed differently than children. Previous work has demonstrated that CT use for injured children at pediatric hospitals is significantly less than at nonpediatric centers.4. AMH indicates antimüllerian hormone; FSH, follicle-stimulating hormone. 95% CIs are shown as shading. Model adjusted for age, body mass index, race, current smoking status, history of pregnancy, and hormonal contraceptive use in the preceding year. The median number of cycles each woman contributed was 4 (interquartile range, 2-6). The ability of these biomarkers to predict reproductive potential is uncertain. Antimüllerian hormone has been associated with time to menopause in a number of cohorts.6,7 Among women with infertility undergoing controlled ovarian hyperstimulation for in vitro fertilization, AMH is an excellent predictor of oocyte yield.8 Studies on the ability of these biomarkers to predict which women will conceive with in vitro fertilization have had inconsistent findings.9-11 Despite lack of evidence of their utility, biomarkers of ovarian reserve are being used as markers of reproductive potential or fertility tests. Home fertility tests based on day 3 urinary FSH levels are commercially available. Additionally, clinicians use these tests when counseling about elective oocyte cryopreservation. The Time to Conceive study, a prospective time-to-pregnancy cohort study, was conducted from April 2008 to March 2016 (date of last follow-up). Women were eligible to participate if they were between 30 and 44 years of age, had been attempting to conceive for 3 months or less, and were cohabitating with a male partner. Women were excluded if they had known fertility problems (history of sterilization, diagnosis of polycystic ovarian syndrome, previous or current use of fertility treatments, known tubal blockage, surgically diagnosed endometriosis) or a partner with a history of infertility. Women who were currently breastfeeding or had used injectable hormonal contraception in the preceding year were also excluded. This study was approved by the institutional review board of the University of North Carolina; all participants provided written informed consent. Women were recruited through flyers in the community, radio and print ads, informational letters, and mass emails. They were screened for eligibility by telephone using a standardized questionnaire. Women who met eligibility criteria completed a questionnaire including demographics and information on factors potentially related to fertility. To characterize the study population, this questionnaire included a question about race. Women self-selected their race from categories provided. Women were instructed to contact the study coordinator with their subsequent menses. They were scheduled for a study visit on cycle day 2, 3, or 4 of their menstrual cycle. Women were mailed a urine collection kit and instructed to collect a first-morning urine sample on the day of their study visit. At that visit, a blood sample and urine sample (if not collected at home) were obtained from the participant. Women were provided with home urine pregnancy tests (sensitivity: 20 mIU/mL human chorionic gonadotropin). For the first 3 years of the study, women were instructed to perform the pregnancy test with missed menses; subsequently, women were instructed to test starting on menstrual cycle day 28 and every 3 days thereafter. While attempting to conceive, women completed a daily diary in which they recorded bleeding, intercourse, medications, and results of pregnancy tests. Women completed these diaries for up to 4 months and then subsequently completed monthly questionnaires. Initial versions of the questionnaires were on paper and later versions were web-based. Women were instructed to contact study personnel if they tested positive for pregnancy. They were provided a free pregnancy ultrasound between 6 and 8 weeks’ gestation to encourage communication of results. Women were initially followed up for up to 6 months, but the protocol was subsequently modified in March 2010 to follow up with all women for up to 12 months of pregnancy attempt. Women were withdrawn from the study at initiation of fertility medication, on request (most commonly because they were moving or stopped trying to conceive), or when lost to follow-up. Serum samples were stored at −30°C until analysis. Samples were shipped frozen in a single batch to the University of Southern California Reproductive Endocrinology Laboratory. There, they were assayed using sensitive and specific assays for FSH (Immulite analyzer, Siemens), inhibin B (enzyme-linked immunosorbent assay [ELISA], Ansh Laboratories), and AMH (ultrasensitive AMH ELISA, Ansh Laboratories; lower limit of detection, 0.078 ng/mL). Interassay coefficients of variation ranged from 4% to 5% for FSH, 5% to 8% for inhibin B, and 9% to 11% for AMH. Urine samples were stored and shipped frozen to the National Institute for Occupational Safety and Health Reproductive Endocrinology Laboratory, Cincinnati, Ohio. There, they were assayed for FSH and creatinine as described previously.12 To adjust for urine flow rate, urinary FSH values were divided by the respective creatinine concentration. Results are presented as milli–international units of FSH per milligram of creatinine. Intra-assay coefficients of variation were 3.5% for FSH and 1.5% for creatinine. The biomarkers of ovarian reserve were considered as categorical variables where informed choices for cut points were available. It was hypothesized that the relationship between AMH and fertility would be nonlinear. After exploring clinical AMH cutoff values of 0.4 ng/mL, 0.7 ng/mL, and 1.0 ng/mL, the middle cutoff value of 0.7 ng/mL was selected based on previous research.13 The 90th percentile was selected as the upper-level AMH cutoff value (8.5 ng/mL). The clinical value of 10 mIU/mL was selected a priori as the serum FSH cutoff value.14 For urine, the corresponding FSH value is 11.5 mIU/mg creatinine, as documented previously.13 Inhibin B was modeled as a continuous variable because no clinical cutoff values were available. Nonparametric bivariable analyses were used to compare median biomarker levels by participant characteristics. Because women did not all enter at the same point during their attempts to conceive and some women withdrew, started fertility medications, or were lost to follow-up, the cohort was analyzed using a discrete-time Cox proportional hazards model. Time was menstrual cycles at risk of pregnancy (pregnancy attempt cycle). Pregnancy attempt cycle was determined from the time a woman started trying to conceive, not from the time of enrollment. Attempt cycle at enrollment was defined by the pregnancy attempt cycle (usually cycle 1, 2, or 3) in which the woman began participation (completed diaries or baseline questionnaire). Women were censored at the time they withdrew, started fertility medications, or were lost to follow-up. Thus, cycles from enrollment to censoring were included in the analysis. Because time in these models is measured by menstrual cycles (and not chronologic time) the hazard ratios (HRs) are referred to as fecundability ratios, which are the relative probability of pregnancy in a given cycle for the exposed group relative to the reference group.15 In such models an HR of less than 1 suggests reduced fecundability in the exposed (or nonreferent) group. The Cox proportional hazard models were then used to calculate the cumulative probability of conceiving (with 95% confidence intervals) at 6 and 12 cycles of attempt for each biomarker level. All models were adjusted for age (3 categories: <35, 35-37, or 38-44 years),16 body mass index (4 categories: <18.5, 18.5-24.9, 25-29.9, or ≥30; calculated as weight in kilograms divided by height in meters squared),17 race (white or nonwhite), current smoking status (yes or no), and hormonal contraceptive use in the preceding year (yes or no). Adjusted Kaplan-Meier curves with 95% confidence intervals were also constructed. The predicted probabilities and Kaplan-Meier curves were calculated by setting all of the covariates to the mean of the cohort. Planned subgroup analyses were conducted by age and parity. To test for interaction by age and parity, a likelihood ratio test was used to compare the fit for the model without the interaction term with that of the model including the interaction term. In addition, post hoc sensitivity analyses were conducted by creating additional Cox models to assess different cutoff values and to evaluate potential biases. A sample size of 750 women was selected based on an a priori power analysis. A 10% loss to follow-up, 70% pregnancy rate in the control group, 57% pregnancy rate by 6 months in the diminished ovarian reserve group, and 80% power at a type I error rate of .05 was conservatively presumed based on the pilot study.13 SAS version 9.3 (SAS Institute Inc) and R version 3.3.0 (R Project) were used for statistical analysis. All testing was 2-sided. P<.05 was considered statistically significant; there was no adjustment for multiple comparisons. Study flow is presented in Figure 1. Nine hundred eighty-one women were enrolled; 770 of these women had a study visit; 750 women were ultimately included in the analysis. Of these, 37 (5%) withdrew, 47 (6%) started fertility medications, 56 (7%) were lost to follow-up, 487 (65%) conceived, and 123 (17%) completed the study but did not conceive. Of the analyzed cohort, 69% of participants were aged 30 to 34 years, 19% were aged 35 to 37 years, and 12% were aged 38 years or older. Most participants were white (77%) and highly educated (62% with a graduate degree). The majority of women had a normal body mass index (62%), while 3% were underweight and 36% were overweight or obese. Cox analysis showed that the probability of conception was 65% by 6 cycles of attempt and 77% by 12 cycles of attempt. Fecundability over each attempt cycle is presented in Table 1. The distributional statistics for the observed biomarkers of ovarian reserve are as follows. Serum AMH, inhibin B, and FSH values were missing for 13 study participants (2%), who were excluded from AMH, inhibin B, and FSH analyses accordingly. Urinary FSH values were missing for 21 participants (3%), who were excluded from the analyses of urinary FSH. Each participant had at least 1 biomarker value recorded. Eleven percent of women had an AMH value of 0.7 ng/mL or less; by design, 10% had an AMH value of 8.5 ng/mL or higher. Eleven percent had a serum FSH value of 10 mIU/mL or higher; 9% had a urine creatinine-corrected FSH value of 11.5 mIU/mg creatinine or higher. The median value for inhibin B was 70 (interquartile range, 38-102) pg/mL. Table 2 presents the unadjusted median values of each biomarker (with interquartile ranges) by participant characteristics. As expected, AMH levels decreased and urinary FSH values increased with age. Compared with nonobese women, obese women had lower AMH (P = .007) and inhibin B (P = .005) values. Biomarker values did not significantly differ by education level, race, smoking status, hormonal contraceptive use in the preceding year, or cycle of pregnancy attempt in which the study participant was enrolled. Women who had previously been pregnant had significantly lower AMH values and higher urinary FSH values in this unadjusted analysis. The predicted probability of conceiving by 6 cycles or 12 cycles of attempt, as calculated from the Cox models, was not lower for women with low AMH or high FSH, as had been hypothesized (Table 3). Women with low AMH values (<0.7 ng/mL) did not have a significantly different cumulative probability of conceiving by 6 cycles of attempt (65%; 95% CI, 50%-75%) compared with women with normal values (62%; 95% CI, 57%-66%) or by 12 cycles of attempt (84% [95% CI, 70%-91%] vs 75% [95% CI, 70%-79%], respectively). Women with high serum FSH values (>10 mIU/mL) did not have a significantly different cumulative probability of conceiving after 6 cycles of attempt (63%; 95% CI, 50%-73%) compared with women with normal values (62%; 95% CI, 57%-66%) or after 12 cycles of attempt (82% [95% CI, 70%-89%] vs 75% [95% CI, 70%-78%]). Kaplan-Meier curves comparing adjusted cumulative probabilities of conception by categories of ovarian reserve biomarkers are presented in Figure 2. Although the curves suggest longer times to pregnancy in women with higher AMH values and for those with lower FSH values, confidence intervals overlap for both biomarkers. High and normal urinary FSH value curves are almost indistinguishable from one another. Planned subgroup analyses by age and parity were conducted (Table 4). In every age group, low AMH was not associated with diminished fecundability. Point estimates suggested higher fecundability among women with low AMH at any age. The relationship between high AMH and fecundability appeared to differ by a woman’s age. In younger women, high AMH suggested reduced fecundability. However, among older women, high AMH suggested higher fecundability. Although these point estimates differed, the confidence intervals overlap and the age interaction was not found to be statistically significant (P = .35). Subsequent subgroup analysis by pregnancy history also did not reveal significant effect modification by pregnancy history (Table 4). In sensitivity analyses, different cutoff values for AMH were examined. Women with AMH values of 0.4 ng/mL or lower had an HR of 1.40 (95% CI, 0.95-2.07) compared with women with AMH values between 0.4 ng/mL and 5.0 ng/mL. Women with AMH values of 1.0 ng/ml or lower had an HR of 1.16 (95% CI, 0.89-1.50) compared with women with AMH values between 1.0 ng/mL and 5.0 ng/mL. Also after adjusting for hormonal contraception use in the preceding 3 months (information available for 552 women), the HRs were further from the null but remained statistically nonsignificant (for low AMH: HR, 1.30 [95% CI, 0.92-1.86]; for high AMH: HR, 0.75 [95% CI, 0.51-1.10]). Restricting the analysis to women who entered into the study at cycles 1, 2, or 3 of pregnancy attempt did not change the findings. In an earlier, small pilot study (n=100 women), we found that low AMH (≤0.7 ng/mL) as measured using the Diagnostic Systems Laboratory assay was associated with a 60% reduction in the day-specific probability of conception.13 Those findings are different from current findings in this larger cohort, most likely because of sample size. Also, the pilot study used a day-specific probability analysis. This method uses information on intercourse patterns around the time of ovulation that relied on a calendar method that could have led to misclassification. Additionally, a different AMH assay was used. There is some evidence that the Ansh assay results in higher values compared with other assays.18 However, a sensitivity analyses with the higher (AMH ≤1.0 ng/mL) and lower (AMH ≤0.4 ng/mL) cutoff values was conducted, and there was still no evidence of reduced fecundability in either of the low AMH groups. Given that prior studies were small or based on secondary analyses, the finding in the current study that women with diminished ovarian reserve did not have reduced fertility was surprising and contrary to the hypothesis. Although both ovarian reserve and fertility decline with chronological age when looking at cross-sectional data, there may be little association between a given woman’s ovarian reserve and factors that affect her fertility, such as egg quality. Antimüllerian hormone and FSH levels may, however, affect follicular recruitment in those with diminished ovarian reserve. It is possible that low AMH allows for a greater proportion of the remaining primordial follicle pool to activate and become growing follicles. Additionally, high FSH seen in women with low reserve could lead to “superovulation” with multifollicular ovulation, increasing the odds of pregnancy. It has previously been shown that women of advanced maternal age are at higher risk of dizygotic twins.22. It was hypothesized that younger women with diminished ovarian reserve might not have decreased fertility but that older women would. However, the exploratory subgroup analysis did not support this. Neither the younger (30-35 years of age) nor the older women with diminished ovarian reserve (as measured by AMH) showed reduced fecundability. However, high AMH was nonsignificantly associated with reduced fecundability in the younger women and increased fecundability in the older women. Hagen et al similarly found that young women with high AMH levels had reduced fecundability (HR, 0.62; 95% CI, 0.39-0.99).19 Antimüllerian hormone is not only a marker of ovarian reserve but also a potential marker for polycystic ovarian syndrome. While the appropriate AMH cutoff value for polycystic ovarian syndrome is debated, multiple studies have shown that AMH is elevated in women with polycystic ovarian syndrome.18,23 In younger women, high AMH values may suggest undiagnosed polycystic ovarian syndrome. High AMH may inhibit follicle sensitivity to FSH and subsequent follicular recruitment.24 In older women, high AMH may simply reflect higher-than-normal ovarian reserve. Further study of women with high AMH across various age groups and over time is warranted. This study has several strengths. First, it was specifically designed to address an important public health question: Is diminished ovarian reserve a cause of infertility in women of late reproductive age? Second, the sample size is large enough to detect even relatively small effects. Third, its prospective design allows for biomarker testing at the appropriate time and inclusion of participants with the full range of natural fertility. Fourth, most women were enrolled during their first 3 menstrual cycles of attempting to conceive. Enrolling women later selects a less fertile cohort, as 50% of women are likely to conceive within the first 3 cycles.25 Fifth, the age range studied (30-44 years) focuses on women at risk of diminished ovarian reserve. Sixth, the study protocol standardized the outcome measure (whether a woman conceived in any given menstrual cycle). This was done by providing women free pregnancy tests and instructing them on when to test for pregnancy. Thus, the sensitivity of the test was the same for all, and the set timing of testing minimized the potential for differential identification of pregnancies. Seventh, the ovarian reserve markers evaluated include urinary FSH, which is used in the commercially available test kits marketed for women to assess their natural fertility. Thus, the findings relate directly to the usefulness of such tests. Eighth, biomarkers were measured in all study participants during the early follicular phase, minimizing potential variation in biomarkers due to the phase of the menstrual cycle. This study has several limitations. First, conception, not live birth, was the primary outcome. Fecundity, the capacity to reproduce, is composed of the ability to both conceive and carry a fetus to viability. Diminished ovarian reserve could affect fecundity by increasing the risk of miscarriage, perhaps through an effect on egg quality. Prior studies to date have failed to show such an association.26,27 Second, not all women remained in the study for 12 cycles of attempt. This was anticipated given the older-reproductive-age cohort. Current recommendations advise women older than 35 years to obtain an infertility evaluation after 6 months of attempt. The median attempt cycle at which women started infertility treatment in the study was 8 cycles. For this reason, conception by 6 cycles of attempt was calculated, and Cox models, which allow participants who initiate fertility medications to contribute time to the analysis until they are censored for their fertility medication use, were constructed. Third, ovulation was not assessed. This information would have allowed us to evaluate the strictest definition of fecundability (the probability of conceiving in a given ovulatory menstrual cycle). Fourth, male partners did not provide a semen sample for analysis. However, there is no reason to believe that women with diminished ovarian reserve would be more or less likely to be partnered with a man with abnormal semen parameters. Fifth, not all women were enrolled in their first, second, or third cycle of attempt; however, when the less than 10% of women who entered after their third cycle of attempt were excluded, the findings did not differ. Sixth, although various AMH cutoff values were explored, the study was not powered to look at very low (≤0.1 ng/mL) AMH values, which reflect diminished ovarian reserve more consistent with the late perimenopausal transition. It is possible that in such advanced stages, fecundability may be affected, especially if it results in frequent anovulation. In Reply Prior to implementing new clinical interventions, study of their effect is necessary. Unintended consequences of overtesting occur, regardless of how benign the intervention appears. In our RCT, the FAST examination failed to improve any important clinical outcome compared with standard care. Therefore, routine use of the FAST examination in children is not supported by this study. Dr Marin and colleagues also suggest that trauma evaluations are often protocol driven and many trauma centers “engage in … pan scanning.” During our study, no institutional protocols guiding CT use existed. Most CT scans were ordered by emergency physicians. The top 3 reasons for obtaining CT scans after normal FAST examinations were abnormal abdominal examination (119 [49%]), severe mechanism of injury (97 [40%]), and trauma team request (48 [20%]). In a subanalysis excluding patients for whom the only indication was trauma team request, no significant differences in CT rates (FAST group, 49.3%; standard care group, 52.6%) occurred. Marin and colleagues also state that “the majority of physicians … cared for adult patients.” In our study, 56% of patients were treated by pediatric emergency physicians and 8% more were treated by physicians trained in adult and pediatric emergency medicine. High-grade evidence is needed before the routine use of FAST examinations in hemodynamically stable, injured children is recommended. However, the FAST examination has a role in the evaluation of specific subgroups of injured children (hypotensive) and may decrease CT use in low-risk injured children.5 Clinicians may build proficiency by performing FAST examinations routinely on injured children. In addition, in the trial, younger children in the FAST cohort had lower CT rates following FAST examination. Although this finding did not achieve statistical significance, it warrants further study. We agree with Marin and colleagues that future studies are needed, specifically a multicenter RCT with various hospital types including all clinicians who care for injured children. Until such studies are conducted and show utility, routine use of FAST examinations in hemodynamically stable, injured children does not appear to add value as a diagnostic test. Although secure messaging offers higher standards for information security, HIPAA is technology neutral and has no specific guidelines for security protocols. This neutrality, along with the “reasonably anticipated risk” standard, has allowed alphanumeric text pagers and fax machines, which are unencrypted and unsecure, to be the gold standard in health care telecommunication for decades. During that time, innumerable text pages and fax messages with protected health information have surely been lost, misdirected, or left unsecured, resulting in breaches. Yet little attention has been given to these unsecured communication forms. But now, with growing financial appeal and public attention, dozens of vendors have created a market for secure text messaging products. However, these products are not truly “HIPAA compliant” because there are no standards with which to comply. Despite the warnings of fines for breach,1 there have been no reported punishments levied against a covered entity for breach of SMS communication. There are several probable reasons for this. First, SMS breach occurs on the scale of an individual, which is of lower impact compared with high-profile cases involving millions of patient records in a breach of research data or a server.2 Second, the content of most health-related text messaging is not particularly sensitive, even if the content is protected. Therefore, the Department of Health and Human Services should have bigger concerns than the content of most text messages. Third, and perhaps most importantly, there is no practical way to enforce a ban of SMS at the individual level—it is simply too common and too practical. Choi and Intner too easily dismiss the use of deidentification for HIPAA compliance and maintaining privacy. It is certainly possible to communicate regarding specific patients without using HIPAA identifiers. I maintain that covered entities should minimize (or remove) identifying information and maximize reasonable safeguards to protect the privacy of health information. This due diligence is the standard required by HIPAA, not high-level encryption or recipient authentication, because even the highest levels of security are vulnerable to breach—including secure messaging platforms and the US Department of Defense.3. Antibiotic prescribing rates at primary care office visits over time for each intervention are marginal predictions from hierarchical regression models of intervention effects, adjusted for concurrent exposure to other interventions and clinician and practice random effects. Error bars indicate 95% CIs. Interventions started at day 0 and ended at day 540. The plot in Panel A differs slightly during the intervention period from Panel 2 of the study by Meeker et al3 due to attrition of 5 clinicians, who were not included in this analysis. We randomized 47 primary care practices in Boston, Massachusetts, and Los Angeles, California, and enrolled 248 clinicians to receive 0, 1, 2, or 3 interventions for 18 months. All clinicians received education on antibiotic prescribing guidelines. Two behavioral interventions were electronic health record (EHR)–based: (1) suggested alternatives presented order sets that offered nonantibiotic treatments when clinicians attempted to prescribe antibiotics for acute respiratory infections (ARIs) and (2) accountable justification prompted clinicians to enter free-text written justifications for prescribing antibiotics for ARIs. The third behavioral intervention, peer comparison, sent monthly emails to clinicians comparing their inappropriate antibiotic prescribing rates for ARIs to clinicians with the lowest rates.3. Interventions began between November 1, 2011, and October 1, 2012. Measurements of baseline antibiotic prescribing began 18 months before the start of the intervention and ended 18 months after intervention stopped. The primary outcome was the rate of inappropriate antibiotic prescribing among office visits by adult patients for nonspecific upper respiratory tract infections, acute bronchitis, and influenza.2 In the study, accountable justification and peer comparison significantly reduced inappropriate antibiotic prescribing at the end of the intervention period.3 As a prespecified secondary objective, data were collected for 12 months postintervention, ending on April 1, 2015. During the postintervention period, 5 clinicians left the study and were excluded from this analysis. The analysis was a piecewise logistic hierarchical model, with random effects for practices and clinicians and knots demarcating the intervention start and stop dates for each practice. This model measured the persistence of effects of each intervention during the postintervention period compared with practices that did not receive the intervention, adjusting for exposure to other interventions and practice-level and clinician-level effects. We used Stata (StataCorp), version 14.0, and considered 2-tailed P values less than .05 significant, unless otherwise specified. The institutional review board of each participating institution approved the study and waived patient informed consent. In the 12 months after removing behavioral interventions, inappropriate antibiotic prescribing for ARIs increased relative to control practices—whose inappropriate prescribing rates continued to decrease. However, there was still a statistically significant difference between peer comparison and control practices 12 months after the interventions were removed, possibly because this intervention did not rely on EHR prompts whose absence might have been quickly noted by clinicians. Peer comparison might also have led clinicians to make judicious prescribing part of their professional self-image. Although these findings differ from a prior antibiotic-prescribing feedback intervention that did not have persistent effects,4 peer comparison–induced improvements have been durable in other nonmedical domains.5. Flawed explanations of child abuse fall into 3 categories. One category includes legitimate diagnoses that should be considered in the differential diagnosis of a child with injuries. These conditions might occasionally mimic abusive injuries. One example is the rare household fall that results in life-threatening injury or death. Another example is osteogenesis imperfecta, a well-defined genetic disease that predisposes to multiple fractures and is sometimes confused with abuse. Carefully obtained history, thorough physical examination, imaging studies, laboratory tests, and when appropriate, scene investigation by child protection agencies, law enforcement authorities, or both can reliably differentiate these conditions from abusive injuries. Proponents of these flawed theories argue that alternative diagnoses can look just like physical child abuse. They argue that if an alternative diagnosis is possible then it is not possible to conclude that abuse occurred. If it is not possible to conclude that abuse occurred, then no crime has been committed and there is no need to provide child protection. Some have even suggested that the shaken baby syndrome does not exist, despite documented admissions of shaking by perpetrators of abusive head trauma whose victims died or sustained serious neurological injuries.4. The US justice system is not perfect. There is reason to believe that innocent people have been wrongfully convicted of child abuse and likewise that guilty people have been acquitted. Physicians provide critically important guidance for legal decisions regarding potential abuse. The high stakes of these decisions underscore the importance for all physicians and others to base their medical testimony on solid science. The use of flawed theories has serious consequences, including failure to hold guilty parties responsible and failure to protect children at risk of returning to an abusive environment. Is there considerable controversy regarding the diagnosis of abusive head trauma in clinical medicine as opposed to the courtroom? The short answer is no. In 2009, the American Academy of Pediatrics published a statement advising pediatricians to use “‘abusive head trauma’ rather than a term that implies a single injury mechanism, such as shaken baby syndrome, in their diagnosis and medical communications.”6 The statement confirmed that abusive head trauma was a valid diagnosis and that injury mechanisms include shaking alone, blunt impact alone, or shaking and blunt impact together. Since then, additional clinical studies have appeared confirming that each of these mechanisms can cause abusive head trauma. Currently, in addition to the American Academy of Pediatrics, the World Health Organization, and the Centers for Disease Control and Prevention, many other organizations in both North America and Europe have publicly acknowledged the validity of abusive head trauma. Clearly, there is a consensus regarding the validity of abusive head trauma in clinical medicine. Studies of documented perpetrator admissions4 and of confessed abuse vs witnessed unintentional injuries7 confirm this consensus. The only controversy remains in the courtroom and in the media. Another concern is whether physicians who care for abused children are correctly diagnosing abusive head trauma. In a 2003 study examining the incidence of abusive head trauma, investigators identified all cases of traumatic brain injury resulting in death or admission to an intensive care unit in North Carolina among children younger than 2 years. An expert panel then reviewed these cases to determine if the correct diagnosis of inflicted vs noninflicted injury had been made.8 Of the 152 cases identified over 2 years, 53% were classified as inflicted injuries. Only 2 cases were reclassified by the research team; both cases had been classified by the medical examiner as “undetermined.” One was reclassified as inflicted traumatic brain injury and the other as noninflicted injury. Based on this study, abusive head trauma was correctly diagnosed and not overdiagnosed. No data were provided about cases of abusive head trauma that might have been missed. Physicians who care for injured children must continue to use a scientific approach and careful clinical judgment in diagnosing abuse because it is critically important to get the diagnosis right. The same scientific approach and careful clinical judgment should be used by those who have advanced scientifically unsupported explanations of the findings of abuse. Denying that abusive head trauma occurs, quoting publications that describe flawed theories as if they are scientifically supported, and using fabricated diagnoses are actions that have no place in science or medicine. Furthermore, these flawed theories have no place in law or journalism. Advocacy of theories based on misrepresentation, omission, or both makes a mockery of scientific reasoning and does a disservice to children, families, and justice. The world of paper medical records has almost disappeared, ushering in a new era of electronically stored, analyzed, and shared medical information that offers exciting opportunities for improved patient care. However, this major shift in information management has introduced unintended and unfavorable consequences, such as theft of patient-protected health information, wide-scale sequestering of medical records by ransomware (malicious software—malware—that permanently blocks the access to records unless a ransom is paid), and the ability for hackers to directly harm patients. For example, the recent global WannaCry ransomware attack resulted in more than 48 National Health Service organizations in the United Kingdom being forced to cancel surgical procedures and outpatient appointments. This virus also affected several intravenous contrast power injectors in the United States.1 In addition to health care organizations, more than 230 000 computers in 150 countries were infected. Physicians, as well as others in the health care industry, have historically considered IT issues as an IT problem. Problems such as connectivity, usability, and even short-term down times are events that affect efficiency and satisfaction, but they are usually not viewed as major risks to patient care. However, the era of electronic health records has now created the possibility of new significant risks for patients. Theft of patient medical information is a potentially valuable commodity on the dark web, a group of websites that allows anonymous access to sites often associated with illegal activities. This stolen information not only can be sold for financial gain, but also can be used by other individuals to receive medical care, providing an opportunity for intermingling of medical information that can alter an allergy history, medication list, or other critical elements of a patient’s history. This vulnerability can undermine public trust and prompt patients to withhold sensitive but needed information about medical history. Ransomware also can sequester huge data files, making patient care difficult for extended periods. A large-scale denial of service attack, in which a network is purposefully overloaded with false requests, can substantially interfere with health care clinicians’ activities by preventing access to electronic health records. Coupled with a mass casualty event, such as the Boston Marathon bombing, a denial of service cyber attack that prevents access to medical records, laboratory reports, and radiology results could amplify the disaster, possibly contributing to increased morbidity and mortality. Outdated software in an infusion pump or implantable device can allow hacking of these devices and potentially lead to patient harm or possibly death. This was highlighted in a recent report that used a simulation scenario to demonstrate the risk to patients.4 The scenario revealed what could occur if an infusion pump was illegally accessed and medications were delivered in potentially lethal doses. Cybersecurity is clearly a patient care issue. Health care is an industry that can be described as a mosaic with many components fitting together with a goal to ensure seamless delivery of high-quality, safe health care. The components differ more than just by the services delivered. The US health care system consists of large health systems, academic medical centers, stand-alone hospitals, small critical access hospitals, long-term care facilities, large- and medium-sized physician practices, and small practices of 1 to 3 physicians. In addition, the health care system includes nonclinical organizations such as the pharmaceutical industry and medical device manufacturers. Recent events have underscored the vulnerabilities of larger health care institutions, many of which scrambled for days to protect themselves from WannaCry. With the subsequent Petya malware attack several weeks later, many hospitals were unable to use the Nuance-based dictation service, some hospitals were forced to revert to paper records, and Princeton Community Hospital in rural West Virginia has replaced its entire computer system.5. The HCIC did address issues related to small hospitals and physician practices. These organizations have neither the resources to prepare for a cyber attack nor a robust ability to respond to one. The HCIC had extensive discussions regarding this issue and made recommendations for Congress to modify antikickback and similar statutes to allow the sharing of expertise by larger systems (perhaps as regional consortiums) with smaller health care organizations without the risk of being accused of inurement. There is already a precedent with health systems supplying electronic health record software for a reduced cost if there was to be future sharing of data for meaningful use and performance improvement. Even if physicians or small hospitals could afford to buy the level of security expertise needed, which they cannot, there is a national shortage of that expertise. This resource gap was also addressed in the HCIC report. They should not object to having to change passwords on a regular basis, and they should use passwords that are strong. They should be wary of email phishing, a common portal of entry for hackers. Vulnerabilities, such as nonupdated software, must be mitigated; cybersecurity software must be deployed; and suspicious network activity needs to be reported. Clinicians should never assume that because their practice or organization is small, they will not be a target of hackers and malware. Advocating for adequate resources is also important. It is the professional duty of physicians to be advocates for patients and comprehensively address this situation. The promise of improved care from a digital world will be broken and patients could be placed at risk if cybersecurity is not made a priority issue. Academic medical centers find it increasingly necessary to pursue economies of scale by merging, partnering with, acquiring, or being acquired by nonacademic hospital networks and health systems. These arrangements may provide greater purchasing power, leverage with payers, and a reduction in the size of clinical and ancillary staff for services that are deemed to be redundant. In this environment, a new set of challenges confront the ability of academic medical centers to fulfill the mission to create new medical knowledge. Clinical research is encountering a “hyper-level of scientific and operating complexity”1 and pressures to generate clinical volume and revenue leave many academic researchers with limited ability to focus on investigative work. The process may require bridging a language mismatch by use of terms that underscore shared mandates (eg, quality improvement, program building, efficiency, safety) and a focused effort to align the research agenda with health system needs, such as the identification of variability in care delivery and outcomes by disease state or procedure. Ideally, the entities involved will have strong research champions in a position of influence who can contribute to collaborative stewardship and governance (eg, of research access to the electronic health record) and minimize the perception that clinical research is a silo enterprise. In addition, professional organizations and their members should engage with health system leaders and emphasize through conference calls, seminars, webinars, and related efforts the potential benefits of network-oriented clinical research. The Association of American Medical Colleges, for example, has organized the “Research on Care Community” specifically to encourage collaborations of health system researchers and clinicians.5. Rates of response and remission achieved by CBT were found to be only moderate. For depressive disorders, for example, response rates of about 50% were reported. This is true for anxiety disorders as well. Rates for remission are even smaller. Thus, a considerable proportion of patients do not sufficiently benefit from CBT. After psychoanalysis had dominated the early years of psychotherapy claiming to be the gold standard, a “CBT-centric” era8(pE2) began and some of its proponents succeeded in presenting CBT not only as the empirically best studied treatment,2 but also as the most effective psychotherapy: “The most effective contemporary approach is Cognitive Behavior Therapy…”9(p36) While CBT is beneficial for many patients, and CBT researchers developed and tested treatments often long before other approaches, the evidence suggests that CBT should not be considered the gold standard of psychotherapy. Of note, most of the critical results reviewed were reported by CBT proponents or by independent researchers. Thus, the view that the evidence for CBT is limited should not be attributed to a bias against CBT. Furthermore, the critical results do not stem from arbitrarily selected individual studies but from several meta-analyses or systematic reviews.1,5-7 These findings are based on a substantial number of studies, showing a pattern of results. For example, the meta-analysis on study quality and efficacy of CBT in depressive and anxiety disorders included 144 RCTs.1. Prematurely declaring one treatment as the gold standard not only has important clinical implications, but also may seriously limit the progress of research because research on other methods of psychotherapy may not be given an equal chance for funding. No form of psychotherapy can presently claim to be the gold standard, suggesting the need for plurality in treatment and research, ie, a variety of different psychotherapeutic approaches. All evidence-based therapies have their strengths, be it a focus on cognitive, emotional, interpersonal, or unconscious processes. Only plurality allows for bridging the gap between the different approaches and for learning from each other to further improve the treatment of patients with mental disorders.8. During the vast majority of the rest of our lives in Malawi, when all of us are healthy, I take care of patients with cancer and lead a research program focused on finding innovative solutions for cancer prevention, early detection, diagnosis, treatment, and palliation in one of the most resource-limited settings in the world. We have been fortunate to find interested funders and to have a remarkable team of partners, clinicians, and scientists to move these activities forward both in Malawi and other low-income countries. In discussing this work, I am often asked why I do cancer care and research in Malawi, and how it benefits the United States. These questions have a particular urgency now given the uncertain future of global research funding including the Fogarty International Center, which has been targeted for possible elimination and is a major supporter of our program and my career specifically. After a lot of thought, I usually have a couple of responses to such questions. First, rigorous clinical science addressing important questions benefits human health broadly. I knew to give my daughter intravenous artesunate for her malaria because of studies conducted in African and Asian children demonstrating it is the best treatment, results that have directly benefited countless Americans with malaria including military personnel and my daughter. Likewise, studies that our group led in Malawi demonstrated that treating HIV-infected people with antiretrovirals makes them noninfectious to sexual partners, changing worldwide policy including in the United States with respect to when HIV treatment should be started. We similarly believe that our clinical, laboratory, and public health studies of high-burden cancers in Malawi will add substantially to overall knowledge about individual tumor types and improve care for patients around the world. Second, it is important to note that no Malawi colleague who helped with my daughter’s care ever asked, “What’s in it for me?” In contrast, there was absolutely no hesitation to expend impossibly scarce time or resources from the public sector of one of the least resourced countries in the world to help her. I would argue from an admittedly biased personal experience, that self-interest above all else is not necessarily a widely prevalent worldview, even in the world’s least resourced countries where survival is most fraught. Reflecting this, my Malawi colleagues are typically exceptionally talented individuals who were chosen, sponsored, or both for advanced training outside their country, following which they passed up far more lucrative opportunities in order to return home and serve their fellow citizens. Humanitarianism can be a sufficient motivation in and of itself for individuals and even entire programs. Helping patients who otherwise would surely die, but for our program, makes us and our partners feel happy and proud and grateful, and that’s often precisely how my work benefits me as an American, with nothing more. Cluster sizes are estimated according to the annual number of emergency department visits for all patients and for patients aged 75 years or older. There was no screening log in the emergency departments. The number of patients screened for inclusion in each hospital was estimated with the annual number of emergency department visits of patients aged 75 years or older, of which an estimated 8% (data from the ICE-CUB 1 study) were related to critical conditions, of which an estimated 33% (data from the ICE-CUB 1 study) fulfilled the inclusion criteria. Data are not available for the number of hospitals screened for eligibility or the number of or reasons for exclusion prior to randomization. Median duration of follow-up in alive patients was of 6 months (interquartile range, 6.0-6.1 months) in both the systematic strategy and the standard practice groups. Panel A shows Kaplan-Meier curves of the unadjusted probability of survival between the systematic strategy group and the standard practice group from emergency department visit to 6 months. Patients from the systematic strategy group had a lower 6-month survival rate vs patients in the standard practice group (Kaplan-Meier estimates, 55.7% [95% CI, 53.2%-58.2%] vs 61.8% [95% CI, 59.4%-64.3%]; P < .001; hazard ratio, 1.24; 95% CI, 1.02-1.51). Panel B shows Kaplan-Meier curves of the inverse probability-weighted adjusted probability of survival from emergency department visit to 6 months. The P value from the inverse probability-weighted Cox regression model is shown in the graph. After adjustments for baseline characteristics, survival rates at 6 months were not significantly different between groups (hazard ratio, 1.10; 95% CI, 0.93-1.31). The trial was approved by the Comité de Protection des Personnes d’Ile-de-France 9. Participants or their surrogate decision makers were informed orally about the trial, and their nonopposition to trial participation was recorded in patient’ files. In accordance with French law, written informed consent was not required because both interventions are part of standard care. Patients who were eligible but incapable of receiving information and for whom a surrogate decision maker was not available might be included by deferred information. These participants were informed about study participation as soon as they were able to understand information. Information was collected during a 6-month follow-up period. Participants were informed that the authorization to use patient data could be withdrawn at any time. Details on steering committee members and conduct of the study are available in the eAppendix in Supplement 1. The allocation schedule was independently established by a statistician at the clinical research unit using a computer-generated randomization list. Randomization was stratified by the median annual number of emergency department visits in participating centers (n=44 616), presence or absence of a geriatric ward, and geographical area (Paris area vs other regions in France). Each site allocation was kept secret by the clinical research unit until the training of the study personnel was completed and was revealed to the hospital primary investigators at the beginning of the inclusion period. Due to the study design, no allocation concealment was possible. Quiz Ref IDIn hospitals assigned to the systematic strategy group (intervention), a program to promote systematic ICU admission was implemented. In this program, emergency department and ICU physicians were asked to systematically recommend an ICU admission for all included patients during the triage process. Other interventions to promote ICU admission included the following: a member of the steering committee visited each center and presented the trial protocol; when including a participant in the trial, the emergency department physician was required to systematically call the attending ICU physician; the ICU physician was required to systematically evaluate the patient at the bedside; and the emergency department and ICU physicians were required to jointly decide whether to admit the patient to the ICU with consideration of participant or surrogate decision-maker opinions about ICU admission. If no ICU bed was available in the hospital, the patient had to be transferred to an ICU located in another hospital. Monthly meetings were organized with the emergency department and ICU staff. Booklets and posters presenting the recommendations for ICU admission were used. All patients aged 75 years or older who arrived in the emergency department were assessed for eligibility. Patients were included if they had one of the preestablished critical conditions listed in eTable 1 in Supplement 1, had preserved functional status as defined by an Index of Independence in Activities of Daily Living score of 4 or higher or not evaluable,11 had preserved nutritional status (defined as absence of cachexia, subjectively assessed by physicians at the bedside), and were free of active cancer. Exclusion criteria were an emergency department stay of more than 24 hours, a secondary referral to the emergency department, and refusal to participate. Patients in whom cancer was diagnosed after inclusion remained in the statistical analysis. Further information on inclusion and exclusion criteria and follow-up are available in the eAppendix in Supplement 1. The primary outcome was overall mortality at 6 months. Secondary outcomes were ICU admission rate, in-hospital mortality, functional status at 6 months as assessed by the Index of Independence in Activities of Daily Living,11 and quality of life at 6 months as assessed by the 12-Item Short-Form Health Survey.12 The Index of Independence in Activities of Daily Living11 is based on an evaluation of the functional independence or dependence of patients in bathing, dressing, toileting, transferring, continence, and feeding and ranges from 0 (totally dependent) to 6 (independent). The 12-Item Short-Form Health Survey12,13 is a measure of health-related quality of life and includes a physical and mental component; the score ranges from 0 to 100, with higher scores representing better self-reported health. Other secondary outcomes were the characteristics of the triage process: opinions about ICU admission and involvement of physicians and participants in the decision process and the number of ICUs with all beds filled. Caregiver burden14 at 6 months was assessed in 2 centers and is not reported in this article. For patients discharged alive, information on outcomes were obtained through telephone calls, either directly from patients, from patients’ relatives or general practitioners, or from appropriate legal institutions. Data on patients who were lost to follow-up before 6 months were censored at the last follow-up assessment. The statistical analysis plan is available in Supplement 3. Assumptions based on sample size calculation were based on the ICE-CUB 1 study.6 Considering an estimated 32% 6-month mortality rate in the control group and an estimated intracluster correlation coefficient of 0.01, a sample size of 2802 was required to have 74% power to detect a 6% difference in mortality rates in a 2-sided test at a .05 level of significance. Twenty-four hospitals agreed to participate instead of 20 initially planned; to take into account cluster randomization with inflation dependent on intraclass correlation coefficient, the number of patients to be included was increased to 3000. The absolute reduction of 6% in mortality was established as a compromise resulting in a feasible sample size and a clinically relevant reduction in mortality.15 Recruitment was ultimately ended when the targeted sample size was achieved in each cluster. Baseline characteristics of patients were analyzed as frequencies and percentages for categorical variables and as medians and interquartile ranges (IQRs) for continuous variables. Bivariable associations were evaluated using the t test for continuous variables and the χ2 or Fisher exact test for categorical variables as appropriate. Variables for all adjusted analyses were age, illness severity, initial clinical diagnosis, seniority of the emergency physician, time of ICU admission, baseline functional status, living place, and type of home support. Generalized estimating equation methods using robust sandwich estimators to estimate the variance-covariance matrix of the regression coefficient estimates were used to account for clustering. Binary outcomes (ICU admission rate, in-hospital death, death within 6 months, change in Index of Independence in Activities of Daily Living11) were analyzed using logistic regression models. Absolute risk differences and relative risks (RRs) were derived from logistic regression models and 95% confidence intervals for these measures were obtained using nonparametric bootstrap methods with 1000 samples.16 The crude overall survival at 6 months was estimated by the Kaplan-Meier method and compared using a log-rank test. The 6-month survival adjusted for baseline characteristics was estimated using a Cox model. Adjusted survival curves were produced using an inverse probability-weighted Kaplan-Meier estimation. Significance was tested using a Cox regression model weighted by the same weights (inverse probability-weighted Cox). Health-related quality of life at 6 months was analyzed using linear regression models. We used multiple imputation for participants with missing data, using predictive mean matching for continuous variables, logistic regression for binary data, and polytomous regression for (unordered) categorical data. Ten imputations were drawn. The analyses were performed on an intention-to-treat basis according to the prespecified analysis plan, without planned interim analysis. A post hoc exploratory analysis was performed to compare the characteristics of patients admitted to the ICU in each group. All analyses were performed at a 2-sided α=.05; a P<.05 was considered statistically significant. All analyses were performed by A.B. with R software, version 3.2.2 (R Foundation for Statistical Computing). Between January 2012 and April 2015, a total of 3037 patients were enrolled in the trial, with 1519 patients in 11 hospitals randomized to the systematic strategy group and 1518 patients in 13 hospitals randomized to the standard practice group (Figure 1). The recruiting rate was higher in the systematic strategy group compared with the standard practice group. To achieve the total number of participants needed, the recruiting period was extended in the hospitals randomized to the standard practice group. The mean inclusion period was 22.5 months in the systematic strategy group and 28.5 months in the standard practice group. One patient subsequently withdrew consent, leaving a total of 3036 patients. There were 12 medical and 12 mixed (medical and surgical) ICUs. The baseline characteristics of the patients are presented in Table 1. The median age at inclusion was 85 years (IQR, 81-89 years); 45% (1361/3036) were men. The most frequent initial clinical diagnoses for hospital admission were septic shock (413/3036 [13.6%]), acute respiratory failure requiring noninvasive ventilation (347/3036 [11.4%]), severe pneumonia (250/3036 [8.2%]), and cardiac insufficiency requiring noninvasive ventilation (218/3036 [7.2%]) (a complete list is available in eTable 2 in Supplement 1). Patients in the systematic strategy group had a higher severity of illness at admission, as reflected by a higher Simplified Acute Physiology Score  III score.18. A post hoc analysis was conducted to compare patients admitted to the ICU in each group (eTable 6 in Supplement 1). Patients admitted to the ICU in the systematic strategy group had a higher Simplified Acute Physiology Score III18 score (difference in medians, 3; 95% CI, 2-5; P < .001), more often underwent mechanical ventilation (374/884 [42%] vs 147/470 [31%]; difference in proportions, 11%; 95% CI, 6%-16%; P < .001), less often underwent noninvasive ventilation (251/884 [28%] vs 170/470 [36%]; difference in proportions, –8%; 95% CI, –13% to –3%; P < .001) and less often underwent fluid resuscitation (177/835 [21%] vs 151/469 [32%]; difference in proportions, –11%; 95% CI, –16% to –6%; P < .001) than patients admitted to the ICU in the standard practice group. The ICU and hospital length of stay were not significantly different between groups (difference in medians, respectively, of 1 [95% CI, –0.5 to 1] days and 1 [95% CI, –1 to 3] days). The overall mortality rate in the ICU was 23% (328/1448) and was not significantly different between groups (220/932 [24%] for systematic strategy vs 108/516 [21%] for standard practice; difference in proportions, 3%; 95% CI, –2% to 7%; P = .27). There was a greater decrease from baseline Index of Independence in Activities of Daily Living at 6 months in the systematic strategy group  than in the standard practice group (median of the difference between follow-up and baseline scores, –0.5 [IQR, –2.0 to 0.0] vs –0.5 [IQR, –1.5 to 0.0]; P = .02; difference in means, –0.20; 95% CI, –0.37 to –0.03) (detailed baseline and follow-up data are reported in eTables 7 and 8 in Supplement 1). The probability of a decrease in at least 1 of the Index of Independence in Activities of Daily Living18 domains was not significantly different between groups after adjustments for baseline characteristics (RR, 1.02; 95% CI, 0.99-1.05) (Table 3). Self-reported physical quality of life at 6 months was not significantly different between groups (mean score, 36.7 vs 36.2; difference in means, 0.5 [95% CI, –0.6 to 1.5]). Self-reported mental quality of life at 6 months was higher in the systematic strategy group (mean score, 44.6 vs 43.7; difference in means, 0.9 [95% CI, 0.1-1.6]) (Table 3). Additional data at 6-month follow-up are available in eTable 9 in Supplement 1. The mortality rates were also consistent with available data in the literature. Recent observational data on critically ill elderly patients admitted to the ICU suggested in-hospital mortality rates ranging from 24% to 40%,1,10,21-23 3-month mortality rates from 39% to 41%,1,17,24 6-month mortality rates from 37% to 51%,1,6,25 and 1-year mortality rates from 44% to 68%.1,10,17,21,23 In the current trial, the selection of patients with preserved baseline functional and nutritional status and free of active cancer may explain the observed low rates of in-hospital (26%) and 6-month (42%) mortality compared with observational data. The higher in-hospital mortality rate observed in the systematic strategy group may be explained in part by the fact that a higher ICU admission rate in the intervention group might have precipitated early discussions about or more frequent withdrawals of life-sustaining therapies, possibly leading to a higher mortality rate in the systematic strategy group. However, the ICU and hospital lengths of stay and the number of patients with at least 1 organ in need of support were similar between the 2 groups. To our knowledge, this is the first randomized clinical trial evaluating the effect of ICU triage on long-term mortality in an elderly population. Randomization by cluster (hospital) allowed evaluation of the effect of ICU triage without difficulties incurred by ethical considerations of randomization for ICU admission at an individual patient level. The intervention program led to different ICU admission rates between groups. Furthermore, senior physicians were more frequently involved in the triage decision-making process, and the decisions were frequently shared between physicians. Quiz Ref IDHowever, despite a recommendation for systematic ICU admission in the intervention group, more than one-third of patients were not admitted to the ICU. This was expected, as physicians had the final decision for ICU admission and could refuse an ICU admission even if the hospital was randomized to the intervention group. This trial has several strengths. First, the design takes into account the 2 steps of the triage process by both emergency department and intensive care physicians. This avoided potential bias related to variation in triage criteria for ICU admission proposal by emergency department physicians. Second, participants were patients without long-term adverse outcome factors as identified in the ICE-CUB 1 study,6 to avoid inclusion of patients with very high expected mortality. Third, the trial was pragmatic, which contributes to the generalizability of the results. Fourth, in addition to survival analyses, this trial assessed functional status and quality of life, which are important and relevant outcomes in the elderly population. Fifth, there were very few protocol violations, allowing for an intention-to-treat analysis, and few patients were lost to follow-up. This study also has several limitations. First, the recruitment period in the standard practice cluster was longer that in the systematic strategy cluster, which could have introduced confounding due to secular changes. Second, because of the nature of the intervention, no blinding to group assignment was possible. However, the effect of the nonblinded design on 6-month mortality is considered low. Third, data on withdrawal of life-sustaining therapies were not collected. Fourth, the potential benefits of ICU admission might be confounded by recruitment of patients who were more severely ill in the systematic strategy group. However, results were adjusted for several baseline characteristics, including illness severity. Fifth, the benefits of increased ICU admission could accrue only to patients who would normally have received floor care but instead received ICU care. In this study, the question of whether benefit might accrue to patients on the margin was not addressed. However, because of the design of the study, variation in individual probability of ICU admission is likely to be small. Sixth, a qualitative approach to understand why patients were not admitted to the ICU was not conducted. Seventh, differences in the quality of care provided in the wards may affect the outcome. For example, ward admission as an alternative to the ICU might partially explain the negative results observed in the present study when patients were admitted to wards providing very good care. The cluster design and sample size should reduce this potential bias because hospitals with emergency departments and ICUs were randomized. Moreover, patients already in the hospital (secondary transfer) or admitted from another hospital were excluded to minimize the effect of previous quality of care on prognosis. Nonetheless, the hospitals that had lower ICU admission rates at baseline may in particular have developed higher-quality ward care; thus, a strategy to boost ICU admission rates may not have yielded the same gains in outcome in such centers as in other settings. Quiz Ref IDThe findings of this study may be informative for health care delivery from a hospital perspective. When considering the increasing demand for intensive care among elderly patients, the importance of health care resource allocation and expenditure control, and the possible absence of long-term benefit of intensive care, systematic ICU admission of all critically ill elderly patients might not be warranted. However, this study should not be interpreted as suggesting that no elderly patient should be admitted to the ICU. Because of substantial uncertainty in outcomes among individual patients, there is a need to systematically and thoughtfully assess the potential benefits and harms of ICU admission for every elderly patient presenting with critical illness. The trial was conducted among 632 patients with type 2 diabetes who were 18 years or older receiving stable treatment with diet and exercise or stable treatment with metformin for 30 days prior to screening. Patients had a hemoglobin A1c level of 7.0% to 9.5% and an estimated glomerular filtration rate of 60 mL/min/1.73 m2 or more. There was no dose escalation in the oral semaglutide 2.5-mg and placebo groups. In the other oral semaglutide groups, the dose was doubled from a starting dose of 2.5 mg or 5 mg every 4 weeks until the trial maintenance dose of the group (5-40 mg) was achieved (blue shades). The subcutaneous semaglutide dose was doubled every 4 weeks from a starting dose of 0.25 mg until a 1 mg trial maintenance dose was achieved. In addition, a slow dose escalation (purple) to 40 mg of oral semaglutide at 8-week intervals and a fast dose escalation (green) to 40 mg of oral semaglutide at 2-week intervals were included. Subcutaneous semaglutide was supplied as a 1.34-mg/mL solution in a 1.5-mL prefilled PDS290 pen injector (FlexTouch, Novo Nordisk A/S), and was administered in the abdomen, thigh, or upper arm on the same day of the week. RMM indicates repeated measures model; SC, subcutaneous. A, Data are estimated means from RMM with treatment, stratum, country, and baseline value all nested within visit. Error bars indicate 95% CIs. B, The proportion of patients achieving an HbA1c level of less than 7.0% after 26 weeks of treatment was significant for the oral semaglutide 2.5-mg group vs placebo (P = .01) and for all other oral semaglutide dosages and SC semaglutide (P < .001). Missing HbA1c values are imputed from RMM analysis before calculating the proportions of patients reaching the target. This 26-week, randomized, parallel-group, phase 2, dosage-finding trial (Figure 1), conducted between December 2013 and December 2014, assessed the dosage-response relationship on glycemic control (mean change in hemoglobin A1c [HbA1c]) level of 5 dosages (2.5, 5, 10, 20, and 40 mg) of once-daily oral semaglutide compared with placebo in a double-blind design (primary end point) and open-label, once-weekly subcutaneous semaglutide (secondary end point) in patients with type 2 diabetes. Oral semaglutide and placebo doses were blinded from both the investigator and the patient. In addition to a 4-week interval dose escalation (standard escalation), in which oral semaglutide or placebo doses were doubled every 4 weeks until the trial maintenance dose was achieved, the efficacy and safety of an 8-week interval (slow escalation) and a 2-week interval (fast escalation) dose escalation regimen for the highest dose (40 mg) of oral semaglutide were explored. The 26-week treatment period was followed by a 5-week follow-up period and visit at week 31. The trial protocol and statistical analysis plan are available in Supplement 1. Patients (18 years or older) with type 2 diabetes and insufficient glycemic control (HbA1c level range, 7.0%-9.5%) on diet and exercise alone or with a stable dose (at least 30 days) of metformin were enrolled at 100 sites in 14 countries (eBox 1 in Supplement 2). Additional eligibility criteria were HbA1c level of 7.0% to 9.5% and a body mass index (BMI, calculated as weight in kilograms divided by height in meters squared) of 25 to 40 (for key exclusion criteria, see eBox 2 in Supplement 2). Because the trial was conducted in Europe, North America, and single countries in Africa, Asia, and the Middle East, race and ethnicity were recorded for completeness of data, according to local regulations. Race and ethnicity were self-reported by participants from categories predefined in the study protocol (race: American Indian or Alaska Native, Asian, black or African American, Native Hawaiian or other Pacific Islander, white or other; ethnicity: Hispanic or Latino, or not Hispanic or Latino). An open-label design was chosen for once-weekly subcutaneous semaglutide to limit unnecessary injections. Patients were randomized using an interactive voice and web response system with equal ratio to 1 of 9 treatment groups, stratified according to history of treatment (metformin at screening, yes or no). Treatment groups included 5 oral semaglutide dosage groups (2.5, 5, 10, 20, and 40 mg) and an oral placebo group (these groups received a once-daily dose with 4-week interval dose escalation), and a 1-mg subcutaneous semaglutide group (receiving a once-weekly dose). Two additional 40-mg dosages were included to evaluate 8-week (slow) and 2-week (fast) dose escalation. Trial products were supplied by Novo Nordisk A/S. Oral semaglutide tablets (but not placebo) included 300 mg of SNAC (based on the Eligen Carrier concept, Emisphere Technologies).13 Patients administered oral semaglutide or placebo in the morning after at least 6 hours of fasting, and abstained from food or fluid intake for at least 30 minutes thereafter. If fasting plasma glucose exceeded 270 mg/dL/15 mmol/L (week 1-5), 240 mg/dL/13.3 mmol/L (week 6-11), or 200 mg/dL/11.1 mmol/L (week 12 to trial end), rescue medication was to be offered. Safety end points included the number of treatment-emergent adverse events and severe (American Diabetes Association criteria14) or blood glucose–confirmed (plasma glucose value of 70 mg/dL [to convert to mmol/L, multiply by 0.0555] or lower with symptoms) hypoglycemic episodes recorded from baseline until week 31. Change in vital signs, electrocardiogram, physical examination, and laboratory safety parameters were assessed after 26 weeks. Adverse events relevant to the GLP-1 drug class were given specific attention. eBox 3 in Supplement 2 lists the 8 predefined medical events of special interest that were adjudicated in a blinded fashion by an external, independent event-adjudication committee. A standard repeated measures model analysis, with treatment, country, and stratification (metformin, yes or no) as fixed factors, and baseline value as a covariate, all nested within visit, was used for analysis of continuous end points, including the primary end point, body weight, BMI, waist circumference, and fasting plasma glucose. End points for patients attaining HbA1c level and weight loss targets were analyzed using a modified Poisson regression model with treatment, country, and stratification as fixed factors and baseline HbA1c level or body weight, respectively, as a covariate. Before analysis, missing data were imputed from a repeated measures model with treatment, stratum, country, and baseline value all nested within visit. This model was specified post hoc. Fasting insulin, glucagon, C-peptide, insulin resistance, beta-cell function and lipids were log-transformed at week 26 and analyzed by the standard repeated measures model analysis. Treatment-emergent adverse events were summarized descriptively. All analyses were performed using SAS (SAS Institute), version 9.3. To investigate the efficacy of oral semaglutide without the risk of inflating a type I error, a confirmatory statistical analysis was carried out, whereby an initial comparison of the primary end point between the pooled 40-mg standard escalation and fast escalation groups vs placebo group was performed at week 26. The standard repeated measures model analysis was used to estimate the treatment difference and corresponding 2-sided P value at week 26. Efficacy of oral semaglutide was considered confirmed if the upper limit of the 95% CI for the ETD was less than 0, corresponding to a 2-sided P value of less than .05. If efficacy was confirmed, the comparisons between the 9 treatment groups and other secondary end points were evaluated with no adjustment for multiplicity and considered exploratory in nature. Of the 1106 patients screened, 632 were randomized and 630 exposed to trial medication (Figure 2). Baseline characteristics were similar in the 9 groups (mean age, 57.1 years (SD, 10.6); men, 395 of 630 patients (62.7%); mean HbA1c level, 7.9% (SD, 0.7%); diabetes duration, 6.3 years (SD, 5.2); body weight, 92.3 kg (SD, 16.8); BMI, 31.7 (SD, 4.3) (Table 1). Overall, 583 patients (92%) completed the trial (completed the 31-week follow-up visit), with 492 (78%) completing treatment (Figure 2). The proportion of patients completing the trial without rescue medication, and contributing to the analysis at week 26, was 64% to 83% in the dosage-dependent oral semaglutide groups, 73% in the subcutaneous semaglutide group, and 72% in the placebo group (Figure 2). All dosages of oral semaglutide reduced mean HbA1c level significantly more than placebo by week 26, in a dosage-dependent manner (Figure 3A). ETDs for dosage-dependent oral semaglutide vs placebo were −0.4% (95% CI, –0.7% to –0.1%] for the 2.5-mg group; –0.9% [95% CI, –1.2% to –0.6%] for the 5-mg group; –1.2% [95% CI, –1.5% to –0.9%] for the 10-mg group; –1.4% [95% CI, –1.7% to –1.1%] for the 20-mg group; and –1.6% [95% CI, –1.9% to –1.3%] for the 40-mg standard escalation group (P = .007 for the 2.5-mg group, <.001 for other dosages). The decrease in mean HbA1c level in the subcutaneous semaglutide group (1.9%) was also significantly greater than the placebo group (secondary analysis) (ETD, –1.6% [95% CI, –1.8% to –1.3%]; P < .001), and not significantly different from oral semaglutide dosages of 20 mg and 40 mg (standard escalation). The cumulative distribution of the HbA1c level reduction in the oral and subcutaneous semaglutide groups illustrates that with the exception of the 2.5-mg group, almost 100% of patients experienced a reduction in HbA1c level vs 74% in the placebo group (eFigure 1 in Supplement 2). In addition, a range of sensitivity analyses supported the primary comparisons with similar results in favor of oral semaglutide (eFigure 2 in Supplement 2). At week 26, significant dosage-dependent decreases in mean fasting plasma glucose level of up to 51 mg/dL from baseline were observed with oral semaglutide vs placebo (Figure 4A). Fasting plasma glucose level decreases with oral and subcutaneous semaglutide occurred mostly within the first 4 to 8 weeks (Figure 4A); at week 26, decreases observed with 40-mg standard escalation of oral semaglutide were not significantly different to those with subcutaneous semaglutide (Figure 4A). eTable 1 and eTable 2 in Supplement 2 show improvements in insulin, glucagon, and homeostasis model assessment insulin resistance and beta-cell function. No consistent pattern was observed with fasting C-peptide. Body weight decreased over time (Figure 4B). At week 26, the decrease from baseline in mean body weight in the oral semaglutide groups was dosage-dependent and significantly greater than placebo (–1.2 kg) (ETD: 2.5-mg group, –0.9 kg [95% CI, −2.4 to 0.6]; 5-mg group, −1.5 kg [95% CI, −3.0 to 0.0]; 10-mg group, –3.6 kg [95% CI, −5.1 to −2.1]; 20-mg group, −5.0 kg [95% CI, −6.5 to −3.4]; 40-mg standard escalation group, −5.7 kg [95% CI, −7.3 to −4.2]) (significant vs placebo in the ≥10-mg dosages [P < .001]) (Table 2). No significant difference was observed between 20 mg and 40-mg standard escalation groups of oral semaglutide and the subcutaneous semaglutide group. The proportion of patients achieving 5% weight loss was significantly greater for oral semaglutide dosage groups of 10-mg and higher (P < .001; 10-mg group: 38 of 69 patients [56%], RR, 4.1 [95% CI, 2.2 to 7.6]; 20-mg group: 45 of 70 patients [64%], RR, 5.2 [95% CI, 2.8 to 9.6]; 40-mg standard escalation group: 50 of 71 patients [71%], RR, 5.4 [95% CI, 9.2 to 9.9]) vs the placebo group (9 of 71 patients [13%]), and the subcutaneous semaglutide (45 of 69 patients [66%], RR, 5.2 [95% CI, 2.8 to 9.6], P < .001) (eTable 1 in Supplement 2). Treatment-emergent adverse events for all treatment groups are shown in Table 3. There were no fatal events. The number of serious adverse events and patients reporting them were low (total of 31 events reported in 21 patients), with no grouping of events (eTable 3 in Supplement 2). The most common adverse events were gastrointestinal, which were mostly mild to moderate in severity with oral semaglutide (Table 3). The proportion of patients reporting gastrointestinal events was higher with oral semaglutide (31%-77%; 255 of 490 patients) and subcutaneous semaglutide (54%; 37 of 69 patients) than with placebo (28%; 20 of 71 patients). Overall, similar proportions of patients reported gastrointestinal-related adverse events in the three 40-mg dose escalation groups (2, 4, and 8 weeks). Fewer nausea events were reported when patients started on a lower dose (eg, 2.5 mg vs 5 mg) (eFigure 3 in Supplement 2). With continued therapy, nausea prevalence and severity decreased in most patients (eTable 4 in Supplement 2), partly explained by some patients discontinuing treatment prematurely because of these events. Reductions in systolic and diastolic blood pressure occurred in all treatment groups; systolic blood pressure reductions were more pronounced with oral and subcutaneous semaglutide than with placebo (eTable 1 and eTable 2 in Supplement 2). Change in mean heart rate ranged from –1.7 to 3.0 beats/min with oral semaglutide vs 2.6 beats/min with subcutaneous semaglutide and –4.0 beats/min with placebo. At week 26, changes in heart rate were significantly greater with oral semaglutide 5 mg or higher and subcutaneous semaglutide compared with placebo (eTable 1 and eTable 2 in Supplement 2). Six cardiovascular events in 5 patients were confirmed by adjudication (oral semaglutide: 10-mg group, 1 patient; 40-mg slow escalation group, 2 patients; placebo: 2 patients) (eTable 6 in Supplement 2). Quiz Ref IDAmong patients with type 2 diabetes, oral semaglutide resulted in better glycemic control than placebo over 26 weeks (primary end point). From a mean baseline HbA1c level of 7.9%, between 44% (2.5-mg group) and 90% (40-mg standard escalation group) of patients receiving oral semaglutide achieved the target HbA1c level of less than 7.0%. Clinically relevant (5% or more) weight loss was achieved in up to 71% of patients receiving oral semaglutide. The magnitude of improvements with oral semaglutide at 20 mg and 40-mg standard escalation was not significantly different than subcutaneous semaglutide and was similar across the dosage escalation groups. IQuiz Ref IDmprovements in glycemic control and body weight with oral semaglutide were achieved with a low rate of hypoglycemia. No unexpected safety findings were identified. Gastrointestinal adverse events, with consequent premature treatment discontinuation, was observed in the oral semaglutide groups, consistent with the known adverse effects of GLP-1 receptor agonists. Fewer adverse events were reported when patients started on a low dose (eg, 2.5 mg) and the frequency of gastrointestinal adverse events was highest during the dose-escalation period and decreased over time in the oral semaglutide groups. This study has several limitations including duration. Longer-term data will provide more information about the safety and efficacy durability of oral semaglutide. A longer study duration may have demonstrated the maximum HbA1c level and weight reductions in the groups administered the higher doses of the medication. Future trials should assess the efficacy of oral semaglutide in patients with a high baseline HbA1c level to explore its potential in patients who are less well controlled, and in combination with other glucose-lowering agents. There was no adjustment for multiplicity in the statistical analyses, which may contribute to a type I error. One of the most important decisions that a physician makes is whether to admit a patient to the intensive care unit (ICU). The modern ICU provides a capacity for advanced monitoring and life support that is typically unavailable elsewhere in the hospital and is lifesaving for patients with a wide array of acute deteriorations in health. However, ICU care is also one of the most expensive, intensive, and intrusive endeavors in health care. Although patients admitted to the ICU account for approximately one-quarter of hospitalized patients, they account for half of total hospital expenditures in the United States, with costs estimated at $110 to $260 billion per year or approximately 1% of the gross domestic product.1-3 Furthermore, ICU care can be unnecessary, harmful, or futile. Importantly, the provision of ICU services is increasing. In an era when efforts to contain health care costs have decreased total hospital beds, the number of ICU beds continues to increase.4 An important question is whether this growth in ICU services and beds is necessary to meet the demands of an expanding population of critically ill patients or whether ICU beds are being oversupplied and subsequently are being filled with patients who might be cared for in less-intense settings at lower cost with similar or better outcome. Optimal ICU bed supply and admission decisions have been debated for decades. Central to the debate is the difficulty anticipating whether ICU care can yield meaningful long-term benefit. As such, most controversy surrounds whether to admit patients with limited life expectancy, such as very old patients or those with significant chronic health problems, with wide variability in ICU admission documented in clinical practice.5,6 However, both the debate and current practice are informed almost exclusively by opinion and observational studies. Only 1 of 62 recommendations in the 2016 Society of Critical Care Medicine ICU admission, discharge, and triage guidelines is supported by randomized trial evidence, and there has never been a formal trial of different ICU admission strategies.7. In this issue of JAMA, Guidet and colleagues report the results of the ICE-CUB 2 study, a cluster-randomized clinical trial that studied 3036 critically ill older patients (aged ≥75 years) who were free of cancer and in good functional and nutritional state and who presented to 24 hospitals in France.8 The hospitals were randomized to a program that promoted systematic ICU admission based on consensus-derived criteria (n = 12 hospitals) or to usual care (n = 12 hospitals). The investigators’ concern was that under usual care, these patients frequently did not receive ICU care despite meeting consensus-derived criteria for admission. Based on prior work,6 the authors presumed the reasons included lack of education and poor communication between the referring emergency medicine physicians and the receiving intensive care physicians and poor planning when the ICU was already full. Therefore, the intervention was multifactorial, including formal education sessions, emphasizing to all staff the desire to admit all patients who met appropriate criteria, together with strategies to support and document discussions between emergency medicine and intensive care physicians and with patients and families; strategies to find ICU beds elsewhere when the ICU was full; and monthly audit and feedback. The primary outcome was 6-month mortality, and secondary outcomes included functional status, measured by the Index of Independence in Activities of Daily Living, and self-reported physical and mental health-related quality of life, measured by the 12-Item Short Form Health Survey (SF-12). Patients cared for at the intervention hospitals (n = 1518) were more likely to be admitted to the ICU than those cared for in the control hospitals (n = 1518) (61% vs 34%; relative risk [RR], 1.80; 95% CI, 1.66-1.95). Physicians, patients, and families also reported more favorable agreement with the decision to admit to the ICU at the intervention hospitals. However, mortality at both hospital discharge (30% vs 21%; P < .001; RR, 1.39; 95% CI, 1.23-1.57) and 6 months (45% vs 39%; P < .001; RR, 1.16; 95% CI, 1.07-1.26) was higher in the intervention group. The higher ICU admission and hospital mortality rates persisted after adjusting for baseline severity of illness, although the findings at 6 months were no longer statistically significant (RR, 1.05; 95% CI, 0.96-1.14). There were no differences in functional status or physical quality of life, although self-reported mental quality of life was slightly higher in the intervention group (mean SF-12 mental health component score, 44.6 [95% CI, 44.1-45.1] vs 43.7 [95% CI, 43.2-44.2]; difference in means, 0.9 [95% CI, 0.1-1.6]). The trial by Guidet et al demonstrates that a strategy designed to promote consistent ICU admission based on consensus-derived medical need successfully doubled the ICU admission rate for older patients yet yielded no benefit and possibly caused harm. Before considering the implications of these findings, it is important to consider some limitations involving the study design and interpretation, which are acknowledged by the authors. First, with randomization by cluster, in this case by hospital rather than by patient, the design can be hampered by imbalance in baseline patient characteristics between groups. Although the cluster (hospital) characteristics were well balanced, the patients in the intervention group had a higher severity of illness at baseline. The authors adjusted for these factors in secondary analyses, which still failed to demonstrate any benefit, but the possibility remains that additional unmeasured patient characteristics could have confounded the results. Second, to ensure adequate cluster size, the authors continued enrollment in usual care by several months, potentially opening the study to secular bias. Third, the intervention, like many health services and delivery interventions, involved a number of different elements and was not blinded. It is difficult to know whether particular aspects of the intervention had unintended consequences on physician decision making. Fourth, hospitals that traditionally used the ICU less frequently may have developed “workaround” strategies, such as provision of higher-quality or higher-intensity ward care. In such hospitals, efforts that boost ICU use may thus not improve patient outcome. Fifth, the patients studied were all older patients with a set of clinical characteristics deemed worthy of ICU admission. This cohort may be both too narrow and too broad—too narrow in that hospital-based interventions to promote change in the use of the ICU could have affected the care and outcomes of other critically ill patients who might otherwise have received ICU care; too broad in that the intervention presumably could improve outcome only in the one-quarter of patients who received ICU care in the intervention group yet received ward care in the control hospitals. Given that the study was powered for an overall 6% mortality difference, the effect in this marginal group would have to be 4 times that, which may be implausibly large. Many of these limitations are inherent to the study question and likely do not undermine the validity of the results. This well-conducted study by Guidet et al provides the first randomized data on the effects of ICU care for a broad range of critically ill elderly patients, an important question for resource-intensive health care systems worldwide. The lack of benefit is consistent with an earlier observational study from Paris, which similarly found no clear benefit to higher ICU use,6 though it is inconsistent with a recent observational study from the United States that suggested that greater access to ICU care benefited older patients with pneumonia.9 Thus, certainly in France, clinicians and policy makers may conclude that there is no pressing need to increase access to ICU beds, at least for older patients. However, a number of persistent questions arise. First, were there patients within the trial who were harmed by ICU care in ways that could be better predicted in the future? For example, did the intervention unintentionally suppress the better judgment of physicians regarding when to avoid ICU care by failing to capture key patient variables? Second, if ICU admission rates that are poorly adherent with consensus criteria yield equally good or better outcomes, then what criteria should be used to audit ICU use going forward? Third, are there beneficial practices on the general hospital ward that can be promoted and harmful practices in the ICU that can be eradicated? Moreover, for countries like Germany or the United States, with 3 to 4 times higher ICU bed supply,10 the findings from the trial by Guidet et al certainly support an argument for close examination of ICU admission decisions, with the potential to safely reduce ICU beds, care, and costs. In the study by Caram-Deelder et al, the authors analyzed data for 42 132 patients who received red blood cell transfusions, using 3 methodologic approaches. In the first approach, attempting to avoid dilution of the observed associations, the primary analyses were restricted to a “no-mixture” cohort of 31 118 patients who exclusively received red blood cell transfusions (n = 59 320) from either male (88%), never-pregnant female (6%), or ever-pregnant female (6%) donors. Patients who received blood from more than 1 type of donor were excluded. Patients who first received red blood cells from one type of donor and later received blood from another type of donor were censored from further follow-up. In the second approach, the analysis was restricted to 16 959 recipients of only a single red blood cell unit. The third approach involved a full-cohort analysis, in which all patients were included, irrespective of whether they received a mix of blood units from female and male donors. For all analyses, patients who received red blood cell transfusions from female donors with an unknown pregnancy status were excluded or censored. During follow up, there were 3969 deaths in the no-mixture cohort. Using Cox proportional hazards regression models, per-unit hazard ratios (HRs) for death were computed that compared recipients of blood from different donor types. In the no-mixture cohort, among female transfusion recipients, mortality did not differ according to donor sex and pregnancy status (ie, for receipt of blood from ever-pregnant female donor vs male donor: HR for mortality, 0.99 [95% CI, 0.87-1.13]); for never-pregnant female donor vs male donor: HR for mortality, 1.01 [95% CI, 0.88-1.15]). However, among male transfusion recipients, those who received blood from ever-pregnant female donors, compared with those who exclusively received blood from male donors, had an increased mortality risk (HR for mortality, 1.13 [95% CI, 1.01-1.26]; P = .03). This increased male recipient mortality was not observed with never-pregnant female donors (HR for mortality, 0.93 [95% CI, 0.81-1.06]; P = .29). Similar results were reported in the other 2 cohorts, although with consistently wider confidence intervals in the smaller “single-transfusion cohort” and with a smaller magnitude of association in the full cohort. Although the no-mixture and single-transfusion cohort approaches may avoid dilution of the association, excluding or censoring patients who received blood units from more than 1 type of donor—or, in the case of the patients in the single-transfusion group, were transfused again—is problematic, because the requirement for more transfusions indicates continued poor health and thus a higher possibility of mortality. Because red blood cell units from female donors contain about 8% less hemoglobin than those from male donors,6 patients who receive red blood cell units from female donors may require additional transfusions, which may alter the rate of censoring in such a way as to bias the mortality estimates. Consistent with this possibility, there were substantial differences in length of follow-up and number of deaths between patients who received red blood cells from men vs women. The extent to which this problem affects the results—including those attributable to any differences between never-pregnant and ever-pregnant female donors—is unclear. Independent replication of these findings using other data sources and preferably also different statistical methods is important. If the results reported by Caram-Deelder et al are confirmed, these finding would have major implications for the management of blood transfusions by blood banks and transfusing physicians. It is also important to understand the mechanisms underlying these observations. The mortality differences appear to increase even a year or more after the transfusion (Figure in the article by Caram-Deelder et al). This suggests that the observed findings, if true, are not related to differences in donor red blood cells or iron physiology but rather an immunologic mechanism based on maternal immunization to paternal antigens. Some findings in the study support an immunologic mechanism. For example, because of data limitations, 44% of the female donors could not be classified regarding their pregnancy status. These donors gave their first donation before the implementation of the current question (“Have you ever been pregnant?”). Consequently, the unclassifiable donors are likely to be older and further removed from their last pregnancy. However, details of the pregnancy history are not available. When analyzed separately, there was no association of red blood cell donation from these women with male recipients’ mortality (eFigure 4 and eTable 11 in the article by Caram-Deelder et al). It is likely that more than half of these older, unknown status donors had in fact been pregnant. That no association was observed could hypothetically be related to the longer length of time since their last pregnancy. For example, presence of HLA antibodies in female donors is strongly correlated with how recent the last pregnancy occurred.7 Such a mechanism should be explored in future studies. It is surprising that a hypothetical immunologic mechanism affecting male recipients of red blood cells donated by ever-pregnant donors was not also associated with increased mortality among female recipients of these red blood cells. In further analysis by the authors, tests for interaction to assess differences in the strength of the association between transfusion of red blood cells from ever-pregnant donors vs male donors and mortality among male vs female recipients  of all ages did not meet statistical significance; however, tests for interaction to assess differences in the strength of association for transfusion from ever-pregnant donors and mortality between male vs female recipients younger than age 50 years were statistically significant. As the authors acknowledge, these findings, suggesting significant effect modification by sex only when stratified by recipient age, suggest that the results are “very tentative,” and require validation in other studies. Nevertheless, a discussion of 2 possible explanations for the reported results may be helpful. First, the immunologic mechanism could involve only paternal Y chromosome epitopes. While this seems unlikely, such a phenomenon would limit the association between donor pregnancy history and male recipient mortality to female donors with male children. Furthermore, a significant risk factor for graft-vs-host-disease following adult and childhood stem cell transplantation has been reported for female-to-male transplants,9-11 which argues for a possible role of Y chromosome antigens. Second, as supported by the tests for effect modification cited above, the association may not even be limited to male recipients but may be more clearly expressed in male individuals. Two aspects of the data reported by Caram-Deelder et al further suggest this possibility. The results in the single-transfusion cohort (although not in the other 2 cohorts) are similar for female and male recipients (Table 2 and Figure in the article by Caram-Deelder et al), although only the differences for male recipients were statistically significant. Also, the stronger association observed in younger male recipients may reflect an additive or synergistic effect of androgens on a hypothetical immunologic mechanism. The latter hypothesis could also explain the differences between male and female recipients. Similar androgen-mediated differences in murine donor red blood cell hemolysis in response to a variety of in vitro stresses, and in vivo red cell survival differences after transfusion to syngeneic recipients, have recently been reported.12. A large prior study in Sweden and Denmark of 968 000 red blood cell recipients reported no association of female donor status of red blood cells with recipient mortality. However, this study did not stratify analyses by recipient sex and did not consider donor parity.3 Confirmation of the results reported by Caram-Deelder et al is necessary. Future studies should use more complete data on female donor pregnancy history and a similar study design, as well as alternative statistical methods less affected by the differential exclusion or censoring of patients who receive blood units from more than 1 type of blood donor. It would also be helpful to study populations with more ethnic and racial diversity, given the relative genetic homogeneity of the Dutch population. In light of the limitations of the study by Caram-Deelder et al, current criteria for blood donor selection should not change. However, additional investigation is needed. If confirmatory studies in other cohorts demonstrate that a donor’s sex and pregnancy status are associated with posttransfusion mortality, blood centers and transfusion services will need to mitigate this risk. Potential options could include identification and deferral of high-risk donors (assuming a specific mechanism or relevant test can be identified, because the blood supply cannot be maintained if red blood cell donors with a pregnancy history are deferred), donor-recipient sex matching to avoid increased risk, and mitigation of underlying mechanisms by blood product modifications such as further reducing residual plasma or lymphocytes (if these are implicated in an immunologic mechanism). In the meantime, appropriate and conservative use of blood products continues to be the gold standard for safe transfusion. Still, the mechanisms underlying these findings are difficult to disentangle and have different implications. Among many possible explanations, at least 4 merit consideration. First, there may be a true assimilation of values and preferences over time in a new country. If so, then the larger differences in care observed among more recent immigrants would not be problematic, but may instead reflect uniformly goal-concordant care for patients and families with different underlying objectives. Prior research on regional variation in the intensity of end-of-life care in the United States has suggested that most variation stems from unwarranted differences in clinician behavior rather than differences in patients’ preferences.7-9 However, such studies have focused on care variation for well-characterized patients across regions, and thus yield findings that may not apply to the current comparison of within-region differences among immigrants vs long-standing residents. Third, with greater time in the country, immigrants may develop greater abilities to communicate their preferences, such that goal-discordant care is progressively reduced. Although the authors adjusted for whether immigrants were fluent in English at the time of immigration, this coarse dichotomization of linguistic ability is unlikely to capture the more nuanced aspects of communication required for high-quality family meetings regarding goals of care. If true, this explanation would suggest a true disparity in care delivery for immigrants. Mitigating this disparity would require development and testing of an intervention, such as ensuring greater availability of translators or more routine involvement of translators earlier in the courses of care for immigrants. These interpretive challenges do not diminish the importance of the work by Yarnell and colleagues.3 The first of 3 steps in health disparity research is to detect differences in care processes or health outcomes.10,11 Only then can research begin to explore whether the observed variation is attributable to true (ie, unwarranted) disparities in care and to eventually develop interventions to mitigate such disparities. Following this framework, the authors have helped to identify key needs for future research on intensive and palliative care. Future studies that are able to account for differences in patients’ preferences, advance care planning, and other factors may help to identify which among several potential mechanisms explain the observed differences in care provided to immigrants and native residents near the end of life. Until such time, it is premature to suggest changes in policy or practice. First, the incidence of observed hypoglycemia depended on the definition of hypoglycemia in each study. Definitions of hypoglycemia were variable across studies. Second, diabetes duration and its association with the outcomes were not evaluated in the review. Third, many of the trials excluded individuals with low estimated glomerular filtration rates. Fourth, none of the included trials evaluated the addition of sodium-glucose–linked cotransporter 2 to insulin monotherapy. Fifth, adding an oral hypoglycemic agent to insulin is not the same as adding insulin to an oral hypoglycemic agent; therefore, the results should be interpreted accordingly. Major recommendations  (1) A diagnosis of gender dysphoria/gender incongruence (GD/GI) should be confirmed before proceeding with gender-affirming hormone therapy (strong recommendation; moderate evidence). (2) Clinicians should evaluate and address medical conditions that can be exacerbated by hormone depletion and treatment with sex hormones of the affirmed gender before beginning treatment (strong recommendation; moderate evidence). (3) Clinicians should counsel regarding options for fertility prior to initiating puberty suppression and hormone therapy (strong recommendation; moderate evidence). (4) Adolescents, when indicated, should be offered gonadotropin-releasing hormone (GnRH) analogs to suppress pubertal hormones (strong recommendation; low evidence). (5) Hormone therapy can be started in adolescents after a multidisciplinary team of medical and mental health professionals (MHPs) has confirmed the persistence of GD/GI and sufficient mental capacity to give informed consent (strong recommendation; low evidence). (6) Clinicians should refer hormone-treated transgender individuals for genital surgery when an individual (a) has had a satisfactory social role change; (b) is satisfied about the hormonal effects; and (c) desires definitive surgical changes, and after the MHP and the clinician responsible for endocrine transition therapy both agree that surgery is medically necessary and would benefit the patient’s overall health and well-being (strong recommendation; very low evidence). Transgender individuals have a gender identity that does not align with their sex assignment at birth (gender incongruence). They account for about approximately 0.4% of the US population (1 million people).1 Some transgender people experience gender dysphoria, distress caused by the discrepancy between gender identity and birth-assigned sex, linked to lifetime suicide attempt rates of 40%.2 Some may start to live or transition into their affirmed gender. This may include social or legal transition, such as changes to attire, name, and pronouns. Some may request hormones, surgery, or both to diminish the secondary sex characteristics associated with their birth sex. Children diagnosed as having GD/GI may start GnRH agonists after beginning puberty to prevent permanent changes, including facial hair, deepening of the voice, and breast growth. Thereafter, they may initiate hormones to acquire the secondary sex characteristics of their affirmed gender. Prospective studies suggest these interventions are associated with improved psychological functioning and reduction in anxiety and depression.3. The guidelines were developed following the GRADE system. The task force commissioned 2 systematic reviews to evaluate the effect of hormone therapy on (1) cardiovascular health and lipids and (2) bone health. Consensus recommendations were rated as strong or weak and quality of evidence as high, moderate, low, or very low, with few of the recommendations based on high-quality evidence. The task force made 28 recommendations addressing social transition for prepubertal youths, use of GnRH agonists for pubertal youths, gender-affirming hormone therapy for youths and adults, surgery, and primary care/preventive health. Significant changes from the 2009 guidelines include updated language to reflect changes in the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition, and cultural shifts. Use of the term clinician instead of endocrinologist recognizes that nonspecialists are increasingly delivering gender-affirming medical interventions. Substantive changes address the competencies of MHPs, such as the ability to distinguish between GD and other conditions such as body dysmorphic disorder. The 2017 guidelines have greater flexibility, stating that, after careful assessment, it may be appropriate to initiate hormone therapy before age 16 years and that for transgender men, the decision to undergo mastectomy may occur before age 18 years. There is greater recognition that some people may not wish to use hormone therapy, and this step is removed as an expectation before genital surgery. Changes to hormone dosing include the option to use subcutaneous rather than intramuscular testosterone; a reduction in the maximum dosage of the transdermal estradiol patch; changes in recommended maximum dosages of estradiol valerate and cyproterone; and addition of the 1.6% testosterone gel. Screening recommendations for cardiovascular disease, bone health, and prostate cancer were also updated. The 2017 ES guidelines are an important tool but they do not address access to care issues. Transgender people face significant health disparities related to stigma, discrimination, and lack of access to culturally competent care. Few physicians receive adequate training in transgender health during medical school.7 Approximately 70% of transgender people have encountered discrimination in medical settings.8 One in 4 report avoiding needed medical care owing to such issues.2 Best practices address creating an affirming clinical environment, including clinic practices such as having registration/intake forms that capture gender identity and sex assigned at birth, readily inquiring about and consistently using the correct name and pronoun, awareness about gendered language, ensuring the presence of single-use or gender-neutral restrooms, not making assumptions about sexual orientation or sexual behaviors, and gaining knowledge in transition-related care.9 The guideline addresses initial and maintenance dosages of hormones, appropriate laboratory monitoring, and medical risks with hormone therapy. While there is a moderate learning curve, transgender care can be well within the competency of motivated primary care clinicians, most of whom already prescribe hormones and make surgical referrals for both men and women as part of usual care. Resources to assist clinicians and patients with gender-affirming referrals, completion of documentation for name/gender change, access to care, and protocols to appropriate gender-affirming treatments are listed in the Box. A 2011 Institute of Medicine report highlighted the limited transgender research in all domains.10 Testosterone and estrogen therapy have been used for medical transition since the 1930s, yet there are few high-quality studies with long-term outcomes to support preventive care recommendations and assess outcomes such as cardiovascular disease and cancer risk among transgender people. Many of the studies have been conducted in Europe, where hormone protocols differ from the United States, limiting generalizability of results to a heterogeneous US population. Few data exist on health outcomes of nonbinary transgender people. Optimal dosing and routes of hormones as well as optimal surgical techniques have not been rigorously evaluated and should be assessed through prospective studies or clinical trials. Physical examination revealed 7 sharply marginated, irregularly shaped, polymorphic hyperpigmented macules of 10 to 15 mm in diameter, without scales, on the back and thighs. The hyperpigmented macules were of orange hue, with unevenness of the pigmentation (Figure 1). Vital signs were normal. There was no lymphadenopathy or hepatosplenomegaly. Laboratory test results, including complete blood cell count, liver function tests, and renal function, were normal. The key clinical feature to the correct diagnosis in this case is the sharply marginated, nonscaly hyperpigmentation consistent with mastocytosis. In mastocytosis, within a few minutes after stroking a hyperpigmented lesion, the proliferated mast cells release histamine, leading to the transitory reactions of itching, erythema, wheal formation, or blistering on the lesion, a phenomenon known as Darier sign.1 Epidermomycosis (choice A) is unlikely without scales. Fixed drug eruption (choice B) can leave persistent, well-demarcated hyperpigmentation; however, the prodromal inflammatory stage was not present in this case. In café-au-lait macules as seen with neurofibromatosis 1 (choice C), the pigmentation has typically more of a brown tint than orange and it is uniform throughout the macule. The Darier sign was positive (Figure 2), and a skin biopsy confirmed the diagnosis of mastocytosis. The serum tryptase concentration was within normal limits. No signs or symptoms of systemic involvement were present, and bone marrow biopsy was not performed. The patient’s mother was counseled on measures to prevent mast call degranulation as described above. The number of children potentially affected is not likely to be small. Proton pump inhibitor  prescriptions among infants younger than 1 year increased 4-fold from 1999 to 2003, and 7.5-fold from 1999 to 2004 when partial data from 2004 were included, according to a 2007 study. This despite the fact that, at the time, no PPI was approved by the US Food and Drug Administration (FDA) for any use in children younger than 1 year. More recently, esomeprazole and omeprazole have been approved by the FDA for use in infants aged 1 month to 1 year but only for erosive esophagitis due to acid-mediated gastroesophageal reflux disease (GERD), a far more serious condition than GER. The evidence tying acid suppressors to bone deficiency comes from the first-of-its-kind retrospective cohort study of 874 447 children without diagnosed GERD born within the Military Health Care System (MHS) from 2001 to 2013. All had received follow-up MHS care for 2 or more years, with a median of 5.8 years and a range of 3.6 to 9.1 years. Outpatient pharmacy data from the first 6 months of life identified prescriptions for PPIs in 6943 infants, H2 blockers in 67 096, and both in 10 777, or about 10% of the entire cohort. The researchers used the International Classification of Disease, Ninth Revision, codes to identify fractures after 6 months of age and calculated hazard ratios adjusting for confounding factors including sex, prematurity, and low birth rate. It is important to distinguish between GER and the much more corrosive GERD, which needs treatment with acid reducers, at least in the short-term, she explained. “If you use an endoscope on a baby with classic GERD markers, you can see esophagitis. There is erosion, changes in the lining. There may be blood in the spit. They may stop wanting to eat and then lose weight. Swallowing becomes hard. So it can cause problems. But less than 5% of babies have GERD.”. Lightdale admits she is taking a wait-and-see attitude about the MHS study. “This is just one more association study. Causality remains elusive,” she said, referring to previous observational studies linking PPI use to bone fracture in adults and the elderly. “But that doesn’t mean you should ignore the drumbeat. It is possible that the era of considering these to be scot-free medications where you need not worry about side effects is over,” she said. Marketed as Mylotarg, the drug is now indicated for adults with newly diagnosed acute myeloid leukemia (AML) whose tumors express the CD33 antigen (CD33-positive AML). Approval also was granted to treat pediatric patients aged 2 years or older with CD33-positive AML who have relapsed or haven’t responded to initial therapy. The CD33 antigen is expressed on AML cells in up to 90% of patients, according to a statement from the manufacturer, Pfizer Inc. Gemtuzumab ozogamicin consists of an antibody that binds with the CD33 antigen and kills cancer cells by releasing the antitumor agent calicheamicin into them. Diagnostic radiologist Kathy Schilling, MD, medical director of Boca Raton Regional Hospital’s Christine E. Lynn Women’s Health and Wellness Institute in Florida, said the system launches “a new age in breast imaging.” Schilling conducted a clinical review of the device. “Patients who used the remote control said the exam was more comfortable and they were visibly more relaxed,” Schilling said in a statement. “Any breast radiologist knows that when patients are relaxed, we are able to get better images and better images lead to a more confident diagnosis.”. The single-transfusion cohort consists of all the follow-up time during which patients had received only a single transfusion. Follow-up time was censored at the time this inclusion criterion was violated. Median follow-up for male recipients of red blood cell transfusions exposed to male donors, 16 (interquartile range [IQR], 1-780) days; to female ever-pregnant donors, 12 (IQR, 1-567) days; and to female never-pregnant donors, 14 (IQR, 1-563) days. Median follow-up for female recipients of red blood cell transfusions exposed to male donors, 24 (IQR, 1-846) days; to female ever-pregnant donors, 21 (IQR, 1-584) days; to female never-pregnant donors, 17 (IQR, 1-640) days. Quiz Ref IDPrimary analyses were performed in a “no-donor-mixture” cohort, to avoid dilution of effects from mixing patients who received red blood cell transfusions from both male and female donors. This cohort consisted of patients who received all their red blood cell transfusions exclusively from male donors, or all exclusively from female donors without a history of pregnancy (never-pregnant donors), or all exclusively from female donors with a history of pregnancy (ever-pregnant donors). Follow-up time was censored at the time these inclusion criteria were violated. This censoring could occur at time 0, in which case recipients contributed zero follow-up time and were not included in the denominator. Similarly, a “single-transfusion” cohort also was selected, consisting of patients who received only a single transfusion. Additionally, all analyses were repeated in the full cohort, to check whether any observed association potentially depended on the selection of the no-donor-mixture cohort. The race/ethnicity of recipients and donors was not recorded. Information on transfusion recipients’ dates of birth, dates of death, and sex, as well as transfusion dates, product types, and identification codes of transfused red blood cells, were provided by the hospitals from electronic records of the blood transfusion services. All transfusions, given for any indication, were included. Mortality data were verified by the hospitals until the date of data extraction. Mortality data were considered complete because of the use of a nationally linked computer system and the legal requirement for reporting all deaths to this system. The final follow-up date was September 1, 2015. At their first donation, female blood donors self-reported any previous pregnancy. At all subsequent donations, they reported whether they had been pregnant since the previous donation. However, since some female donors had their first-ever donation prior to the establishment of the current electronic recording system at the Sanquin blood bank, the answer to the question at first donation could be missing. When the first donation was registered and the question answered as never-pregnant, the pregnancy status was considered never-pregnant until the first subsequent donation at which a pregnancy was reported. If the first donation was missing, the pregnancy status was considered unknown until the first subsequent donation at which a pregnancy was reported. Survival analyses were performed with follow-up starting on the day of the first red blood cell transfusion. Follow-up ended at death or on the reference day, determined for each hospital separately (eTable 1 in the Supplement). The reference day was the last day for which the hospital had provided data. Follow-up time of recipients in the different cohorts was censored at the time the inclusion criteria for that cohort were first violated. To increase homogeneity, follow-up time of patients who received more than 15 transfusions was censored at the time of the 16th transfusion. All analyses were stratified by recipients’ sex. Transfusions of other blood products were ignored, because receipt of these blood products was not correlated with sex and pregnancy history of the donor of red blood cells (eTable 3 in the Supplement). Exposures (ie, cumulative number of transfusions from never-pregnant or ever-pregnant female donors [time-varying]) were included in the models as continuous variables. Consequently, hazard ratios (HRs) should be interpreted on a multiplicative scale. However, since the model estimates the HR based on observed numbers of transfusions only, the HRs should not be extrapolated beyond the observed mean number of transfusions in each cohort (see eTable 3 in the Supplement for an illustration of this interpretation). The proportional hazards assumption was checked for all models and no gross violations of this assumption were detected, implying that the HR can be interpreted as a valid estimate of the average HR over the observed period. Separate models were run for the 2 different exposures (ie, never-pregnant and ever-pregnant). For the no-donor-mixture cohort, this meant exclusion of patients who received any transfusions from the other exposure group, any transfusions with unknown pregnancy history, or a mixture of exposed (ie, ever-pregnant or never-pregnant, depending on the analyses) and unexposed (ie, male) red blood cell units. This way, the exposure group of interest was always compared directly with male donors, since all other units were excluded. For the full model, recipients of transfusions both from the exposure group of interest and from male donors were additionally included. A total of 42 132 patients received 106 641 units of red blood cells (76% from male donors, 12% from ever-pregnant donors, 12% from never-pregnant female donors). The median number of transfused units per recipient was 2 (interquartile range [IQR], 2-3). These recipients were followed up for a median of 380 days (IQR, 27-1217), had a median age of 66 years (IQR, 46-77), and 21 915 (52%) were female. The number of deaths was 6975 (17%). Among this full cohort, 31 118 patients received 59 320 units of red blood cells exclusively from 1 of the 3 types of donor (ie, the no-donor-mixture cohort: either all units exclusively from male donors, exclusively from female donors without a history of pregnancy [never-pregnant donors], or exclusively from female donors with a history of pregnancy [ever-pregnant donors]). These recipients were followed up for a median of 245 days (IQR, 9-1172) and had a median age of 65 years (IQR, 42-77); 16 123 (52%) were female. The number of deaths in the no-donor-mixture cohort was 3969 (13%). Table 1 shows a comparison of recipient characteristics between the no-donor-mixture cohort, single-transfusion cohort, and full cohort, stratified by recipient sex. Cumulative incidences of death, in the single-transfusion cohort, at different follow-up times, are reported in eFigure 3 and eTable 5 in the Supplement. eTable 3 reports the distribution of donor types according to recipient sex and plasma and platelets transfusions received. Data on numbers of recipients, transfusions, and deaths per subgroup—also for all female donors combined, regardless of pregnancy history—are reported in eTable 2 and eTables 6-8 in the Supplement. Results of analyses of red blood cells corrected for plasma and platelet transfusions are reported in eTables 9-10 in the Supplement. Results of analyses for female donors with unknown pregnancy history are shown in eTable 11 and eFigure 4 in the Supplement. A direct comparison between ever-pregnant and never-pregnant female donors is reported in eTable 12 in the Supplement. Analyses of platelet transfusions are reported in eTables 13-14 in the Supplement. The association of increased mortality among male patients who received transfusions from ever-pregnant donors suggests a possible mechanism based on immunologic changes occurring during pregnancy. Of all changes occurring during pregnancy, the immunologic ones are the most enduring. An alternative explanation could be a difference in iron status between ever-pregnant female and male donors. Iron deficiency in donors has recently been shown to be associated with worse recovery of red blood cells after transfusion in a murine model.17 Some studies also report differences in red blood cell physiology between the sexes.13-17 Results from studies on the association of donor sex and recipient mortality, including the current study, tend to be consistent in showing associations for male recipients but not for female recipients.2-6 This specificity for male recipients seems difficult to explain based on differences in red blood cell physiology, supporting a possible role for a sex-specific immunologic mechanism. It is difficult to predict whether the small amount of plasma in red blood cell transfusions contains enough antibodies to confer an increased risk of mortality, but it cannot be ruled out. Furthermore, leukocyte-depleted red blood cell transfusions routinely contain fewer than 1 million leukocytes. However, to allow for naturally occurring variation, quality control standards allow up to 5 million leukocytes in a small percentage of products. These could include both antigen-specific lymphocytes or regulatory T cells. This study has several strengths. The large size of the cohort allowed selection of the no-donor-mixture cohort and enabled study of patients who received blood transfusions from only 1 type of donor (ie, male vs previously pregnant female vs never pregnant female). However, the selection of a no-donor-mixture cohort could limit generalizability. The recipients in the no-donor-mixture cohort receive fewer transfusions, since the probability of receiving mixed transfusions increases with the total number of transfusions. Similarly, the censoring of recipients who received 16 or more transfusions could limit generalizability to this group. This study also has limitations. First, the difference in effect size and direction between male and female recipients was not significant among recipients of all ages, only among those 50 years and younger. This makes the findings very tentative, and they require validation in other studies. Second, this study was retrospective, and data were recorded for routine clinical practice and not specifically for this study. This could cause both inaccuracy of data and unavailability of data. Third, there were missing data, particularly regarding pregnancy status for the women donating red blood cells. Fourth, information on cause of death was not available. Fifth, there may have been residual confounding or confounding by an unidentified variable. Sixth, the analysis included a large number of comparisons, but there was no adjustment for multiple comparisons. Nearly 3 decades ago, David Axelrod, MD, then commissioner of the New York State Department of Health (DOH), became concerned by the nearly 5-fold variation in cardiac surgery mortality rates across New York hospitals. Out of this concern, the DOH released the first public reports of surgical outcomes to the New York Times, labeling the performance of hospitals on cardiac surgical mortality rates across the state. After learning that individual surgeon mortality rates were also available, the Long Island–based Newsday sought release of the mortality rates of individual surgeons. The state demurred, arguing that data were less reliable at the surgeon level. Newsday quickly filed a Freedom of Information Act lawsuit to obtain the information and eventually prevailed in court, partly on the argument that patients generally choose surgeons, not hospitals. This debate—whether it is best to report data on the hospital level only or whether to also report on individual surgeons—continues to this day. The evidence on whether the public or payers use these data to select high performers suggests that for the most part, they do not. But we do know that hospitals and physicians are paying attention. And there is widespread hope that with increasing consumerism in health care and with more patients paying out of pocket for their care, they will increasingly look for these kinds of reports. That’s part of the reason public reporting is now so pervasive. While public reporting continues to become more common, how to report—whether to include only institutions or to also report on individual physicians—remains controversial. This debate was recently renewed when ProPublica, a highly regarded independent and nonprofit investigative journalist group, used national Medicare data to publish complication rates for hospitals and individual surgeons. It was their decision to name individual surgeons that was most controversial. Critics of individual surgeon reporting have made a series of important arguments against the practice, each of which is worthy of consideration. And as more states and the federal government continue their efforts towards public reporting, these arguments against naming individual surgeons are gaining traction. But I believe that a careful examination of the literature indicates that moving away from surgeon-level reporting would be a critical mistake. One common argument is that volumes of surgery for individual surgeons may be too low to be reliable. Small sample sizes can be a problem, but there are ways to address this issue. First, as New York State has done with cardiac surgery, one can aggregate performance across multiple years, thus substantially increasing sample size. Second, given that total volume of similar surgeries is also related to performance on a given surgery (for example, a surgeon’s performance on esophagectomy improves with the number of other similar surgeries she performs), one could use a surgeon’s performance across a range of procedures to increase sample size. Finally, publicly reported data could and should be presented with confidence intervals, to highlight the level of imprecision so that those reading the report are aware of the statistical limitations. Critics of naming individual surgeons also argue that the practice would increase the likelihood that surgeons will avoid the hardest cases, thus reducing access to surgical care for the sickest patients. This is an important concern that has been voiced widely. However, the evidence on the extent to which this occurs is weak and anecdotal. Quite a few studies have tried to examine whether publicly reporting individual surgeon data leads to sicker patients not receiving care, and most have failed to find an effect. And to the extent that a few studies have found a reduction in access, these have been transient, with access returning to baseline within a couple of years of the reporting program. Finally, and possibly most importantly, people who oppose reporting findings for individual surgeons have argued that much of medical care is now delivered in teams, and that each of the team members matters. They argue that naming an individual surgeon puts undue emphasis on that surgeon when the team’s performance is paramount. This argument is understandable; the importance of teamwork on surgical care is increasingly recognized. As the leader of the team, the surgeon plays a unique and outsized role, responsible for ensuring effective communication and well-delineated roles among team members, and for fostering a culture that supports team members speaking up when things have gone wrong. So when we publicly report a surgeon’s performance, we are essentially reporting on his or her surgical team’s performance. A second critical step in determining outcomes is the surgical technique itself. Recent studies have shown that there are large differences in basic surgical capabilities (from dissecting to cutting to suturing), and that highly skilled surgeons have dramatically better outcomes than less-skilled ones. The surgeon’s hands still matter. There are likely other factors that matter as well, but the empirical evidence is clear: picking the right surgeon is at least as important as picking the right hospital. And that brings us to what is probably the most important reason to report data on individual surgeons: it’s information that patients want. When people seek advice on where to go for surgical care, they ask about the best surgeon for them. Telling the public that the surgeon doesn’t matter, that only the choice of  hospital matters, is neither useful nor accurate. And it won’t work, because for patients, undergoing surgery creates deep anxiety, and trusting the judgment of an individual surgeon is paramount. Consumers would reject reports that only showcase institution-level data because they would find it less useful. Publicly reporting on outcomes of individual surgeons is a high-stakes endeavor. We know from studies that some surgeons stopped practicing or moved away as a result of the New York State cardiac surgery reporting. There are still lingering fears that surgeons will turn away risky patients, although most surgeons have enough professionalism that they are unlikely to deny someone life-saving care just because it might hurt their rankings. But these fears mean we need to work harder at risk adjustment, and possibly create safe harbors for cases that are the highest risk. But the surgeon remains critically important to surgery, and suggesting that what matters is the institution, not the individual, is both empirically inaccurate and unlikely to resonate with patients. We must address the challenges that come with individual surgeon-level reporting—not shy away from doing it. We must expand our efforts to report performance publicly, including for individual surgeons, not curtail them. Because if the goal is to improve surgical care, we must ensure that the person most influential in the process remains accountable for its outcome. In the first study, the international team of investigators analyzed data from the Global Burden of Diseases, Injuries, and Risk Factors Study 2015 (GBD 2015) to estimate the burden of mental disorders in the EMR, which comprises 22 countries including Afghanistan, Iran, Pakistan, Somalia, Sudan, Syria, and Iraq, and is inhabited by more than 600 million people. Overall, EMR countries had a higher burden of mental health disorders compared to global levels. Depression and anxiety disorders contributed most to the mental health burden. Women had a higher burden of mental disorders than men. An increasing mental health burden is mainly attributable to population growth and aging. A severe shortage of psychologists and psychiatrists in the region presents a challenge to epidemiological surveillance of mental disorders and adequate prevention and treatment services. In the second study, the same team of investigators analyzed data from the GBD 2015 to estimate the mortality, morbidity, and burden of intentional injuries in the EMR. They found that suicide, homicide, and sexual assault are increasing much more rapidly in the EMR than in other parts of the world. Between 1990 and 2015, the countries with the highest mortality rates from suicide and interpersonal violence—Afghanistan, Somalia, Djibouti, and Iraq—experienced many episodes of conflicts and social unrest, as well as terrorism. The findings collectively support the need to increase efforts aimed at stabilizing the region. This article explores the scope and variety of fraudulent conduct in health services and discusses emerging anticorruption tools and interagency frameworks that could limit corruption. Data and technology can play a big role in improving transparency and accountability. Online social network platforms can be used as educational tools to increase public awareness about corruption as well as to increase citizen participation in monitoring and reporting of corruption. Efforts aimed at analysis and mining of large reimbursement data sets can also detect health care fraud. Even though they declined and fell, the ancient Romans were remarkably well organized. Roman youth, like youth everywhere, tried to get ahead in the world, develop their abilities, acquire renown, work themselves into positions of power and control. In ancient Rome, politics and the army offered the greatest opportunities for fame and wealth, desiderata for which there is always great competition. Hence, Roman society—like society everywhere—had the function of promoting the competition and at the same time limiting the areas in which it could take place. Among the Romans, both the military and political scenes had definite regulations which controlled the stages through which rise could occur. There were sharp class distinctions, with different pathways for different social levels. Young men of the highest rank—the families of senators who possessed adequate financial resources—had to serve first in certain minor offices; then, if at least 25 years of age, they could become quaestor. After a year they were then eligible to be aedile, then praetor, and finally consul. In the first group, a young man must begin by serving a five-year residency and then take certain examinations which qualify him as a specialist. Then he must publish a certain number of papers, to achieve the rank of assistant professor in a medical school. Then he gains a progressive number of research grants. These represent an intricate exchange whereby, in return for certain sums of money, the recipient pays in kind, through nonnegotiable paper called a “publication.” A sufficient number of these will lead to successive advancement, through the grade of associate professor to professor. However, since the ranks of professors are heavily crowded, further rise depends on acquisition of other titles, such as president of learned societies, director of research foundations, consultant, commissioner, chancellor, and other positions of power. A final cursus has different stages and different goals. The members may serve only a limited residency, do not necessarily become specialists, do not seek grants, do not publish papers, do not hold academic positions. Their rise is measured principally by an intangible, which future historians may find difficult to analyze. It is now known as “the esteem of their patients.”. Codeine and tramadol are opiates, which are a group of medications generally used to treat pain. Codeine also acts as a cough suppressant and is found in some cough and cold medications. Although both codeine and tramadol are effective for treating pain and cough, like other opiates, they can have serious side effects. The main side effect is sleepiness, which can range from mild to extreme. In cases of overdose, these medications can slow breathing or cause it to stop altogether, which can be life-threatening. As with other opiates, there is also a risk of becoming dependent on the medication with repeated use. In April 2017, the FDA issued a new warning that recommends against use of codeine and tramadol in children younger than 12 years. The FDA also warns against use in breastfeeding women because of possible harm to infants. For teens aged 12 to 18 years, the FDA warns against using codeine and tramadol if there is a history of obesity, obstructive sleep apnea, or severe lung disease. In particular, neither codeine nor tramadol should be given to children or adolescents as a pain medication after surgery to remove the tonsils or adenoids. To treat pain, acetaminophen (Tylenol) and ibuprofen (Advil or Motrin) are safe for children when taken in the correct weight-based dose. Ibuprofen should not be given to infants younger than 6 months. To treat cough, natural remedies such as steam, extra fluids, honey (for children older than 1 year), and gargling (for older children) are safe options. Cough medications are generally not recommended for children younger than 4 years. Ask your pediatrician for more information about safe alternatives to codeine and tramadol in children. To the Editor A randomized clinical trial of women with stress urinary incontinence found that electroacupuncture reduced urinary leakage compared with sham electroacupuncture.1 However, the electroacupuncture used in the study was not a typical acupuncture treatment. First, the authors did not make a traditional Chinese medicine diagnosis to determine the reason for the urinary leakage. A traditional Chinese medicine diagnosis is important to decide the acupuncture points to be used. If participants had a traditional Chinese medicine diagnosis of kidney deficiency or lung deficiency, more than 2 acupuncture points should have been used, but the authors only used 2 acupuncture points. The acupuncture interventions in our trial were developed by a consensus of acupuncture experts and based on the results of a previous study.1 Acupuncture is part of traditional Chinese medicine, and these acupuncture experts fully understand the theories of traditional Chinese medicine. The acupoints used in the trial belong to the bladder meridian, which, according to traditional Chinese medicine theory, controls the functions of the urinary bladder. Rather than 2 acupoints being used, we actually used 4 acupoints (bilateral BL33 and BL 35) of the urinary bladder meridian for treatment in the trial. Nonetheless, we agree with the uniqueness of acupoint selection in this trial: electroacupuncture may stimulate S3 via BL33 and the pudendal nerve via BL35 at the lumbosacral region. The lack of correlation between improvements in urinary leakage and in the number of participants using urine pads could be explained by the relationship between quantitative and qualitative data. The percentage decrease in urinary leakage does not necessarily equal and is not necessarily accompanied by the same degree of decrease in the number of participants using urine pads. In this trial, the quantitative urine leakage data were used for primary outcome assessment, and the qualitative data on the number of participants using urine pads were used for secondary outcome assessment. The results of the 2 assessments were in the same direction in the treatment group. Forest plot analogous to Figure 1 depicting the ratio of the proportions of decedents dying in intensive care units comparing recent immigrant with long-standing resident cohorts (relative risk). In contrast to Figure 1, this figure focuses on subgroups defined only among the recent immigrant cohort including region of origin, language ability on arrival, education level on arrival, immigration class, and time since immigration. The proportion of recent immigrant decedents dying in intensive care within each subgroup is compared with the proportion of long-standing resident decedents dying in intensive care (92 270 of 919 499 [10%]), and so a relative risk greater than 1 corresponds to an increased relative risk of recent immigrant compared with long-standing resident decedents being in the intensive care unit at death. Note that percentages are based on the size of each subgroup by row, not based on the overall analytic sample size for recent immigrant decedents of 47 514. The size of each square is proportional to the precision of the relative risk estimate. All individuals (recent immigrants and long-standing residents) who died in Ontario between April 1, 2004, and March 31, 2015, were identified. Individuals with fewer than 6 months of enrollment in the provincial health care plan were excluded. Data on individuals who received care in Ontario but died in another country or province were not available. Patients with some missing baseline data were included in unadjusted analyses but not in adjusted analyses requiring a missing variable. Recent immigrants were identified within the data set through previously validated combined probabilistic and deterministic linkage of the list of deceased individuals to the registry of landed immigrants maintained by Immigration, Refugees and Citizenship Canada.16 Deterministic linkage occurs when 1 or more identifiers (eg, health card number and name) are identical, while probabilistic linkage uses probability scores to identify linkages among records where deterministic linkages were not possible. Recent immigrants were defined as those granted permanent residency or citizenship status in Canada between 1985 and 2015 (the years available in the Immigration, Refugees and Citizenship Canada data) and created subgroups according to duration since immigration. All other residents were defined as long-standing residents. Other research has reserved the term recent for immigrants arriving within shorter timeframes, but this broader definition sought to include all available data and acknowledge that some members of the long-standing resident cohort may also be immigrants but have lived in Canada for more than 30 years. Immigration, Refugees and Citizenship Canada data also included information obtained at the time of immigration application on immigration class (economic, family, refugee, and other), education level, language ability, and country of birth. Information on the level of health literacy, religion, and specific cultural practices was not available. The primary outcome described end-of-life care according to location of death: intensive care unit, acute care hospital, long-term care facility (or nursing home), and other (including hospice or home). The results are described in terms of relative risk (RR), which in this case refers to the ratio of proportions of recent immigrant compared with long-standing resident decedents that experienced a given outcome. Secondary outcomes assessed whether a patient experienced intensive or invasive interventions in the last 6 months of life including hospital admission, intensive care admission, mechanical ventilation, dialysis, percutaneous gastric/gastrojejunal tube, or tracheostomy. Other secondary outcomes included emergency department, hospital, and intensive care use in the last 6 months of life. Separate modified Poisson regression analyses of location of death (intensive care unit, acute care hospital, long-term care facility, or other including home) were conducted among recent immigrants compared with long-standing resident decedents to estimate RRs.20 We also performed separate modified Poisson regression analyses of type of invasive care received in the last 6 months including hospital admission, intensive care unit admission, mechanical ventilation, dialysis, percutaneous feeding tube placement, and tracheostomy. All analyses adjusted for potential confounders of age, sex, income quintile, urban-rural residence, and cause of death. To account for the correlation of outcomes among patients residing within the same geographic area, the analysis implemented generalized estimating equations using an exchangeable correlation structure, clustering by postal code.20 Recent immigrants were separately analyzed according to region of birth, years in Ontario, language ability on arrival, education level on arrival, and immigration class while adjusting for the same covariates as above. Two-sided P values less than .05 were considered significant but were not adjusted for multiple comparisons. Analyses were performed with SAS Enterprise Guide 6.1 (SAS Institute) and R 3.2.2 software (R Foundation). Details of the analysis protocol, regional definitions, causes of death, and further analyses assessing robustness across multiple fixed intervals preceding death can be found in eAppendix 1 in the Supplement. A total of 967 013 decedents were analyzed, of whom 47 514 (5%) immigrated since 1985. Recent immigrant decedents originated from diverse global regions (eTable 3 in the Supplement). The median age at death was 79 years, with ischemic heart disease, lung cancer, and dementia being the most common causes of death. Compared with long-standing resident decedents, recent immigrant decedents tended to be younger and more likely to live in an urban area and of lower socioeconomic position (Table 1). The median duration in Canada for recent immigrants was 16 years. The RR of death in intensive care comparing recent immigrant and long-standing resident decedents was highest among patients older than 80 years, female patients, and patients with a lower comorbidity index (Figure 1). There was substantial variation in end-of-life care according to region of birth and time since immigration (Figure 2). The RR of dying in intensive care (using the overall long-standing resident risk of dying in intensive care as baseline) ranged from 0.84 (95% CI, 0.74-0.95) among decedents born in northern and western Europe to 1.78 (95% CI, 1.66-1.92) among decedents born in western and central Asia, 1.84 (95% CI, 1.70-2.00) among decedents born in Africa, and 1.96 (95% CI, 1.89-2.05) among decedents born in South Asia (eFigure 2 in Supplement). After adjustment for age and other covariates in the recent immigrant population, the increased RR of dying in intensive care persisted among recent immigrant decedents from East Asia, Central America and Mexico, South America, Africa, western and central Asia, Southeast Asia, and South Asia. All other regions, including Northern and Western Europe, did not show statistically significant differences from Northern America (eTable 6 in Supplement). Differences were associated with time in Canada, with an RR of dying in intensive care of 1.42 (95% CI, 1.36-1.48) among those who immigrated 21 to 30 years before death and an RR of 2.03 (95% CI, 1.80-2.29) seen in those who immigrated fewer than 2 years before death. In adjusted analyses, the increased RR of dying in intensive care seen among recent immigrant decedents who immigrated 2 or fewer years before death remained statistically significant relative to recent immigrant decedents who immigrated more than 10 years before death, but the differences between those who immigrated 2 or fewer years before death and those who immigrated between 3 and 10 years before death were no longer statistically significant (eTable 6 in the Supplement). There were no significant differences in the adjusted analysis according to immigration class, language ability on arrival, socioeconomic position, or education level on arrival. This study has several limitations. The most important limitation is that the recent immigrant and long-standing resident cohorts differed significantly in terms of age, socioeconomic status, and geography, which leaves the possibility of residual confounding.36 However, comprehensive coverage of all hospital care for every Canadian resident reduces confounding due to economic barriers that may exist to a greater extent in some other jurisdictions and with adjustment of these and other baseline characteristics, the potential for residual confounding should be greatly reduced. Another limitation is that different diseases have different terminal time courses, while the design analyzed fixed intervals preceding death; therefore, some aspects of end-of-life care may have been missed or some care prior to end of life may have been included.37 Health administrative databases are also limited in terms of risk adjustment by disease severity; however, decedent analyses involve inherent severity adjustment through selection of patients who have died. Although data were captured on all decedents in Ontario, no data were available about recent immigrants who returned to their country of origin to die; however, these populations are likely very small (eAppendix 2 in the Supplement). There were no analyses of hospital length of stay prior to intensive care unit admission. There were no data on or analyses of marital status, language ability for long-standing residents (or language ability more recently than arrival for recent immigrants), education level for long-standing residents, or goals of care and preferences for any patients or families. In our trial, we did not measure ovarian blood flow response to treatment, but there is no evidence of a link between ovarian blood flow response and live birth in women with PCOS. Therefore, we disagree with Szmelskyj and Szmelskyj, who suggest that low-frequency electroacupuncture in women with PCOS might be detrimental as they are at increased risk of developing OHSS, possibly due to high ovarian blood flow. Of note, we did not observe an increase in OHSS in the active acupuncture group. Hence, our data did not indicate that low-frequency electroacupuncture had an adverse effect on OHSS. We disagree with the rationale that future research should consider comparing low- with high-frequency electroacupuncture because this frequency has not been shown to affect fertility. We agree with Dr Li that the control acupuncture procedure in our trial was not a negative control but rather an active treatment. However, we disagree that a no-acupuncture control group is needed in future trials. Rather, we suggest that future studies should be designed as comparative effectiveness trials that do not involve control acupuncture, because there are no inert acupuncture controls. We found no difference between active acupuncture with combined manual and low-frequency electrical stimulation with deep needle placement and control acupuncture with superficial needle placement without any stimulation. Therefore, if there is an effect of acupuncture, it is nonspecific and not related to number of needles, their placement, or stimulation. With our rigorous study design, well-defined hypothesis, and large cohort, the appropriate conclusion is that clomiphene is superior to acupuncture for live births in infertile women with PCOS. Both acupuncture and control acupuncture groups had ovulation and live birth rates higher than what has been reported for patients who received metformin only3 or no treatment. To investigate whether acupuncture has a higher live birth rate than, for example, metformin requires a head-to-head comparison without control acupuncture. Our trial indicates that acupuncture is not a substitute for clomiphene for infertility in PCOS, although why and how acupuncture may work require further study.4. In Reply As we noted in our Editorial, the German decision to support acupuncture for low back pain was based on the evidence for substantial benefit of sham and verum acupuncture compared with usual care. The decision was made, as Dr Marcus notes, despite the fact that verum acupuncture failed to show significant benefit compared with sham.1,2 However, results of a well-conducted meta-analysis3 have found significant differences between verum and sham acupuncture for some pain conditions. These differences, although modest, are similar to the effect size seen in some analyses of standard allopathic treatments for chronic pain, including nonsteroidal anti-inflammatory drugs.4 The safety profile of acupuncture is favorable, particularly when compared with nonsteroidal anti-inflammatory drugs and other drugs. The result is some uncertainty about the clinical significance of the modest specific benefits of acupuncture and an inevitable (and perhaps unresolvable) debate about the appropriateness of using the nonspecific benefits. Potential case-patients were reported to public health authorities from 9 reference hospitals throughout Puerto Rico with a clinical suspicion of GBS and neurologic illness onset from April 2016 through December 2016. Case-patients were offered enrollment within 1 month of reported neurologic illness onset. GBS diagnosis was retrospectively confirmed by chart review using the Brighton Collaboration criteria after hospital discharge.4 Case-patients were matched to community controls 1:2 by age group (ie, 7-20, 21-39, 40-64, and >65 years) and place of residence (ie, ≤1-km radius from the residence of a case-patient). The control group included members of the community who lived continuously at the enrollment site for the previous 2 months and were enrolled within 1 week of the matched case-patient. Community control enrollment sites were identified using a randomly generated distance (ie, 0-1000 m) and degree from North (ie, 0°-359°) from each case-patient’s residence. For all participants, a questionnaire was administered on demographics, behaviors, exposures, and medical history, including acute illness within the previous 2 months, and serum, urine, and saliva specimens were collected. Participants were defined as having acute Zika virus infection if they had a positive reverse transcription–polymerase chain reaction (RT-PCR) result in any specimen. Participants were defined as having laboratory evidence of Zika virus infection if they had a positive RT-PCR result in any specimen or anti–Zika virus immunoglobulin M (IgM) detected in serum by enzyme-linked immunosorbent assay. All 47 potential case-patients reported from the 9 hospitals were enrolled, and GBS neurologic diagnosis was confirmed for 39 case-patients (83%). Compared with the 78 controls, the 39 case-patients did not differ by age, but were more often male (Table). Comparing case-patients and controls, identified GBS risk factors were acute illness within the previous 2 months (82% for case-patients vs 22% for controls; MOR, 12.8 [95% CI, 4.6-35.3]), including multiple symptoms; acute Zika virus infection (23% case-patients vs 4% controls; MOR, 16.0 [95% CI, 2.1-120.6]); and any laboratory evidence of Zika virus infection (69% case-patients vs 24% controls; MOR, 36.0 [95% CI, 4.9-262.5]). No other behaviors, exposures, or medical history variables were identified as risk factors. This study was limited by a small sample size, which resulted in wide CIs, unknown generalizability, inability to routinely identify alternative GBS triggers (eg, Campylobacter jejuni), and inability to assess timing of infection. The predominance of males among case-patients could be due to sampling bias because sex is not statistically associated with GBS and there was a greater proportion of female GBS patients with evidence of Zika virus infection during the 2016 Zika virus epidemic in Puerto Rico.6 The pathophysiology of Zika virus infection and risk factors for developing GBS require further investigation. Clinical trials of the Zika virus vaccine should monitor for GBS. During Zika virus outbreaks, clinical suspicion should be elevated to improve GBS patient prognosis through prompt diagnosis and treatment. Enabling access to personal health data, clinical or patient-generated, may benefit patients and health care professionals. Research is beginning to show that providing patients with their complete health data may help improve their health. For example, timely access to laboratory results can increase patient engagement.2 Access to physician notes after appointments appears to encourage individuals to improve their health and participate in decision-making, with electronically engaged patients demonstrating more successful medication adherence, quality outcomes, and symptom management.3 Economic benefits may include the avoidance of duplicative imaging or laboratory tests.4 Clinicians may also benefit from more informed patients. For example, they may score higher in quality performance programs because patients who are more informed may better adhere to treatment plans and hence may improve clinician scores. Despite growing evidence of such benefits, albeit with limited patient outcomes, and legislative and regulatory initiatives that facilitate electronic patient engagement, patients’ access to a complete, longitudinal digital health record remains rare. While such access may be possible for certain patients who receive care within a few select health systems, it remains elusive for many others, including patients who have changed physicians, lived in different places, have multiple chronic conditions, or who have had services provided outside of a clinical setting such as through a home health service.5. Health care, under pressure to embrace interoperability, is poised for transformation. The potential for future system improvements is vast, but depends, in part, on increased patient participation. Health care must find a way to shift from “the doctor will see you now” to “the patient will see the doctor now.”6 Patients need engagement beyond passively receiving services, but this will be challenging until they can easily access and use their health data. For this to proceed, control of health data must be transferred to the patient or the patient’s authorized representative. With widespread implementation of common data elements and value sets, semantic and clinical interoperability can be achieved, and health information can be merged, while maintaining data integrity. New initiatives, such as the Standard Health Record,7 that focus on standardizing data within health records instead of solely on exchange standards enable the development of one complete, digital health record per patient containing health data merged from all of a patient’s clinicians and related health data sources. For example, applying common data elements to platforms can enable patients to add patient-generated data into the record in addition to clinician-generated data. With its common, unifying template, the Standard Health Record can also support a host of secondary uses, such as patient-centered outcome research, precision medicine, and precision public health surveillance. This common digital health data language is also anticipated to reduce translation and comprehension errors. Common data elements enable information from anywhere (eg, clinicians, patients, sensors, and smartphones) to be added to a patient’s digital health record. These additions can be automatically pushed to the record by EHRs, similar to receiving an itemized receipt after banking, only with health data. Receipts must be timely and contain all information from the encounter, including not just the visit summary, but images, billing, and any other related information. Receipts ensure timely updates to the patient’s complete digital health record. Modest EHR system changes would be required to implement receipt processes and, since automated, could minimize clinician and patient burden. Levers to implement receipts include amendments to Meaningful Use, the Merit-Based Incentive Payment System, or other programs that require use of certified EHR technology. The 21st Century Cures Act enables the Department of Health and Human Services to expand guidance on transmitting health information pursuant to a patient’s right of access to the health record. Alternatively, minor amendments to the Health Insurance Portability and Accountability Act could expand patients’ right of access to require automatic generation of receipts. With common data elements, the transfer of receipt data to a patient’s complete health record can occur seamlessly as a background business process. To fully enable robust, patient-centered health data access and control, a final requirement is a data use agreement (DUA), executed between the patient and an entity that manages the patient’s digital health record (the health data manager). The DUA, with provisions addressing data quality, integrity, privacy, security, and patient control, must necessarily be patient-centered, but must also enable clinicians and payers to trust and use the patient’s digital health data as necessary. DUAs can also be tailored to patient preferences. For example, DUAs may enable patients to contribute data for secondary uses. DUAs with health data managers will enable patients to collect and access all of their health data. This addresses the independent maintenance of records by every clinician who sees a patient and, if a patient has seen multiple clinicians, the strong likelihood that those various records have not been aggregated. Further, health records shared with patients are often read-only and may contain only basic information, preventing patients from effectively managing their complete health record. Patients often lack real access to electronic health data; in many instances, “for patients, EHRs are remote and unseen.”8. Simple changes built on business structures already loosely in place can effectuate change. Health data managers may be health care organizations, commercial entities, payers, trusts, or cooperatives. Commercial entities that offer patients health record aggregation services continue to emerge and, in some cases, already enable incorporation of patient-reported health data into an aggregated record. Health information exchanges serving health care organizations and payers could offer such services to patients. Alternatively, a trustee model could be run by an entity that adheres to specific trust requirements, managing and sharing the patient’s health data pursuant to the patient’s predetermined, revocable instructions. Fostering widespread clinician buy-in to a patient-centric approach to health data may be challenging. Some may resist the proposed structure for reasons such as competitive advantage, skepticism, or concern that changes to EHRs increase clinician burden. Today’s existing architecture, however, fails both patients and clinicians. With a new approach, clinicians can truly partner with patients, receiving the benefit of improved quality performance through increased patient engagement and improved, longitudinal digital health records. The 3 components of the proposed system—common data elements, patient encounter data receipts, and DUAs—can improve patient engagement, data accuracy, and health outcomes. The proposed system offers potentially vast returns with amendments to existing regulatory and technical infrastructure. Complete digital health records and standardized health data could serve as a platform for innovation, including personalized medicine and algorithmic services, and pave the road to a true learning health system. Most importantly, the proposed system would provide individuals with the opportunity to control the data that tell some of the most personal stories of their lives. Nutritional science presents special challenges for meta-analyses. In clinical trials, nutrition interventions vary from one study to the next in many methodological details, weakening the argument for combining their results. This is in contrast to studies of drugs in which it is generally easier to assess the comparability of interventions. In observational studies, populations range widely in their dietary habits, and while some diet characteristics (eg, coffee use) are fairly consistent for individuals from day to day and are reliably reported, the consumption of most foods (eg, vegetables) and nutrients (eg, sodium) is variable and difficult to quantify. Different studies handle these issues in different ways. Moreover, different studies may report dietary intakes in tertiles, quartiles, quintiles, or other groupings of their own choosing. Combining results may require contacting the original investigators for participant-level data, which may have been produced using dissimilar dietary assessment techniques. The effects of any given dietary exposure depend on what that exposure is compared against. A 2017 meta-analysis evaluated associations between red meat intake and blood lipid concentrations.5 Of the 39 trials that contributed to the analysis on low-density lipoprotein (LDL) cholesterol, 34 compared red meat with other meats, revealing little apparent relationship with LDL cholesterol. The remaining 5 studies compared red meat to plant-based foods, most of which found nonsignificantly increased LDL cholesterol after red meat consumption. However, the investigators combined the results of all these studies, concluding that red meat “does not negatively influence cardiovascular disease risk factors.” A better approach would focus on a single comparator and ensure that an adequate number of studies had used the method of interest. Combining the results of individual studies increases the total number of participants, and more participants should mean more statistical power. However, when there are differences in participant demographics and study methods, combining studies increases variability in findings that can reduce statistical power, making real effects more difficult to identify. So, for example, if saturated fat is associated with a disease outcome in an individual well-conducted study, but not in a meta-analysis, the null result may reflect heterogeneity among studies that dilutes real findings. Sensitivity analyses, which systematically remove some studies from the analysis, sometimes help by focusing, for example, on higher-quality studies. Because meta-analyses, particularly involving diet, influence health policy, carry considerable weight in the media and in public perception, and have the potential to do harm, the peer-review process must go beyond ensuring that standard meta-analytic procedures have been followed. This could include (1) requiring review by editors with expertise in meta-analysis and in the subject matter at hand, (2) requiring authors to confirm with the authors of the original reports that their data were appropriately represented, to the extent possible, (3) requiring authors to share their summary data and methodological details to allow others to reproduce the analysis, and (4) prioritizing meta-analyses derived by pooling original primary data over those using published summary data. Potential conflicts of interest should be carefully scrutinized for meta-analyses and the studies they include. This process could be facilitated by a standardized, permanent financial disclosure registry. The complexity of the disaster risk landscape and the exposure of large human populations to prolonged and potentially traumatizing events were on full display during Hurricane Harvey. During the 5 days of Hurricane Harvey, more than 33 trillion gallons of rain fell on Texas and Louisiana and set a continental US record for rainfall at 51.88 in (131.78 cm). Among 13 million persons directly affected by the storm, more than 22 000 were rescued from floodwaters, an estimated 32 000 displaced survivors were temporarily housed in shelters, and at least 450 000 will apply for Federal Emergency Management Agency (FEMA) disaster assistance.1 More than 100 000 homes were damaged and only 17% of the affected residents had flood insurance. Damage and recovery estimates are projected to exceed those incurred during Hurricane Katrina ($114.5 billion paid on an estimated $160 billion in damages). The usually circumspect National Weather Service tweeted, “This event is unprecedented & all impacts are unknown & beyond anything experienced.”. The Harvey disaster response has been expansive and televised. However, this response is typical of response to such events—early initial high-visibility rescue and response activities. The storm-affected region is currently receiving a massive injection of state and federal emergency personnel and resources. Yet history teaches that this outpouring of resources will be time-limited. Responder units will be repurposed and redeployed and the media focus is already shifting elsewhere, in this case, to the subsequent catastrophic storm (Hurricane Irma). Public health officials and others know that what happens to populations in the months and years after large-scale traumatic events can have more substantial health consequences than the immediate disaster. The mental and physical health consequences of an event like Hurricane Harvey are long-lasting. Harvey’s physical health consequences included mortality, primarily drowning deaths occurring in submerged vehicles and structures (63 storm-related fatalities were tallied through September 5)3 and injuries including lacerations, puncture wounds, abrasions, fractures, and insect bites (floating fire ant colonies have been a particular hazard) that commonly occur during clean-up activities. Residents are actively gutting their homes to stave off mold. The health effects of widespread population exposures to contaminated floodwaters—an admixture of sewage, toxins, and other hazardous substances—filling homes, streets, and neighborhoods have not been evaluated. Available evidence suggests how emergency responders, clinicians, and health care system managers can meet the key challenge of mitigating health consequences. Early psychological intervention is already under way in the immediate postdisaster phase of Hurricane Harvey. Disaster behavioral health teams are integrated into the response, providing psychological first aid and assessing for urgent psychiatric needs at Houston’s 2 large consolidated shelters. However, the reach of these brief-duration approaches is limited. In contrast, research has shown that replenishing social and economic resources that will restore living conditions for those affected by the hurricane will do as much, if not more, for the protection of health and the promotion of resilience than do short-term, individualized medical or psychological interventions.7,8 This population-level approach, equivalent to “community psychological first aid,” illustrates the importance of optimizing social, economic, and environmental conditions to safeguard health. Another potential way to improve reporting methods is to focus on disease states rather than on procedures. A disease-based approach better captures the spectrum and diversity of pathophysiological and clinical presentations, instead of conflating them into a single procedure. Reporting outcomes for conditions such as non–ST-elevation MI, ST-elevation MI, and MI complicated by cardiogenic shock, irrespective of whether a patient is treated with PCI, surgical revascularization, or medical therapy, would encourage use of the most appropriate therapy and diminish physician and health care center concerns about adversely affecting publicly reported measures of procedural success. Furthermore, disease reporting provides a much more comprehensive, patient-centered assessment of patient care and incentivizes clinicians beyond interventional cardiologists to improve the quality, delivery, and systems of care. It also may be easier for patients to understand outcome reports of a disease, rather than a procedure. It would not, however, address other methodological issues such as how to account for critically ill patients being transferred from one hospital to another, although this concern may become less relevant as hospitals more frequently join integrated care networks. Notably, public reporting of PCI outcomes was implemented in part to provide patients with information to make informed decisions about their care. Despite the investment of resources to ensure the public availability of outcomes data, in general, patients do not appear to use this information in a way that meaningfully influences where they choose to receive care.6 For emergency care, such as PCI for acute MI, patients may have limited ability to select hospitals. Furthermore, there is no evidence that public reports affect physician and hospital referral patterns. In fact, reporting of PCI outcomes only appears to affect physician behavior substantially, and there is compelling evidence that it is not always in a positive way. As professional societies as well as news and media organizations begin to engage in public reporting of numerous procedures on a national level, there needs to be a thoughtful discussion about whether such actions may unintentionally propagate risk aversion and “gaming,” respectively, resulting in worse outcomes for the critically ill and inaccurate assessments of care quality and performance. Transparency in health care is incredibly important. Public reporting plays a crucial role in this effort. However, the effect of reporting may be beneficial for some conditions (and procedures), but not for others, within medical and surgical fields. Thus, it is important that individual public reporting programs are continuously evaluated and improved to ensure that they actually enhance care quality and outcomes. Simultaneously, physicians and policy makers need to acknowledge when there is not convincing evidence that public reporting initiatives are unequivocally beneficial to patient care, and they should be amenable to and advocate for other, innovative approaches. I was in my first year of medicine residency, still losing my way to radiology, still forgetting the codes I needed to access one room or another, still desperately trying to learn how to keep track of my multiple patients and their multiple problems when I assumed his care. As the intern, it fell to me to examine Mr Jones and write admission orders. As the intern, I’d also been up all night. My new white coat was stained with coffee spilled while sprinting to accomplish one forgotten task or other. My pager was chirping regularly, a constant reminder of the calls I’d yet to return. Tired, anxious, and painfully aware of his nurse’s brisk competence as she quieted a beeping alarm and tugged the bed sheet, I introduced myself. Had Mr Jones been coherent, I would have continued my line of questioning. I would have asked about the series of events that lead to the hospitalization, his symptoms, his medical history. Before listening to his heart and lungs and tapping my fingers along his belly, I would have taken his “social history.” I might have asked where he lived, about his military service, his family. As a student, I’d loved those questions and would linger over them. As a new intern, it’s possible I would have lingered a bit with Mr Jones as well. But he was delirious. There was no point. I’d seen alcohol withdrawal one other time, as a medical student. Our team in the ICU admitted a patient from a small clinic in a rural corner of the state. It was one of the first times I had ever taken overnight call, a resident and I placed in charge of the sickest patients in the university hospital. After the rest of the team left, the resident told me to monitor the patient while he took a nap in the call room. I sat at the bank of computers facing the glass wall of our patient’s room and watched the screens that traced his accelerating, irregular heartbeat and that showed his blood pressure rising. I listened to the constant beeping of various machines and to his incoherent cries. I watched him thrash in the bed, pulling, like Mr Jones, against his restraints. Each time I ran into the call room to beg the resident to get up, to tell him that our patient was really, really, sick, he would roll over, and tell me he’d be there in a minute. Eventually, the thrashing and pulling stopped. The resident didn’t come out until dawn and the patient died later that morning. In my mind, Mr Jones and I were buddies, both new to the hospital and both confused. But only one of us was afraid. I checked on him several times a day, adjusting his potassium and magnesium levels, making sure he got his medications, that there was no evidence of infection. I would ask him repeatedly, “Mr Jones, do you know where you are?” When he didn’t, I would tell him, “You’ve been sick. You’re in the hospital.” Mr Jones, who was at a family member’s house last December, or in jail the year before, thanked me for the information, sometimes laughing to himself, as if wondering who let the crazy lady into his cell. But every time I checked on Mr Jones, I was not doing something else. I was not following up on a laboratory test, not answering a nurse’s page, not double checking my work. One morning, a few days after admitting Mr Jones and overwhelmed by to-dos, by crises and noncrises, by my inability to do one thing first and completely, I discharged a patient to a skilled nursing facility with the entirely wrong set of medications. It could have been a deadly error. Instead, the receiving staff physician immediately noticed my mistake and called me to correct it. More kindly than the mistake warranted, she explained that some tasks need to be done quickly, and others very slowly, and that an important part of my job was to figure out which was which. To do my job well, I needed to hoard my time and attention, to keep them closer, and dole them out with more precision, to the right person, the right task, at the right time. I started carrying a folded sheet of paper with tiny check boxes next to tiny tasks in the pocket of my laboratory coat. Along one side, I wrote the last few digits of a patient’s medical record number, along the other, the to-dos associated with each number. I learned where and how to focus my attention. Details of my patient’s chest pain: focus. Details about how his wife now has to come back early from her trip, and boy, she is not happy about that, not one bit: move the story along. Once he knew, he deflated. Delirious, he’d been angry or amused. Back in the present, the bravado was gone. He needed help standing, eating, dressing, getting to the toilet. I no longer imagined that he was my comrade. Now, peering up at me from his hospital bed, belly bloated out of proportion to his skinny legs, and hair plastered to the side of his head in the universal sign of the just-napped, he looked like an aging toddler, mostly helpless and a tiny bit ridiculous. But I didn’t do that. Instead, I rushed in, checked his vital signs, listened quickly to his lungs, and asked briefly whether he needed anything. I remembered my tasks, checked my boxes, and kept moving. My job was to cure him. His job was to be cured, to walk out, and, of course, to stop drinking. Then one evening, after a day and a night and a day in the hospital, as I trudged through the cavernous hospital parking lot, I saw Mr Jones and a friend huddled together in a far-off corner, by the now defunct smoking room, Mr Jones in a wheelchair and his goofy hospital gown. As I raised my hand to greet him, I noticed the bottle being passed between the men. Panicked, I ran back into the hospital and frantically called my attending physician. “I saw Mr Jones in the parking garage! And he was drinking.” My attending was quiet for a moment. Then he said gently, “Well, yes, Jessica. He’s an alcoholic.”. The next morning, I was at the hospital early, as usual. I checked Mr Jones’s laboratory test results and vitals and woke him to listen briefly to his heart and lungs. I asked if he had any questions. And then I left. I had other patients. I had no time and no way to make sense out of those two men outside crouched over a bottle—one of them recently risen from the dead and now killing himself again. There were no check boxes that would make the drinking and the dying understandable and tolerable and small enough to fit in my pocket. So, I ignored them. Without question that hospital stay saved Mr Jones’s life. But homeless, still drinking and with a liver on the outs, I doubt he lived much longer after he left. If I had asked about his drinking, or told him that it worried me, or inquired about the guy in the garage— if I had stopped my rushing and my tiny tasking—could I have altered his trajectory? Yes. Or probably not. I don’t know. I only know this, and maybe, maybe, maybe it is enough: unlike the man from the rural clinic, Mr Jones did not die on my watch. Unlike that man, Mr Jones got better. Over the course of those few weeks, both of us got better. More or less. A, The probability density describes the frequency distribution of observed values and is unit-less. The curves are scaled so that the total area under the curve is 1, and the area between any 2 values on the x-axis equals the probability of observing a value in that range. The blue line plots a neutral prior distribution centered at a risk ratio of 1.0 and indicates an equal number of infants would be expected to benefit by either temperature management group, hypothermia or noncooled. The posterior probability of treatment effect is derived by combining the prior distribution with the trial results. The distribution is shifted to the left of a risk ratio of 1.0 with a point estimate of 0.86. The area under the curve that is less than a risk ratio of 1.0 (light blue) represents the posterior probability of any reduction in death or disability (76% for this trial). The area under the curve that is greater than a risk ratio of 1.0 (dark blue) represents the posterior probability of an increase in death or disability (24% for this trial). B, The light blue portion indicates a 64% probability that death or disability in infants treated with hypothermia is at least 2% less than noncooled infants (benefit). The pale blue area (near zero) is an arbitrary zone of indifference to illustrate the probability of risk differences where hypothermia and noncooling may be viewed as equivalent. The dark blue indicates the probability of death or disability among infants treated with hypothermia is higher than for noncooled infants (harm). In this example, a 2% absolute risk difference was associated with a 3.2-times (64%/20%) higher probability of reduced compared with increased death or disability risk among hypothermia relative to noncooled infants. In all previous neonatal trials, hypothermia was initiated within 6 hours after birth,3-8 reflecting a 6-hour therapeutic window for hypothermia after brain ischemia in fetal sheep.12-14 However, only 5 fetal sheep were cooled at 8.5 hours after ischemia,14 and the results could not exclude the possibility of a longer therapeutic window. Quiz Ref IDInitiating hypothermia before 6 hours after birth can be difficult if infants are born in remote communities and need to be transferred, or if encephalopathy evolves or is recognized after 6 hours. Initiation of hypothermia beyond 6 hours is currently without evidence of benefit.15Quiz Ref ID A definitive trial to determine benefit or harm cannot be conducted for this uncommon condition because even large research networks have an insufficient number of patients to achieve high statistical power in a reasonable time. To provide the most feasible estimate of treatment effect, a multicenter, randomized clinical trial was conducted over 8 years among infants with moderate or severe hypoxic-ischemic encephalopathy treated with hypothermia initiated at or after 6 hours but before 24 hours of age compared with noncooled infants. Bayesian analyses were performed to estimate the probability that hypothermia reduced the risk of death or disability at 18 months. Trial enrollment occurred between April 2008 and July 2014 and follow-up was completed in June 2016 at 21 centers of the Eunice Kennedy Shriver National Institutes of Child Health and Human Development Neonatal Research Network located across the United States. The full trial protocol is available in Supplement 1. Each center received institutional review board approval and infants were enrolled after written informed parental consent was obtained. Newborns with gestational age 36 weeks or later and postnatal age 6 to 24 hours were screened for eligibility if they were admitted to a participating neonatal intensive care unit with a diagnosis of encephalopathy, perinatal asphyxia, or neurological depression. Enrollment until 24 hours of age was based on the variability in the manifestations of newborn encephalopathy,16 ongoing injurious processes in the hours to days after hypoxia-ischemia,17 benefit from hypothermia initiated at 12 hours after ischemia in a preclinical study,18 and the uncertainty in extrapolating from animal to human newborns. Quiz Ref IDInclusion criteria were identical to the prior Neonatal Research Network hypothermia trial4 except for postnatal age. Infants who fulfilled biochemical or clinical criteria and were determined to have seizures or moderate or severe encephalopathy on examination by certified examiners were eligible.4 Infants without moderate or severe encephalopathy by examination but with clinical seizures were classified as having moderate encephalopathy. Exclusion criteria included a core temperature of less than 34°C for more than 1 hour, known anomaly, chromosomal aberration, birth weight less than 1800 g, in extremis condition, and parental or attending physician refusal. Race/ethnicity was obtained by maternal report using fixed categories to ensure group comparability along with other demographic variables. Before randomization, temperature control was per center practices. The hypothermia group underwent whole-body cooling similar to the Neonatal Research Network hypothermia trial4,19 to maintain esophageal temperature at 33.5°C (acceptable range, 33.0°C-34.0°C) using a Hyper-Hypothermia Blanketrol system (Cincinnati Sub-Zero). The duration of hypothermia was lengthened from 72 hours4 to 96 hours based on preclinical data before 2008 that a longer duration of hypothermia was needed to achieve neuroprotection with increasing delays between hypoxia-ischemia and initiation of cooling.18,20 At 96 hours, rewarming was conducted at 0.5°C per hour using the Blanketrol system and completed using a radiant warmer to maintain esophageal temperature of 37.0°C over 5 hours. Temperatures were recorded every 15 minutes during the first 3 hours of cooling, every hour until 12 hours, and at 4-hour intervals through the remainder of cooling and rewarming. The noncooled group was treated with an esophageal temperature probe and temperatures maintained at 37.0°C (acceptable range, 36.5°C-37.3°C). Infants were cared for on radiant warmers and the control set point for the skin temperature was adjusted to achieve the desired esophageal temperature. An algorithm was used to correct hyperthermia by ensuring appropriate thermal care if the esophageal temperature exceeded 37.3°C, and a tepid bath and/or cooling blanket if the temperature exceeded 37.5°C. Temperatures were recorded every 4 hours in the noncooled group. At 108 hours, esophageal temperature probes of both groups were removed and temperature control was resumed per local practice. The primary outcome was death or disability, either moderate or severe, at 18 to 22 months of age. Certified examiners trained to reliability21 and masked to treatment assignment conducted a neurological examination and psychometric testing, assessed growth, and reviewed vision and hearing with the family. The Bayley Scales of Infant Development III were used to obtain cognitive, language, and motor scores (reported mean [SD] score, 100 [15]; range, 55-145).22 The Gross Motor Function Classification Score (GMFCS; range, 0 [normal] to 5 [worst]) was used to classify motor findings.23 Severe disability was defined as any of the following: a cognitive score less than 70, a GMFCS level of 3 to 5, and blindness or hearing impairment with inability to follow commands despite amplification. Moderate disability was defined as a cognitive score between 70 and 84 and any of the following: a GMFCS level of 2, an active seizure disorder (antiepileptic drugs in use), or a hearing deficit with the ability to follow commands after amplification.4 Infants who did not meet the primary outcome were categorized as either mild disability or normal. Mild disability was defined by a cognitive score of 70 to 84 alone or a cognitive score of 85 or greater, and any of the following: a GMFCS level of 1 or 2, a seizure disorder (without medication), or a hearing deficit with the ability to follow commands without amplification. In Bayesian analyses, the probability of treatment effect (posterior probability) is estimated after the trial and incorporates the prior probability estimated from the best data from previous studies (clinical trials or pilot trials).25 Judgment of the prior probability may vary and be neutral, enthusiastic, or skeptical. Therefore, analyses were performed using 3 different prior probabilities: (1) a neutral prior, assuming no treatment effect (RR, 1.0); (2) an enthusiastic prior, assuming a 28% reduction in the risk of death or disability as in the earlier Neonatal Research Network trial4 (RR, 0.72); and (3) a skeptical prior, assuming a 10% increase in the risk of death or disability (RR, 1.10). Whether neutral, enthusiastic, or skeptical, assessments of prior probability involve uncertainty about the minimum and maximum likely treatment effects. To reflect this uncertainty in each analysis, a probability distribution for the treatment effect with the 95% credible intervals that ranged from half to twice the assumed RR (SD, 0.35 in the log scale) was used. For example, the probability distribution for the neutral prior was centered at an RR of 1.0 (mean of 0 in the log scale) with a 50% prior probability of a better outcome, a 50% prior probability of a worse outcome, and a 95% credible interval for the RR of 0.5 to 2.0 (eAppendix in Supplement 2). The RR of 0.5 to 2.0 includes treatment effects for major clinical outcomes of the size observed in almost all large clinical trials.26 For adequately powered trials, differences between neutral, enthusiastic, and skeptical priors have almost no effect on the posterior probability. However, for smaller trials, Bayesian analyses allow assessment of how much the estimated probability of a treatment effect is affected by differing assessments of the prior evidence. All analyses followed the intention-to-treat principle. A binomial model was used with a log link to estimate the posterior RR for different binary outcomes for the hypothermia group compared with the noncooled group. The model to obtain the adjusted RR (aRR) included 3 main effects: treatment (hypothermia or noncooling), age at time of randomization (≤12 hours or >12 hours), and level of encephalopathy (moderate or severe). Center was not included because models did not converge with center as a covariate. The original analysis plan specified a logistic model. This represented an oversight as the intent was to present adjusted relative risks to quantify the treatment effect. The decision to use a log binomial model was made prior to the derivation of the primary outcome and all analyses. Interactions of treatment with age at enrollment, level of encephalopathy at randomization, and sex on the primary outcome were assessed with a log-linear model. Bayesian analyses were used to determine probabilities for the absolute risk difference (binomial model) and adjusted cognitive scores (linear regression). P values from parallel frequentist analyses are provided for outcomes not prespecified, using 2-sided χ2, Fisher exact, Wilcoxon, and t tests; 1-sided t tests were used for absolute differences. A P < .05 was considered significant and was not adjusted for multiple comparisons. Statistical software for Bayesian analysis was JAGS version 4.6 and OpenBUGS version 3.2.3., and for frequentist analysis was SAS version 9.3 (SAS Institute). The data safety monitoring committee reviewed safety after every 20 infants and effectiveness at 33%, 50%, and 75% of outcome accrual. There were 168 participants and 83 were randomly assigned to hypothermia and 85 to noncooling (Figure 1). Hypothermic and noncooled infants were term (mean [SD], 39 [2] and 39 [1] weeks’ gestation, respectively), and 47 of 83 (57%) and 55 of 85 (65%) were male, respectively. Emergency cesarean delivery was performed for 99 of 168 infants (59%), of whom 146 (87%) were transferred to the treating center (Table 1). At birth, intubation was performed in 92 of 168 infants (55%) and chest compressions were performed in 44 of 168 infants (26%). Hypothermic and noncooled infants were randomized at a mean (SD) of 16 (5) and 15 (5) hours, respectively. Enrollment beyond 12 hours after birth occurred in 114 of 168 infants (68%) and moderate encephalopathy was found in 151 of 168 infants (90%). Ten infants in each group were enrolled based on clinical seizures without moderate or severe encephalopathy. Infants with incomplete or no follow-up (n = 11) were similar to those with a known outcome (eTable 1 in Supplement 2). Following hypothermia induction, mean (SD) esophageal temperature was maintained at 33.3 (0.3)°C between 3 and 96 hours (eFigure in Supplement 2). The mean (SD) esophageal temperature for the noncooled group during the intervention was 36.8 (0.5)°C. In the noncooled group, the median number of esophageal temperatures per infant below 36.5°C was 1 (interquartile range [IQR], 0-3) and the mean of these values was 36.2°C (IQR, 35.9°C-36.4°C). The median number of esophageal temperatures per infant greater than 37.3°C was 1 (IQR, 0-4) and the mean of these values was 37.5°C (IQR, 37.4°C-37.7°C). Interventions to reduce elevated temperature were performed in 27 of 85 infants (32%) in the noncooled group; 24 infants (28%) received a tepid bath and 7 infants (8%) were treated with a cooling blanket. During the intervention and rewarming, unmasked observers recorded 13 and 6 adverse events in the hypothermia and noncooled groups, respectively (Table 2). One infant (hypothermia) developed subcutaneous fat necrosis and 1 infant (noncooled) developed diabetes insipidus, both remote from the intervention. There were no group differences in organ system morbidities or need for extracorporeal membrane oxygenation. A blood glucose concentration greater than 150 mg/dL occurred more frequently in the hypothermia than the noncooled group (to convert glucose to millimoles per liter, multiply by 0.0555). Complete follow-up at a mean (SD) of 21 (3) months was achieved among 69 of 74 (93%) and 72 of 78 (92%) hypothermia and noncooled survivors to discharge, respectively. There were 9 deaths in each group. There were minimal missing data for the components of disability among survivors, which did not prevent assignment of a primary outcome except for 2 infants with incomplete follow-up evaluations (Figure 1). Death or disability (moderate or severe) at follow-up was known for 157 infants (93.5%) and occurred in 19 of 78 (24.4%) of the hypothermia group and 22 of 79 (27.9%) of the noncooled group (absolute difference, 3.5%; 95% CI, −1% to 17%; Table 3). Bayesian analysis using a neutral prior indicated a 76% posterior probability of reduced death or disability with a posterior aRR of 0.86 (95% credible interval, 0.58-1.29) (Figure 2). The corresponding frequentist aRR was 0.81 (95% CI, 0.44-1.51). Further, 73% and 68% probabilities were identified for reduction in death and in moderate-severe disability, respectively, under the neutral prior. Expressed as an absolute risk difference, the posterior probability that death or disability for hypothermia using a neutral prior was at least 1%, 2%, or 3% less compared with noncooled treatment was 71%, 64%, and 56%, respectively. A 2% absolute risk difference was associated with a 3.2-times (64%/20%) higher probability of reduced compared with increased death or disability among hypothermia relative to noncooled infants, assuming a range of risk differences viewed as equivalent (Figure 2). Using an enthusiastic prior, the posterior probability that death or disability was at least 1%, 2%, or 3% less compared with noncooled increased to 86%, 80%, and 74%, respectively. Death or moderate-severe disability did not differ by age at randomization: 6 of 25 infants (24.0%) and 7 of 26 infants (26.9%) when randomized at 12 hours or less, and 13 of 53 infants (24.5%) and 15 of 53 infants (28.3%) when randomized between 12 and 24 hours in the hypothermia and noncooled groups, respectively. There were no interactions between treatment and age at randomization, level of encephalopathy, or sex for the primary outcome (Bayesian probability of an interaction, 46%, 38%, and 22%, respectively). It can be difficult to study therapies for rare diseases or uncommon features of previously studied disease processes. Extrapolation from animal studies has limitations.27 The therapeutic window during which hypothermia may modify hypoxic-ischemic brain injury may differ between preclinical studies and human newborns owing to species maturation, methods to induce hypoxia-ischemia, and outcomes studied. Randomized clinical trials are unlikely to have sufficient power to identify clinically important treatment effects in traditional frequentist analysis. This is especially pertinent to a trial of hypothermia initiated after 6 hours of age as it addresses a small subset of infants with moderate-severe encephalopathy. Furthermore, based on preclinical studies,12-14 hypothermia started after 6 hours was anticipated to provide less neuroprotection than hypothermia started before 6 hours, making it even less feasible to achieve high power even in the Neonatal Research Network, with 21 centers serving large delivery cohorts and referral bases throughout the United States. However, the high rates of potentially catastrophic outcomes associated with hypoxic-ischemic encephalopathy, and reports of initiating hypothermia after 6 hours in a subset of infants with moderate-severe encephalopathy without any systematic study, provided justification to undertake this trial. With the sample size studied, traditional frequentist analysis identified no significant difference for death or disability between the hypothermic and noncooled groups. A frequentist analysis of an underpowered trial would provide little help for clinicians treating infants with encephalopathy who present beyond 6 hours. However, frequentist analyses do not allow calculation of the probability of a specified benefit while Bayesian methods allow for direct assessment of the probability of treatment effect based on the trial results.24,25,28 A series of absolute risk differences in death or disability were provided to assess effect size. An absolute risk reduction of 1% or 2% in death or moderate-severe disability may be viewed as clinically important given the seriousness of the outcome. Perinatal and adult therapies have been recommended at a similar low absolute risk reduction for major adverse outcomes. A risk reduction of 1.6% for cerebral palsy has been reported among preterm newborns whose mothers were treated with magnesium sulfate,29 a treatment now widely used in obstetrics.30 Statins have been recommended in adults with a cardiovascular disease risk of 10% but without cardiovascular disease31 based on absolute risk reductions of 0.4% and 1.4% for all-cause and composite cardiovascular mortality, respectively.32. The reduction in death or disability for hypothermic compared with noncooled infants is suggestive but not conclusive. No evidence of commensurate harm was found. Adverse events, prespecified and those not prespecified, occurred in more hypothermic than noncooled infants but did not differ by frequentist analysis. The number of deaths per group was the same, but the Bayesian analysis indicated the aRR favored hypothermia and the posterior probability of reduced death was 73% under a neutral prior. A decision to use hypothermia at 6 to 24 hours will need to consider the probability of benefit, the frequency of adverse events, and the availability of evidence-based alternative treatments. In all neonatal trials, hypothermia initiated at less than 6 hours reduced death or disability,33 and therefore a prior based on an RR between 1.0 (no benefit) and 0.72 (the benefit identified in the previous Neonatal Research Network hypothermia trial4) might be considered appropriate. If so, the estimated probabilities for any reduction or a greater than 1% or greater than 2% absolute reduction in death or disability would be somewhat higher and intermediate between the estimates using a neutral prior and those using an enthusiastic prior. Given these considerations, estimated probabilities based on a neutral prior could be considered conservative. Since the initiation of this trial, there is little new information on late hypothermia treatment to provide an estimate of treatment effect. A trial from China randomized 93 newborns within 10 hours of birth to hypothermia or normothermia; only 9 infants received hypothermia between 6 and 10 hours.34 Single-center and registry data reported initiation of hypothermia after 6 hours but did not provide treatment effect.15,35. This study has several limitations. During this trial, the Neonatal Research Network initiated the Optimizing Cooling trial to study longer (120 hours) or deeper (32°C) cooling initiated at less than 6 hours of age.36,37 The Optimizing Cooling trial was stopped early partly owing to safety concerns for increased in-hospital mortality among infants cooled for 120 rather than 72 hours. Whether the current trial results would differ if the intervention was shortened to 72 hours cannot be answered. Elevated esophageal temperature occurred among noncooled infants and such temperatures have been associated with a greater risk of death or disability in prior trials.38-40 The algorithm used in noncooled infants largely mitigated the extent and duration of esophageal temperatures greater than 37.3°C compared with the earlier Neonatal Research Network hypothermia trial.4 Also, the reason for presentation at or beyond 6 hours was not always known. A retrospective cohort study was performed using the Premier Healthcare database (Premier), an all-payer, fee-supported database developed to measure resource use and quality, to assess the usage of the robotic platform for radical nephrectomy. This database captures approximately 20% of all hospitalizations from more than 700 acute care hospitals in the United States (>530 million hospital visits and 6 million inpatient discharges per year since 2011). This database also contains information on demographic and clinical characteristics, such as pharmaceuticals administered, laboratory and other diagnostic tests performed, and therapeutic services provided during admission. The Premier Healthcare database uses a reconciliation process that allows for verification and validation of hospital reporting for the use of resources and cost. Data audits are performed, and if reported costs submitted do not match the hospital’s financial statement, Premier works with the hospital to correct the discrepancy.15 Procedure and comorbidity data are provided by International Classification of Diseases, Ninth Revision (ICD-9) codes. This method has been used in other studies.6,8,16,17 This investigation was deemed exempt from informed consent requirements by the Stanford University Medical Center institutional review board. Patients receiving radical nephrectomy between January 2003 and September 2015 were identified by ICD-9 code (55.51) and included in the analysis. Affiliated codes were identified and reviewed to ensure that radical nephrectomy was the primary procedure performed based on the diagnosis or concern for kidney cancer (eTable 1 in the Supplement). For example, cases of upper tract urothelial carcinoma (ICD-9 codes 189.1 or 189.2), which have unique postoperative complication profiles stemming from the need for concurrent ureterectomy and cystotomy were excluded. Only patients receiving either robotic-assisted  or laparoscopic radical nephrectomy were included. Patients undergoing open radical nephrectomy or nonelective surgeries were excluded. The inclusion and exclusion methodology is further depicted in the eFigure in the Supplement. The primary outcome of the study was the trend in use of the robotic-assisted radical nephrectomy. The secondary outcomes of interest were perioperative complications, resource use, and direct hospital costs. Postoperative complications were classified based on the Clavien classification system.20 These complications were defined as any (Clavien grades 1-5) or major (Clavien grades 3-5). Grade 1 complications include “Any deviation from the normal postoperative course without the need for pharmacological treatment or surgical, endoscopic, and radiological intervention.” Grade 2 complications “[require] pharmacologic treatment with drugs other than such allowed for grade 1 complications.” Grade 3 complications “[require] surgical, endoscopic or radiological intervention.” Grade 4 describes “Life-threatening complications requiring intermediate care/intensive care unit.” Grade 5 complications result in the “[d]eath of a patient.” To identify events defined by the Clavien classification system, we used ICD-9 codes as previously described.6,21 Resource use variables analyzed included blood transfusion (packed red blood cells), operating time (hours), and length of stay (days). Operating time (≤4 hours vs >4 hours) and length of stay (≤4 days vs >4 days) were categorized as dichotomous variables.19,22. Two types of direct hospital costs were provided by the Premier Healthcare database. A total of 78.5% of all patients included in the study were treated by hospitals providing procedural costs (or “reported costs”) and the remainder were treated by hospitals providing estimates based on Medicare cost-to-charge ratios (MCCR or “estimated costs”).17,23,24 If hospitals have their own cost-accounting system, they assign relative value units to procedures to estimate cost. These hospitals are then able to provide Premier with both charge and cost data. If hospitals do not have a cost-accounting system or do not use relative value units to estimate cost, they provide only charge data. Hospital departments are mapped to a specific line on the Medicare Cost Report to determine the appropriate MCCR, which is then used to determine cost at a given resource level. All costs were adjusted to 2015 US dollars using the consumer price index. Categorical variables were presented as numbers and percentages and were compared using the χ2 test. Linear trends in the proportion of robotic-assisted radical nephrectomies over 13 years were assessed using a logistic regression model. To reduce potential confounding, we performed an adjustment for differences in baseline patient characteristics by using a weighted logistic regression model with inverse probability of treatment weighting (IPTW).25 Using this technique, the weights used for patients undergoing laparoscopic radical nephrectomy were the inverse of 1 minus the propensity score, and weights used for patients receiving robotic-assisted radical nephrectomy were the inverse of the propensity score alone. The propensity scores were estimated by multiple logistic regression analysis without regard to outcomes. A full nonparsimonious model was developed including all variables shown in Table 1. Log-binomial regression models were used to estimate risk ratios (RRs) for each exposure on perioperative outcomes. Since it was determined that the outcome variables related to direct hospital costs were not normally distributed, a generalized linear model with gamma distribution was generated, allowing for a link function to connect the predictor with the response variables.26 All models were adjusted for clustering of patients within hospitals using robust standard errors to account for interhospital variability. An analysis was also conducted to determine if the costs related to each surgical approach (robotic-assisted and laparoscopic radical nephrectomy) were related to the source of cost obtained within the Premier Hospital database. For these analyses, the propensity score analyses were re-performed to obtain a new IPTW for each patient. These analyses were not prespecified but rather post hoc and thus interpreted as exploratory. Statistical analysis was performed using 2-sided tests, with a significance level of <.05 and Stata 14 statistical software (StataCorp). A cohort of 23 753 patients undergoing elective laparoscopic radical nephrectomy (n = 18 573) or robotic-assisted radical nephrectomy (n = 5180) for the management of renal masses at 416 US hospitals between 2003 and 2015 was evaluated. The Figure shows the trend in surgical approach for radical nephrectomy over time. Use of robotic-assisted surgery for radical nephrectomy increased from 1.5% to 27.0% in the entire radical nephrectomy cohort from 2003 to 2015 (P for trend <.001). Since 2009, the decrease in laparoscopic radical nephrectomies paralleled the increase in robotic-assisted radical nephrectomies, while the proportion of open radical nephrectomy cases plateaued. By 2015, robotic-assisted radical nephrectomy was performed more commonly than laparoscopic radical nephrectomy in the United States. Unadjusted and IPTW-adjusted perioperative outcomes are presented in Table 2. The unadjusted rate of any (28.2% vs 21.9%; risk difference, 6.3%; 95% CI, 4.9% to 7.6%) or major complications (4.3% vs 3.6%; risk difference, 0.7%; 95% CI, 0.1% to 1.3%), prolonged operating time (43.8% vs 26.2%; risk difference, 17.6%; 95% CI, 16.1% to 19.1%), and blood transfusion (19.5% vs 18.2%; risk difference, 1.4%; 95% CI, 1.4% to 2.6%) for patients receiving robotic-assisted radical nephrectomy were higher than for those who received laparoscopic radical nephrectomy. Prolonged length of stay was less frequent in the robotic-assisted vs the laparoscopic radical nephrectomy group (21.2% vs 25.1%; risk difference, −3.9%; 95% CI, −5.2% to −2.7%). However, the IPTW-adjusted rates of any or major complications, blood transfusion, and prolonged length of stay were similar between the robotic-assisted and laparoscopic radical nephrectomy groups. The IPTW-adjusted rate of prolonged operating time for patients undergoing robotic-assisted radical nephrectomy was higher than for patients receiving laparoscopic radical nephrectomy (46.3% vs 25.8%; risk difference, 20.5%; 95% CI, 14.2% to 26.8%). An unadjusted cost comparison by surgical approach is presented in the eTable 2 in the Supplement. The IPTW-adjusted analysis suggests that robotic-assisted radical nephrectomy was associated with higher mean 90-day direct hospital costs ($19530 vs $16851; difference, $2678; 95% CI, $838 to $4519), likely accounted for by higher operating room ($7217 vs $5378; difference, $1839; 95% CI, $1050 to $2628) and supply costs ($4876 vs $3891; difference, $985, 95% CI, $473 to $1498; Table 3). Further analyses were performed to identify the association of the source of cost obtained by the Premier data set (reported vs estimated) and the difference in direct hospital costs between robotic-assisted and laparoscopic radical nephrectomy (Table 4). The 90-day direct hospital ($19 471 vs $16 779; difference, $2692; 95% CI, $787 to $4597), supply ($4905 vs $3999; difference, $906; 95% CI, $289 to $1524), and operating room costs ($7022 vs $5265; difference, $1758; 95% CI, $869 to $2647) were higher for robotic-assisted radical nephrectomy among patients treated at hospitals providing reported costs. Among patients receiving care from hospitals providing estimated costs using MCCR, robotic-assisted radical nephrectomy was associated with higher supply costs ($4728 vs $3474; difference, $1254; 95% CI, $136 to $2373) and operating room costs ($7589 vs $5810; difference, $1779; 95% CI, $227 to $3331) but similar 90-day direct hospital cost compared with laparoscopic radical nephrectomy ($19 187 vs $17 112; difference, $2075; 95% CI, −$1288 to $5439). The use of the robotic platform has increased rapidly for curative renal surgery, especially for partial nephrectomies. However, little is known about the nationwide use of robotic-assistance for radical nephrectomy in the United States. Some studies have suggested that the proportion of robotic-assisted cases was less than 10% of all radical nephrectomies during the late 2000s.13,27 In contrast, this study found that the proportion of robotic-assisted radical nephrectomies increased to approximately 30% of all radical nephrectomies by 2015, which is higher than for the laparoscopic approach in the United States. A parallel decrease in the use of laparoscopic radical nephrectomy suggests a shift to robotic surgery from cases that would have been previously treated laparoscopically rather than by open surgery. It remains unclear why the use of robotic-assistance has increased substantially and has been steadily replacing laparoscopic radical nephrectomies. One possibility is the financial viability of the robotic system in relatively small hospitals. The costs of purchasing and maintaining the robotic system range from $0.5 to $2.5 million and $80 000–$170 000 per year , respectively.28 Surgeons have to perform at least 100 to 150 procedures annually to offset the upfront and ongoing costs of its acquisition.29 Another possibility is that the increase in robotic-assisted radical nephrectomies might be associated with the known increase in robotic-assisted partial nephrectomies. The use of robotic-assistance has increased rapidly since 2008 and in some areas has overtaken laparoscopic partial nephrectomy.27,30 This trend suggests an overall increase in the risk of intraoperative conversion to radical nephrectomy as surgeons attempt to treat larger and more complex tumors using the nephron-sparing approach.31 Considering that the incidence of intraoperative robotic-assisted partial to radical nephrectomy conversion remains prevalent especially for low-volume hospitals and surgeons in the United States, the increase in unsuccessful robotic-assisted partial nephrectomies may have contributed to the increase in robotic-assisted radical nephrectomy use.32 As urological training has been focused on robotic surgery driven predominantly by the widespread use of robotic-assisted radical prostatectomy (more than 80% of the total prostatectomies in the United States in 2013), urologists completing their residency or fellowship training may also prefer the robotic platform over laparoscopic surgery due to its ergonomic console and 3-dimensional screen.6. Although the use of the robotic platform has been well-received by surgeons performing laparoscopic partial nephrectomy due to ease of tumor resection and renorrhaphy, the evidence supporting the use of robotic-assistance for radical nephrectomy remains somewhat biased. Radical nephrectomy does not require the routine use of intracorporeal suturing, which is a primary advantage of robotic assistance in partial nephrectomy and radical prostatectomy. Furthermore, there are several disadvantages of robotic technology scarcely acknowledged by prior literature. For example, robot arms return minimal tactile feedback to the surgeon. Moreover, the field of view during robotic-assisted radical nephrectomy is relatively narrow. Therefore, special attention is required to prevent unintentional trauma to peripheral organs not felt or visualized by the surgeon.10. There is also a significant cost burden attributed to the use of the robotic system. This study shows that the use of the robotic platform for radical nephrectomy increased the total direct hospital cost by nearly $2700, which is more than 15% of the total cost of the laparoscopic approach. This increased expense for robotic-assisted radical nephrectomy was mostly accounted for by increased operating room cost, which is directly correlated with operating time. These findings are consistent with the observations of a study from Maryland that reported a $5111 increase in hospital charges per robotic-assisted radical nephrectomy compared with laparoscopic radical nephrectomy.33 Hospitals are likely to increase charges for robotic surgery to recoup costs related to the acquisition and maintenance of the robotic system despite not receiving reimbursement for these fixed costs from Medicare and private insurers.34 Increased hospital charges for robotic surgery influence future reimbursement because the Centers for Medicare & Medicaid Services (CMS) use hospital charges to calculate the relative weight for each diagnosis related group (DRG) annually, which in turn help determine the payment made for inpatient services. The DRG weight is determined by the average resources required to treat cases within the DRG and is multiplied by the average payment rate for a typical case to yield the total reimbursement rate.35 Thus, hospitals are incentivized to charge payers for the true cost of robot use. A prior study estimates an additional cost to the health care system of $2.5 billion if conventional surgeries were to be fully replaced by robotic surgery.7. Robotic partial nephrectomy does have some advantages over traditional laparoscopic partial nephrectomy, including reduced ischemic time and total operating time. However, this study suggests that the traditional advantages of robotic surgery are not applicable to radical nephrectomy when compared with conventional laparoscopy. Some high-volume surgeons also argue that robotic-assisted radical nephrectomy may be beneficial for treating advanced kidney cancer with vena cava tumor thrombus in a minimally invasive manner.39 However, that does not adequately explain the rapid increase in robotic-assisted radical nephrectomy within the United States because these advanced kidney cancers have been largely treated by the open approach. Although the initial results of safety and short-term oncological outcomes are promising, further investigation is required to determine the role of robotic surgery for vena cava tumor thrombectomy. This study has several limitations. First, it is subject to potential misclassification bias as billing codes and ICD-9 procedural codes were used to capture robotic-assisted surgeries. However, previous studies using the same method showed that ICD-9 coding for robotic-assisted surgery was sufficiently specific.6,8,17 Second, the Premier Healthcare database does not publish information regarding tumor characteristics. Large or complex renal tumors, such as hilar and endophytic tumors, increase the risk for perioperative complications during laparoscopic surgery, although more notably for partial nephrectomy.40 The influence of tumor characteristics is likely negligible for both robotic-assisted and laparoscopic radical nephrectomy because they are both minimally invasive and have similar clinical indications. Third, because the rate of conversion to open radical nephrectomy is difficult to evaluate retrospectively, the rates of conversion could not be compared between the 2 approaches using the Premier Healthcare database. Fourth, long-term data are necessary to further compare oncological outcomes and quality of life between robotic-assisted and laparoscopic radical nephrectomy. More recently, robotic-assisted surgery has emerged as an alternative minimally invasive surgical option. In essence, laparoscopic instrumentation is connected to a robotic device that is controlled by a surgeon seated at a remote console. Robotic-assisted technology allows for 3-dimensional visualization, enhanced range of motion of the instrumentation, and improved surgeon ergonomics. Robotically assisted surgery first gained prominence for performance of radical prostatectomy, a procedure in which laparoscopic surgery was rarely performed.2 Since the use of the surgical robot for prostatectomy, the technology has been studied and promoted for a variety of other procedures.3-5. In this issue of JAMA, 2 studies examining the effectiveness of robotic-assisted oncological surgery are reported. In one study, Jayne and colleagues6 report the results of a randomized clinical trial involving 471 patients with rectal cancer who underwent either laparoscopic (n = 234) or robotic-assisted (n = 237) resection with curative intent. There was no difference between the groups in the primary outcome, conversion to laparotomy (with rates of 8.1% for the laparoscopic group and 12.2% for the robotic-assisted group; unadjusted difference, 4.12%; 95% CI, −1.35% to 9.59%), or in any of the secondary outcomes including margin positivity, complication rates, mortality, or quality of life at 6 months. The mean operative time was 37.5 minutes longer for the robotic procedure than for the laparoscopic procedure (298.5 vs 261 minutes), and the mean direct costs associated with robotic-assisted resection were higher per case than for laparoscopic rectal resection ($13 668 vs $12 556, difference $1132; 95% CI, $191-$2072; P = .02). The authors concluded that robotic surgery did not confer an advantage over laparoscopy for colon resection. In another article, Jeong and colleagues7 report the results of an observational study using a hospital-based data set to examine the perioperative outcomes of robotic-assisted radical nephrectomy (n = 5180) compared with laparoscopic radical nephrectomy (n = 18 573) for patients with renal masses. The use of robotic-assisted nephrectomy increased substantially over the course of the study, from 1.5% (39 of 2676 cases) in 2003 to 27% (862 of 3194 cases) by 2015. There were no differences in the adjusted rates of any of the major postoperative complications examined (3.8% in the laparoscopic group vs 3.5% in the robotic group) or of blood transfusions (17.8% in the laparoscopic group vs 21.2% in the robotic group). However, a prolonged operative time (>4 hours) was more likely among patients who underwent robotically assisted surgery than those who underwent laparoscopic surgery (adjusted rates, 46.3% vs 25.8%; risk ratio, 1.79; 95% CI, 1.52-2.11). Importantly, the mean 90-day direct hospital costs associated with robotic-assisted surgery was $2678 (95% CI, $838-$4519) greater than that for comparable laparoscopic procedures ($19 530 vs $16 581). Most of the increased cost was related to operating room and supply charges. Robotic-assisted surgery also highlights the difficulty of evaluating and testing surgical innovations. The evaluation of new procedures and devices is unlike drug development that follows an orderly pathway from safety evaluations, to small studies to document an effect, and then larger, comparative trials to demonstrate effectiveness compared with a standard therapy. Most drugs progress through this pathway prior to regulatory approval and widespread use. Surgical innovations differ in a number of important ways. Many new procedures develop from a clinical need and diffuse into practice with limited formal assessment. When evaluation does occur, reporting is often based on the demonstration of feasibility and not efficacy. Randomized trials comparing 2 procedures are difficult to undertake and subject to variation in surgeon experience and dependent on the operator and team setting.8 In addition, the regulatory standards for new procedures and devices are much less stringent than for new drugs and often only require demonstration that a device is equivalent to some previously approved device.9 There are numerous examples of how these challenges have resulted in widespread adaptation and use of new devices and procedures in routine clinical practice without rigorous evaluation.5,10. Regulation of new devices and procedures must balance the needs of timely access to the device with the requirements for evidence of safety and efficacy data prior to widespread dissemination.11 Devices typically receive regulatory approval after the demonstration of safety. However, once approved, questions often remain about the effectiveness of these devices in clinical practices settings.11 Uncertainties about the comparative effectiveness of a new device can be evaluated with postmarket surveillance; however, surveillance has historically relied on voluntary reporting of adverse outcomes and collection of data through registries. More recently, the US Food and Drug Administration has moved to develop more rigorous postmarketing surveillance procedures. The National Evaluation System for Health Technology (NEST) is one such system to monitor device performance.11 NEST will link evidence from registries, electronic health records, and billing data along with other sources to create a data repository that can be used to determine device efficacy.12 This system can be used to examine both established and new uses of a device. A number of robotically assisted procedures have gained widespread acceptance in clinical practice despite a relative lack of data. As Jeong and colleagues demonstrated, by 2015 robotic-assisted procedures accounted for nearly 30% of radical nephrectomies in the United States, even though the evidence supporting the procedure was based largely on single-institution observational trials. Similarly, robotic-assisted hysterectomy accounted for nearly 10% of hysterectomies for benign indications by 2010 despite limited data to support the efficacy of the procedure.5 The influence of nonclinical factors in driving the diffusion of robotic-assisted surgery has been well described.13 Robotic-assisted surgery has been intensely marketed  not only to physicians and hospitals, but also directly to patients.14 Competition among hospitals is also a driving force in the uptake of robotic technology.15 Currently, much of the increased cost associated with robotic-assisted surgery is underwritten by hospitals. Hospitals are willing to bear these costs in the hope of increasing market share and remaining competitive in their regional markets. One report suggested that performance of a robotic-assisted procedure increased costs by an average of 13% across a range of operations.15 The findings reported for radical nephrectomy and rectal resection are consistent with these estimates. Whether robotic-assisted surgery for some procedures represents “value” for either the individual patient or the health care system is unlikely. Without clear demonstration of improved outcomes associated with robotic-assisted procedures, the complicated issue of the cost will become increasingly important. Currently, the majority of the cost associated with robotic-assisted surgery is absorbed by hospitals with little effect on physician reimbursement or in the way of cost sharing by patients. The implementation of alternative payment models, particularly those with 2-sided risk, may heighten the awareness of surgeons of these costs if reimbursement is more directly affected. For many medical conditions, pathways to guide therapy for treatments that are not established have reduced use of these therapies and resulted in cost savings. Similar efforts could be more heavily promoted in surgery. Alternatively, reducing the cost of robotic-assisted surgery could help alleviate much of the criticism surrounding the procedures. As new robotic technologies enter the market, there is at least the potential that some of the fixed costs associated with these procedures could be reduced. From a policy perspective, robotic-assisted surgery exemplifies the difficulty of balancing surgical innovation with evidence-based medicine. Both the generation of high-quality evidence evaluating new procedures and then the utilization of this evidence to guide practice should remain priorities for surgical disciplines. The US military’s early combat casualty care experience during Operation Enduring Freedom in Afghanistan was first described in 2003.1 This publication presaged a rapid string of enhancements to battlefield aeromedical evacuation resuscitative capabilities that improved the outcomes of service members who were critically injured in Afghanistan’s complex, climatologically diverse, and geographically sprawling battle space. In 2003, the United States had not yet established integrated trauma system capabilities in Afghanistan or Iraq (both covered by US Central Command). Recognizing this deficiency, military medicine rapidly developed and implemented an interservice Joint Trauma System to ensure that optimal, timely care was delivered to injured soldiers when needed. Concurrent collection and analysis of clinical data were established to deliver, assess, and refine best practices through a process that has been described as “focused empiricism.”2,3 This process entailed using registry-based clinical data to optimize practices in topic areas in which randomized clinical studies are not possible or feasible using the best data available at the time to make adjustments in care.2,4. Real-time performance improvement is a cardinal feature of military trauma care. Initially, the Joint Trauma System focused on improving surgical and critical care prior to and during hospitalization. As the number of troops involved in Afghanistan and Iraq increased, it became imperative to collect and analyze data from the out-of-hospital setting as well (ie, point of injury and en route care). Responding to the increased burden of injury and prolonged evacuation times in Afghanistan, then US Secretary of Defense Gates imposed a 60-minute evacuation standard in 2009 with the intent to improve outcomes.5 This directive, combined with the challenging operational environment of Operation Enduring Freedom, resulted in numerous system adaptations, including more robust use of paramedical personnel working aboard the casualty evacuation aircraft. The methods used included Cox models designed to assess survival that were adjusted for the matching factors and additional covariates and that used a delayed-entry approach to address the issue of potential immortal time bias among those who survived long enough to receive prehospital transfusion, with rational and straightforward matched covariates. In aggregate, the groups were similar by age, evacuation time, injury year, presence of traumatic brain injury, shock physiology (acidosis), length of hospital stay, and maximum injury scores. The cohorts differed in aggregate with respect to multiple amputations, torso hemorrhage, evacuation by advanced life-support platform (MERT and PEDRO), prehospital tourniquet use, minutes from rescue to initiation of blood transfusion, administration of tranexamic acid, total units packed red blood cells or whole blood administered within the first 24 hours, and evacuation to a combat hospital capable of definitive (role 3) surgical care. Although the study methods included univariate analysis of nonmatched covariates, there was no mention of adjusting for multiple comparisons. The authors demonstrated that prehospital blood transfusion, compared with delayed transfusion or no transfusion, was associated with greater 24-hour survival and 30-day survival. At 24 hours, 3 of 55 prehospital transfusion recipients died (5%) and 85 of 447 nonrecipients died (19%) (between-group difference, −14% [95% CI, −21% to −6%]; P = .01). By day 30, 6 prehospital transfusion recipients (11%) and 102 nonrecipients (23%) died (between-group difference, −12% [95% CI, −21% to −2%]; P = .04). Among the 386 patients in the matched groups without missing covariate data, the adjusted hazard ratio for mortality associated with prehospital transfusion was 0.26 (95% CI, 0.08 to 0.84; P = .02) over 24 hours (3 deaths among 54 recipients vs 67 among 332 matched nonrecipients) and 0.39 (95% CI, 0.16 to 0.92; P = .03) over 30 days (6 vs 76 deaths, respectively). As the authors acknowledge, the study results should be interpreted in the context of the military trauma system in which the transfusions occurred. Shackelford et al10 have effectively used the principle of “focused empiricism” as outlined in the National Academies of Sciences, Engineering, and Medicine’s report2 to demonstrate that prehospital transfusion was associated with improved survival and to inform current practices by the military in combat zones. This finding builds on a previous study that demonstrated that an advanced resuscitative capability (defined as the presence of advanced practice rescue personnel or a physician) was associated with reduced mortality and improved access to surgical intervention among patients with more severe injuries.8 Similarly, the apparent survival benefit associated with prehospital transfusion reported by Shackelford et al10 does not exist in isolation, but rather is comingled with potentially synchronous application of advanced prehospital and in-hospital trauma care. Accordingly, the findings need to be viewed in the context of the capabilities (eg, training and skill level of rescue personnel, surgeons, and clinical staff; and presence of advanced airway capabilities and monitoring) associated with the ability to transfuse blood on these evacuation platforms. As such, further work is needed to optimize casualty evacuation capabilities and match these to the initial triage of injury severity. However, the results reported by Shackelford et al10 suggest that prehospital transfusion is an important factor in survival among combat patients. Attempts to replicate this approach and determine the transferability of prehospital transfusion for severely injured civilian patients have so far been unsuccessful. This inability to demonstrate a benefit in civilian patients is related more to the variability of treatment in the prehospital system rather than the heterogeneity of civilian vs military patient populations. This suggests the need for a broader strategy that allows for adoption and dissemination of best practices (as was done with prehospital blood transfusion in combat zones) to bring these military advances into the larger civilian trauma care model.11,12. This search identified articles using PubMed, EMBASE Ovid, and the Cochrane Library to identify high-quality, multicenter randomized controlled trials; systematic reviews; meta-analyses; and practice guidelines from January 2000 to July 2017 that assessed urinary incontinence evaluation and treatment. The authors selected several landmark, multicenter, randomized, comparative efficacy trials that have important implications for current clinical practice and constructed summary tables of that evidence for stress and urgency incontinence. The prospectively followed cohorts of these trials also provide the highest levels of evidence on the moderate-term safety and efficacy of the most common incontinence therapies for women. Because of the limited comparative efficacy trials across all available incontinence medications, we calculated the average reduction in urinary frequency, urgency incontinence episodes, and common adverse effects, using the trial evidence used for Food and Drug Administration (FDA) registration of the most commonly prescribed medications. Quiz Ref IDMany women do not volunteer incontinence symptoms to their primary care provider because of embarrassment, lack of knowledge, or misconception about treatment.15,16 Once incontinence is detected, the clinician should determine symptom severity and desire for treatment as early as possible. A general principle of care is the need to balance diagnostic certainty with the risk or invasiveness of therapy. In all women, the clinician should identify and treat reversible causes such as urinary tract infection, excessive fluid intake (>2 L/day), use or timing of medications that may worsen incontinence (ie, diuretics), and comorbid conditions contributing to incontinence (obesity, constipation, sleep apnea, tobacco use, dementia, and depression). The Box outlines signs or symptoms suggesting serious underlying pathology, such as cancer or serious neurologic disease, that should prompt immediate referral to an incontinence specialist. Most women do not require an extensive preliminary evaluation of urinary incontinence because initial noninvasive treatments may be begun without clear differentiation between the 2 most common urinary incontinence subtypes, stress and urgency incontinence. The history should focus on the onset, duration, severity, frequency and effect on quality of life. Figure 1 displays 3 simple items in a validated questionnaire to help clinicians discern the common incontinence subtypes. Briefly, the questionnaire describes various life situations and asks participants whether they experienced urinary incontinence during the past 3 months (even a small amount), whether they experienced involuntary urinary leaking, and when they experienced it most often.17. Quiz Ref IDStress incontinence is characterized by involuntary loss of urine with increases in abdominal pressure such as exercise or coughing. The main etiology is a poorly functioning urethral closure mechanism and is associated with loss of anatomic support or trauma from vaginal childbirth, obesity, and situations that repetitively increase intra-abdominal pressure, such as chronic constipation, heavy lifting, and high-impact exercise.18-23 Urgency incontinence is characterized by a sudden compelling desire to pass urine that is difficult to defer.24 Affected women experience little warning before incontinence episodes and an increase in urinary frequency both day and night. In most women, urgency incontinence is idiopathic. However, it is common in a subset of women with systemic neurologic conditions (eg, Parkinson disease, multiple sclerosis, pelvic or spinal nerve injury). Overflow incontinence symptoms are similar to those of stress and urgency incontinence, but this type of incontinence is associated with incomplete emptying of the bladder. It is more common in women with underlying systemic neurologic disease or anatomic abnormalities such as urethral obstruction. Many women with incontinence experience coexisting stress and urgency symptoms, usually called mixed urinary incontinence. Guidelines from international and specialty organizations are largely consistent in their recommendations for the initial incontinence evaluation, which includes history, physical examination, urinary tract infection testing, urinary stress testing, and assessment of postvoid residual.29-34 Urinalysis should be used to identify urinary tract infection and detect hematuria, pyuria, or glycosuria because these may represent comorbid conditions associated with incontinence. When history taking and urinalysis do not provide a clear etiology of incontinence symptoms, a written voiding diary recording quantity and timing of fluid intake and urine output during 1 to 3 days can provide information about potential modifiable factors associated with incontinence episodes. Figure 2 displays diaries of common abnormal voiding patterns. Improved fluid intake patterns can reduce urgency and frequency symptoms in women who infrequently drink large volumes of liquids. More frequent, regular voiding can reduce symptoms in women who have infrequent, large-volume voids. Pelvic examination is recommended when findings, such as detection of a pelvic mass, would alter the planned intervention or influence treatment selection. In postmenopausal women, clinicians should look for vaginal atrophy, which can effectively be treated with vaginal estrogen. Pelvic examination may identify conditions requiring prompt referral (Box). In addition, clinicians should look for pelvic organ prolapse beyond the vagina because it is associated with a higher risk of urinary retention. For these patients, referral to a specialist for treatment addressing both prolapse and incontinence may be warranted. Clinicians can assess pelvic floor muscle integrity and function during the bimanual pelvic examination by asking the patient to contract her pelvic floor muscles (Figure 3). Women who are unable to isolate pelvic floor muscles or who are unable to properly perform pelvic floor muscle contraction often benefit from supervised pelvic floor physical therapy instead of simple verbal instructions or handouts on pelvic exercises. When the diagnosis is unclear or the initial treatment is unsuccessful, consultation with an incontinence specialist can determine whether additional diagnostic studies are needed. Current clinical guidelines offer different recommendations about the utility of these diagnostic tests.29-34 All clinicians can conduct the simple urinary stress test. While in the lithotomic or standing position, the patient strains or coughs with a comfortably full bladder while the clinician directly observes the urethra meatus for urine leakage. Leakage during these maneuvers is highly suggestive of stress incontinence (positive predictive value of 78% to 97%).35 Urodynamic studies are not necessary in the evaluation of uncomplicated urinary incontinence or before every stress incontinence surgery. This conclusion is based on results of a multicenter randomized clinical trial of 630 women with stress urinary incontinence symptoms. In women with demonstrable stress incontinence, defined as a positive cough stress test result in the trial, a preoperative office evaluation provided a noninferior 12-month treatment outcome in women who underwent stress incontinence surgery.36 Urodynamic testing is still used when specialists seek specific information about bladder and urethral physiology or to characterize urinary incontinence subtypes. Within 10 minutes after a measured void, a postvoid residual should be obtained by either catheterization or ultrasonography. There is no established normative value for postvoid residual. Experts consider less than 100 mL for voided volumes of greater than 200 mL or one-third of total voided volume as normal. Postvoid residual measurement is recommended when patients report incomplete voiding, have pelvic organ prolapse beyond the hymen, or will undergo stress urinary incontinence surgery. Assessment of the postvoid residual is not required before medication is prescribed for urgency incontinence. However, because these medications can cause urinary retention, clinicians should stop the medication and proceed with additional assessment if new or worsening bladder symptoms develop. Selection of treatment is based on the nature of the predominant symptom (stress vs urgency incontinence), a woman’s goals and expectations for improvement or cure, her level of commitment to therapy, her tolerance of risk or adverse effects, and her financial situation. Some women prefer to attempt all conservative options before more invasive ones. Others may prioritize expediency or efficacy, accepting risks of surgery or more invasive approaches. Individual patient counseling should include information about expected symptom reduction, estimated time commitment, complications, and adverse effects, as well as expected out-of-pocket expense. Quiz Ref IDNearly all initial incontinence therapy should start with noninvasive measures because the benefits are associated with low risk and limited expense. Clinicians can offer these lifestyle modifications, including smoking cessation, regardless of the incontinence subtype. Management of constipation and avoidance of excessive fluids, with reduction in consumption of caffeine, carbonated beverages, diet beverages, and alcohol, should be discussed.37 Fluid-management strategies promote frequent intake of small amounts of fluid (ie, 4-5 oz/hour) up to 2 L a day of predominantly water in lieu of large, episodic fluid intakes (ie, 36 oz in one drink). Timed voiding measures, or voiding at intervals that are tailored to each patient (typically every 2 to 3 hours) during the day, can reduce urgency incontinence episodes. Although systematic reviews do not provide strong evidence to support these strategies, in the authors’ clinical experience, timed voiding and avoidance of excessive fluids are effective strategies, especially for patients with urgency incontinence.38-40Table 1 summarizes results of important, landmark, randomized trials about urgency incontinence. A multicenter randomized controlled trial evaluated the efficacy of supervised behavioral modification (including pelvic floor muscle exercise instruction, strategies to suppress urge, timed voiding, and fluid management) in addition to drug therapy (tolterodine) for urgency incontinence. Compared with drug therapy alone, combined therapy was more successful, defined by a greater than 70% reduction in incontinence episodes (58% for drug therapy vs 69% for combined therapy). The rates of continued drug use were not different (41%) at 8 months.41. Quiz Ref IDStrong evidence supports the recommendation for weight loss in overweight women with incontinence. A randomized clinical trial of a 6-month structured weight loss program vs education alone in 338 overweight and obese women reported a 47% reduction in mean incontinence episodes compared with a 28% reduction in the control group (P = .01).44 The treatment group had a mean 7.8-kg reduction in weight (8%) vs a mean 1.5-kg reduction (1.6%) in the control group, and these patients were more likely than controls to have a clinically meaningful reduction in all incontinence episodes (47% vs 28%; P < .01). Women in the treatment group experienced a decrease in weekly incontinence episodes, from a baseline mean (SD) of 24 (18) episodes  to 13 (15) episodes. The effect was more pronounced for stress incontinence, with a reduction from 9 (11) to 4 (7) episodes (58% vs 33%; P = .02).44. Systematic reviews consistently report efficacy for pelvic floor muscle exercises for women with urinary incontinence.45-50 Although there are no significant risks or expenses for unsupervised exercises, they require personal engagement and time commitment. Clinicians can provide simple exercise instructions for the patient if she is able to contract her pelvic floor muscle. Thirty contractions per day (3 sets of 10 contractions held for 10 seconds each) is typically recommended; patients should not be instructed to interrupt their urine stream while performing their daily exercises. There are numerous modalities to assist with pelvic floor muscle exercises, but there is insufficient evidence to suggest that any specific exercise program is superior to another.50 A systematic review of clinical trials focusing on 12-month outcomes for supervised pelvic floor muscle training reported stress urinary incontinence cure rates of 58.8% at 12 months. Patients considered cured reported being completely continent or had no evidence of stress urinary incontinence on physical testing, with a significant reduction in their incontinence episodes.51 The addition of vaginal weighted cones, biofeedback, or other feedback may improve these cure rates over exercise alone.48,49 Women should be encouraged to pursue a modality that facilitates compliance. There is some evidence that bladder control pessaries are effective and may be preferable for women who have stress urinary incontinence during specific situations; for example, only during exercise.52Table 2 highlights important, high-impact, stress incontinence treatment trials. In a multicenter randomized trial of pessary vs behavioral therapy with pelvic floor exercises vs combination therapy, 33% of women treated with a pessary reported no bothersome incontinence compared with 49% in the behavioral therapy group (P = .006). Although overall satisfaction at 3 months was higher with the behavioral therapy group (75% vs 63%; P = .02), there were no differences after 12 months, with 50% overall satisfaction.53 Over-the-counter vaginal insert devices, such as Impressa, may provide an alternative noninvasive treatment, although comparative data are lacking. The addition of vaginal devices to pelvic floor exercises is not more effective than either modality alone.50,53 A recent randomized trial from China reported the efficacy of acupuncture for stress incontinence.60 Future studies will be needed to determine the role of acupuncture for women in the United States, given the limited availability and lack of insurance coverage. There are no FDA-approved medications for stress incontinence. There are 6 FDA medications in the primary medication class for urgency incontinence (darifenacin, fesoterodine, oxybutynin, solifenacin, tolterodine, and trospium). These medications are used as second-line treatment.32Table 3 displays the magnitude of improvements reported in FDA regulatory trials of medication for treatment of urgency incontinence. Most efficacy data for these medications are from short-term, industry-supported studies with moderate- to high-level evidence to support efficacy compared with placebo. The medications for treating urgency incontinence have not all been directly compared for efficacy or adverse effects. The magnitude of improvements reported in FDA regulatory trials ranges from 53% to 80% reduction in urinary incontinence episodes and 12% to 32% reduction in urinary frequency, with placebo rates of improvement from 30% to 47% for incontinence episodes and 8% to 15% for urinary frequency. Medication selection is generally made according to formulary availability, patient costs, and specific clinical factors. Contraindications to anticholinergic medications include untreated narrow-angle glaucoma (an uncommon condition). This class of drugs may aggravate existing cardiac arrhythmias. Although 2 drugs in this class, solifenacin and tolterodine, are reported to prolong the QT interval, routine electrocardiogram is not recommended before prescribing this medication. Recent research raises questions regarding the association of long-term anticholinergic exposure with dementia.68. β-3 Agonists are also available for treating urgency incontinence. Stimulation of the β-3 pathway promotes smooth muscle relaxation of the bladder to increase urine storage. Mirabegron, the only FDA-approved drug in this class, has efficacy that is better than placebo’s and not different from that of anticholinergics, and has reported symptom control rates of 43.5% to 45.8% at 12 months.51,65,69,70 Mirabegron’s adverse effects include the possibility of increasing hypertension. It may provide synergistic effects with anticholinergic medications in women who have insufficient response with monotherapy.71-73. Table 3 displays the rates of the most common adverse effects, constipation and dry mouth, as reported in registration studies of FDA-approved medications for urgency incontinence. A systematic review of 86 trials comparing anticholinergic medications revealed comparisons between different therapies for only 4 drugs: oxybutynin, tolterodine, solifenacin, and fesoterodine. The authors concluded that immediate-release tolterodine may be associated with less dry mouth compared with immediate-release oxybutynin, and extended-release formulations of these drugs should be used in preference to immediate-release ones to minimize dry mouth; solifenacin may have better efficacy than immediate-release tolterodine; and fesoterodine may be more efficacious than extended-release tolterodine, with more adverse effects. There were insufficient data to evaluate other anticholinergics or to compare quality of life, cost, or long-term success.74 The only studies comparing mirabegron to other drugs have been industry sponsored in the setting of comparing combined therapy (ie, mirabegron plus low-dose solifenacin) vs monotherapy. A recent study demonstrated significant reduction in urgency incontinence episodes with 50 mg mirabegron plus 5 mg solifenacin compared with 5 mg solifenacin alone (71% vs 54%; mean adjusted difference, –0.2; P = .03); however, no significant difference was achieved compared with mirabegron 50 mg alone (61% reduction). The study was not designed to compare mirabegron 50 mg vs solifenacin 5 mg alone. The placebo group had a 42% reduction in incontinence episodes, and the 10-mg dose that is typically prescribed for solifenacin was not studied.73. Women whose predominantly stress incontinence symptoms persist despite conservative measures may be candidates for surgery. Table 2 displays important stress incontinence treatment trials. Surgery is highly effective, with median cure rates of 84.4% (interquartile range, 74%-90.1%) at 12 months.51 Historically, the standard surgery for stress incontinence included a retropubic urethropexy or a pubovaginal sling. A multicenter randomized controlled trial of these 2 procedures in 655 women revealed higher stress-incontinence-specific success rates (66% vs 49%; P < .001) but higher morbidity (6.1% vs 0% voiding dysfunction requiring reoperation) for the pubovaginal sling compared with urethropexy.56,57 A European randomized trial of retropubic Burch colposuspension vs retropubic midurethral sling in 344 women revealed no difference in success rates at 6 months and 5 years.54,55 Currently, the most commonly performed surgery is the midurethral sling, a 30-minute outpatient procedure in which a synthetic mesh sling is placed through either a retropubic or transobturator approach.78,79 The midurethral sling is the most extensively studied anti-incontinence operation, with documented short-term efficacy (62% to 98%) and long-term efficacy (>5 years: 43% to 92%).79,80 Complication rates are low and synthetic mesh erosion occurs in less than 5% of patients.79 Mesh erosions may require excision because of discharge, bleeding, and pain in the patient and/or her male sexual partner. The Trial of Midurethral Slings revealed similar objective success rates (77.7%-80.8%) and patient satisfaction (85.9%-90%) at 1 year, with small differences in subjective success at 2 years (55.7% vs 48.3% for retropubic vs obturator, respectively).58,59. OnabotulinumtoxinA (100 U) is injected into the bladder through a cystoscope with local anesthetic in an office setting. The drug blocks the presynaptic release of acetylcholine to decrease muscarinic receptor activation involved in detrusor contraction. Treatment is effective in approximately 65% of participants for approximately 6 to 12 months.42 Treatment risks include urinary retention (8%-10%) and urinary tract infections (35%). A large multicenter clinical trial of oral anticholinergic medication demonstrated reductions in incontinence episodes similar to those with onabotulinumtoxinA (100 U) (68% and 66%, respectively) (Table 2).86 More participants reported resolution of urge urinary incontinence after treatment with onabotulinumtoxinA (13% vs 27%; P = .003). In this issue of JAMA, Laptook et al1 report the results of a clinical trial investigating the effect of hypothermia administered between 6 and 24 hours after birth on death and disability from hypoxic-ischemic encephalopathy (HIE). Hypothermia is beneficial for HIE when initiated within 6 hours of birth but administering hypothermia that soon after birth is impractical.2 The study by Laptook et al1 addressed the utility of inducing hypothermia 6 or more hours after birth because this is a more realistic time window given the logistics of providing this therapy. Performing this study was difficult because of the limited number of infants expected to be enrolled. To overcome this limitation, the investigators used a Bayesian analysis of the treatment effect to ensure that a clinically useful result would be obtained even if traditional approaches for defining statistical significance were impractical. The Bayesian approach allows for the integration or updating of prior information with newly obtained data to yield a final quantitative summary of the information. Laptook et al1 considered several options for the representation of prior information—termed neutral, skeptical, and optimistic priors—generating different final summaries of the evidence. Prior information is a form of assumption. As with any assumption, incorrect prior information can result in invalid or misleading conclusions. For instance, if prior information used the assumption that hypothermia becomes less effective with increasing postnatal age and, in fact, waiting until 12 to 24 hours was associated with the greatest benefit, the resulting inferences would likely be incompatible with the data, less accurate, or biased. If the statistical model uses prior information derived from neonates 0 to 6 hours old in evaluating the treatment effect in neonates 6 to 24 hours of age, and is based on the assumption that the patients respond similarly, the results may be biased or less accurate if the 2 age groups actually respond differently to treatment. Laptook et al1 incorporated prior information by allowing for the outcome to vary across time windows of 6 to 12 hours and 12 to 24 hours and prespecifying 3 separate prior distributions on the overall treatment effect (Description of Bayesian Analyses and Implementation Details section of the eAppendix in Supplement 2). The neutral prior assumes that the treatment effect diminishes completely after 6 hours, the enthusiastic prior assumes that effect does not diminish at all after 6 hours, and the skeptical prior assumes that the treatment is detrimental after 6 hours. Primary results are presented based on the neutral prior and, as such, the authors’ approach is transparent and easily interpretable. The authors found a 76% probability of benefit with the neutral prior, a 90% probability of benefit with the enthusiastic prior, and a 73% probability of benefit with the skeptical prior.1. An alternative to this approach might include specifying a model that relates postnatal age at the start of therapeutic hypothermia to the magnitude of the treatment effect, assuming that the effect does not increase over time. This model would explicitly account for a possible decrease in treatment benefit with increasing age at initiation, while still allowing the effect at each age to inform the effects at other ages. Additionally, this model could be heavily informed or anchored in the 0 to 6–hour range using data from previous studies.2 With this anchor, inferences would be improved across the range of 6 to 24 hours, with a particular increase in precision for the time intervals closer to 6 hours. This may have allowed more definitive conclusions to be drawn from the same set of data. Laptook et al1 used a prespecified Bayesian analysis, using prior information, to allow quantitatively rigorous conclusions to be drawn regarding the probability that therapeutic hypothermia is effective 6 to 24 hours after birth in neonates with HIE. Conclusions of the analysis were given as probabilities that benefit exists. For example, the statement that there is “a 76% probability of any reduction in death or disability, and a 64% probability of at least 2% less death or disability” are easily understood by clinicians and can be used to inform clinical care. The use of several options for prior information allows clinicians with different perspectives to have the data interpreted over a range of prior beliefs. Two randomized, double-blind trials compared intravaginal prasterone to placebo in a total of 640 postmenopausal women with VVA who reported moderate-to-severe dyspareunia as their most bothersome symptom. After 12 weeks of daily intravaginal administration, women were asked to self-evaluate the severity of their symptoms using a 4-point scale corresponding to none (0), mild (1), moderate (2), or severe (3). Dyspareunia severity scores were significantly lower with prasterone (by 0.40 units in trial 1 and 0.36 in trial 2 over placebo). Vaginal cytology and pH tests performed to assess changes that are associated with atrophy also showed significant improvement.8,9. Enter Channon and Antoniades’ new study. First they set out to confirm their recent discovery that inflamed human arteries send signals to the fat tissue surrounding them, which senses these signals and changes its composition. To do so, they obtained pieces of aortic tissue from patients undergoing cardiac surgery, and at the same time isolated immature fat cells from adipose tissue deposits surrounding coronary arteries of patients. The fat cells were then cocultured with human aortic tissue pretreated with a proinflammatory agent. The proportion of the US population 65 years or older is expected to almost double by 2050, to more than 87 million, when surviving baby boomers will be older than 85 years and likely frail. Older adults’ needs for assistance in activities of daily living will increase, challenging both care delivery systems and costs. With the average annual cost of a private room in a nursing home now more than $100 000, keeping people in their own homes and communities could dramatically reduce federal Medicaid spending. But how can this be accomplished?. The Altarum Institute Center for Elder Care and Advanced Illness has proposed a model called MediCaring communities for keeping frail elders in the community while enhancing quality of life and reducing the use of medical services. It requires a person-centered approach that integrates long-term services and supports (LTSS) with medical and nursing care. These services may include care coordination, home-delivered meals, dementia training programs for home health aides, home maintenance and modification programs to help with disability, and personal care. Altarum suggests that the Program of All-Inclusive Care for the Elderly (PACE) could launch a MediCaring community program if Centers for Medicare & Medicaid Services (CMS) rules were more flexible, for example, by allowing PACE programs to serve Medicare-only patients according to their health status. PACE started in San Francisco in 1973 with On Lok (Cantonese for “peaceful, happy home”), an organization serving an Asian American community. There are now 233 PACE centers in 31 states serving about 40 000 nursing-home eligible people older than 55 years. With capitated payments from Medicare and Medicaid (if approved by states), PACE programs assume financial risk and provide comprehensive social, medical, and nursing services. Services include adult day care, transportation to and from the center, food programs, home care, primary care, social services, and other preventive services, all at a lower cost than institutionalization. The CAPABLE (Community Aging in Place, Advancing Better Living for Elders) program could augment the MediCaring community model by building older adults’ capacity to remain in their homes. Initially tested with a grant from CMS, CAPABLE uses registered nurses, occupational therapists, and handymen to provide Medicare and Medicaid beneficiaries with home repairs, coaching on self-management to maintain independence, and making referrals, as needed. It reduces disability by improving ability to perform activities of daily living, and lowers rates of emergency department use, hospital readmissions, and depression. Sarah Szanton, PhD, RN, professor of nursing at Johns Hopkins University and founder of CAPABLE, reports that the documented cost savings are sufficient to prompt some home care agencies, 5 cities in Michigan, accountable care organizations, insurers, and others to adopt CAPABLE or integrate it into other models of care. Second, many communities are likely to need funds to create MediCaring communities based on the PACE model. PACE requires substantial preparation to establish a network of services, including housing for the day center, transportation, connections with medical and nursing care professionals, and administrative infrastructure. Peter Fitzgerald, MSc, executive vice president of policy and strategy for the National PACE Association, says, “PACE is not easy. You have to establish provider capacity, and that takes time.” . For example, Balancing Incentives provided states with enhanced federal funding over 5 years for improving and expanding home and community-based services through an equitable, user-friendly, consistent process. The enhanced funds helped states create enrollment systems with  “No Wrong Door”/Single Entry Point for all people in need of information about and access to LTSS, core standardized assessments, and conflict-free case management services. In an evaluation of the program, Mathematica found it successful in increasing the proportion of Medicaid beneficiaries in community settings at no detriment to their health and with increases in quality of life.  . Third, regulatory barriers for Medicare and Medicaid get in the way, providing a “deregulatory opportunity” for the Trump administration.  Certain regulations can facilitate the expansion of PACE, for example, such as expanding PACE to disabled people younger than 55 years and to older adults who are not yet nursing home eligible. This was authorized under the PACE Innovative Act but doesn’t have implementing regulations yet. The Bipartisan Policy Center has proposed, among other things, that Medicare’s payment risk adjustment incorporate a measure of functional impairment because it drives health care use and nursing home placement, and that Medicare pay for supportive services that can reduce the need for institutional care. By age group, incidence was far higher among children aged 1 to 4 years—75 per 1 million—than it was among infants or older children and teens. The incidence of ALL was higher among boys than girls; it also was higher in Western states compared with other parts of the country. Rates were higher in metropolitan areas with a population of 1 million or more than in rural communities. Counties in the top 25th income percentile also had higher incidence rates than did lower income areas. Brucella infections can cause brucellosis, which initially results in fever, sweats, aches, and fatigue. Without treatment, however, long-term complications including arthritis, heart problems, or enlargement of the spleen or liver may result. Postexposure prophylaxis should include doxycycline and trimethoprim-sulfamethoxazole or another suitable antimicrobial for 21 days. The CDC recommends monitoring for broader symptoms such as recurrent fever, depression, endocarditis, neurologic symptoms, and testicle swelling for 6 months after the last exposure. At least 1 illness in Texas has been linked with the dairy. Reports of people who drank K-Bar milk or had brucellosis-like symptoms from RB51 came from a number of states, including Ohio, Tennessee, and North Dakota. In all, the CDC and Texas health officials tried to reach more than 800 households known to have purchased K-Bar raw milk. Of 485 households with contact information, the CDC had reached 236 by mid-September. Some 83% of people in those households had been exposed to RB51 by drinking the milk. This is an international, multicenter, randomized, unblinded, parallel-group trial13 comparing robotic-assisted vs conventional laparoscopic surgery for the curative treatment of rectal adenocarcinoma (distal extent ≤15 cm of the anal margin) by high anterior resection, low anterior resection, or abdominoperineal resection. Participating surgeons had to perform at least 30 minimally invasive (conventional laparoscopic or robotic-assisted laparoscopic) rectal cancer resections before taking part in the trial, of which at least 10 had to be conventional laparoscopic resections and at least 10 had to be robotic-assisted laparoscopic resections.14 The trial received national ethical approval in the United Kingdom or either local ethical committee or institutional review board approval at international centers. An independent trial steering committee and data monitoring and ethics committee oversaw the trial conduct. All participants provided written informed consent. The specifics of each operation were at the discretion of the operating surgeon. The only absolute requirement under robotic surgery was that the robot had to be used for mesorectal resection. Pathology reporting was according to internationally agreed criteria.16 Patient self-reported bladder function and sexual function were measured at baseline and 6 months following surgery with the International Prostate Symptom Score (I-PSS), International Index of Erectile Function (IIEF), and Female Sexual Function Index (FSFI). The I-PSS17 is a standardized, patient self-reported measure of the subjective problems that the patient experiences with urinating, with scores ranging from 0 to 35 and higher scores indicating more severe symptoms. The IIIEF18 is a patient self-reported measure developed for the assessment of erectile function, with scores ranging from 5 to 75 and lower scores indicating greater severity of dysfunction. The FSFI19 is a patient self-reported measure of sexual function in women, with scores ranging from 2 to 36 and higher scores indicating greater function. Patients underwent clinical review at 30 days and 6 months postoperatively. Annual follow-up is continuing. The primary end point was the rate of conversion to open surgery, defined as the use of a laparotomy wound for any part of the mesorectal dissection. The use of a small abdominal wound to facilitate a low, stapled anastomosis and/or specimen extraction was permissible and not defined as an open conversion. Secondary end points were all prespecified and included pathological circumferential resection margin positivity (CRM+; defined as tumor ≤1 mm), intraoperative complications, postoperative (30-day and 6-month) complications, 30-day operative mortality, patient-reported bladder and sexual function, and pathological assessment of the quality of the plane of surgery. Quality of the plane of surgery was judged according to the method of Quirke and Dixon,20 grading the pathology specimen in terms of completeness of surgical resection. For high and low anterior resection, this was defined as mesorectal (best), intramesorectal (intermediate), and muscularis propria (worst). For abdominoperineal excision, this was defined as levator (best), sphincteric (intermediate), and intrasphincteric (worst). Other prespecified secondary end points, not reported herein, include central pathology review with photographic documentation of resection specimens, and a full quality-of-life analysis. A full health economic evaluation was undertaken separately. Longer-term end points (local recurrence rates, disease-free survival, and overall survival) will be reported at 3 years after the last patient randomization. The target sample size was 400 patients, which provides 80% power at the 5% (2-sided) level of significance to detect a reduction in the conversion rate from 25% in the conventional laparoscopic group to 12.5% in the robotic-assisted laparoscopic group, allowing for 16% attrition.13 The anticipated conversion rate in the conventional laparoscopic group was based on the MRC CLASICC Trial, which was the best available evidence at that time. The MRC CLASICC trial reported a conversion rate of 34% for conventional laparoscopic rectal cancer resection,21 which was reduced to 25% to account for advances in surgical technique. Sufficient funding was available to extend recruitment to 471 patients to take advantage of excellent patient recruitment and maximize the power of the study. This decision was made in consultation with the independent trial steering committee and data monitoring and ethics committee without review of data or an interim analysis being performed. All analyses were prespecified and were conducted on the intention-to-treat population, ie, all randomized patients were accounted for in the analyses, and patients were categorized into treatment groups based on their randomization regardless of what treatment they subsequently received. Complete case analyses were performed for all prespecified end points. When the complete case analysis excluded more than 3% of patients due to missing data, exploratory analyses to investigate the effect of missing data were performed. Specifically, to explore the mechanism of the missing data and the validity of a complete case analysis for each end point, patient characteristics were compared between those with and without missing data and multilevel logistic regression models were used to identify any associations between prognostic variables and whether a patient had missing data, to inform whether data were missing at random. All hypothesis tests were 2-sided and conducted at the 5% level of significance. Estimates and their corresponding 95% confidence intervals and P values are presented for fixed effects. For the (random) surgeon effect, the intracluster correlation coefficient, estimated via the analysis-of-variance method, and bias-corrected bootstrapped 95% confidence intervals are reported.22,23 Analyses were carried out in SAS version 9.4 statistical software (SAS Institute Inc). Sensitivity analyses were performed to determine the robustness of the findings from the primary analysis, including extension of the primary analysis to account for potential learning effects by including interaction terms for the operating surgeon’s level of relevant robotic-assisted and conventional laparoscopic experience and the treatment effect. Subgroup analyses relating to the primary end point across sex, BMI class, and procedure received as well as relating to CRM+ across sex, BMI class, and T stage were performed. All subgroup analyses tested heterogeneity of the treatment effect across the subgroups and also estimated the treatment effect within each subgroup, via the inclusion of an appropriate interaction term. All sensitivity analyses and subgroup analyses were prespecified. Cost analysis was undertaken from the perspective of a public (ie, UK National Health Service [NHS]) health care professional for all patients (eAppendix 1 and eTable 1 in Supplement 2). Resource utilization data for 190 UK and US patients were collected at baseline, intraoperatively, 30 days postoperatively, and 6 months postoperatively using study forms. Costs were computed in British pounds using a price year of 2015 and estimated using UK NHS unit costs from national data sources including the NHS Reference Costs database, Personal Social Services Research Unit costs of health and social care, and British National Formulary. For reporting purposes, costs are converted with 2015 Organisation for Economic Co-operation and Development purchasing power parity (0.866 British pound per 1 US dollar) and reported herein as 2015 US dollars. Multiple imputation methods were used for missing data. Sensitivity analysis was undertaken to account for uncertainty (eAppendix 2, eTables 2 and 3, and eFigure in Supplement 2). Given wide variation in costs due to contractual arrangements, acquisition and maintenance costs for the robotic-assisted and conventional laparoscopic systems were excluded. Between January 7, 2011, and September 30, 2014, 1276 patients were assessed for eligibility by 40 surgeons from 29 sites across 10 countries (United Kingdom, Italy, Denmark, United States, Finland, South Korea, Germany, France, Australia, and Singapore). Recruitment by country was 131 patients (6 sites) in the United Kingdom, 105 patients (5 sites) in Italy, 92 patients (3 sites) in Denmark, 59 patients (9 sites) in the United States, 35 patients (1 site) in Finland, 18 patients (1 site) in South Korea, 16 patients (1 site) in Germany, 11 patients (1 site) in France, 2 patients (1 site) in Australia, and 2 patients (1 site) in Singapore. A total of 471 patients (36.9%) were randomized: 234 to conventional laparoscopic surgery and 237 to robotic-assisted laparoscopic surgery (Figure 1). A total of 466 patients underwent an operation, with 456 (97.9%) undergoing the allocated treatment. Follow-up for analysis was at 30 days and 6 months, with a final follow-up date of June 16, 2015. The 2 treatment groups were well balanced with respect to baseline characteristics and operative procedures (Table 1). Of the 466 cases included in the primary intention-to-treat analysis, low anterior resection was performed in 317 (68.0%) and abdominoperineal resection was performed in 97 (20.8%). The mean operative time was 37.5 minutes longer in the robotic-assisted laparoscopic group than in the conventional laparoscopic group (mean [SD] operative time, 298.5 [88.71] vs 261.0 [83.24] minutes, respectively). The length of hospital stay was similar between groups. Conversion to open surgery occurred in 47 of 466 patients (10.1%) overall: 28 of 230 patients (12.2%) in the conventional laparoscopic group and 19 of 236 patients (8.1%) in the robotic-assisted laparoscopic group (unadjusted difference in proportions, 4.1% [95% CI, −1.4% to 9.6%]). There was no statistically significant difference between robotic-assisted and conventional laparoscopic surgery with respect to odds of conversion (adjusted OR = 0.61 [95% CI, 0.31 to 1.21]; P = .16). Table 2 presents results from the multilevel logistic regression model and shows significantly increased odds of conversion in obese patients as compared with underweight or normal-weight patients (adjusted OR = 4.69 [95% CI, 2.08 to 10.58]; P < .001) and in men as compared with women (adjusted OR = 2.44 [95% CI, 1.05 to 5.71]; P = .04). Patients whose intended procedure was a low anterior resection had a significantly higher rate of conversion as compared with patients whose intended procedure was abdominoperineal resection (adjusted OR = 5.44 [95% CI, 1.60 to 18.52]; P = .007). Operating surgeon had a mild-to-moderate effect on odds of conversion, as reflected by the intracluster correlation coefficient estimate of 0.05 (95% CI, 0.01 to 0.06). No results from the prespecified subgroup analyses were statistically significant. Regarding the sex subgroup analysis, 39 of 317 men (12.3%) underwent conversion to laparotomy, 25 of 156 (16.0%) in the conventional laparoscopic group and 14 of 161 (8.7%) in the robotic-assisted laparoscopic group (unadjusted difference in proportions = 7.3% [95% CI, 0.1% to 14.6%]); 8 of 149 women (5.4%) underwent conversion to laparotomy, 3 of 74 (4.1%) in the conventional laparoscopic group and 5 of 75 (6.7%) in the robotic-assisted laparoscopic group (unadjusted difference in proportions = −2.6% [95% CI, −9.8% to 4.6%]). A Wald test of interaction between treatment effect and sex in the adjusted model yielded P = .09, and the estimated adjusted OR for conversion to laparotomy (robotic-assisted vs conventional laparoscopic surgery) in men given by the model was 0.46 (95% CI, 0.21 to 0.99; P = .04). Further details on all subgroup analyses are given in eAppendix 3 and eTables 4, 5, and 6 in Supplement 2. The pathological outcomes are shown in Table 1. The majority of tumors, 356 of 466 (76.4%), were pT2 or pT3. The total number of lymph nodes retrieved at pathology (lymph node yield) was high in both the conventional and robotic-assisted laparoscopic groups (mean [SD], 24.1 [12.91] vs 23.2 [11.97] lymph nodes, respectively). Of 466 patients who had an operation, 459 (98.5%) had complete pathology data available. Of these 459 patients, 26 (5.7%) had CRM+: 14 of 224 (6.2%) in the conventional laparoscopic group and 12 of 235 (5.1%) in the robotic-assisted laparoscopic group (unadjusted difference in proportions = 1.1% [95% CI, −3.1% to 5.4%]). There was no statistically significant difference in the odds of CRM+ between the groups (adjusted OR = 0.78 [95% CI, 0.35 to 1.76]; P = .56) (Table 3). Subgroup analyses were largely uninformative owing to the low overall CRM+ rate. Proximal margin involvement was not observed in any patients, and distal margin involvement was observed in only 1 patient in the conventional laparoscopic group. Pathological assessment of the quality of the plane of surgery for the mesorectal area was captured for 456 of 466 patients (97.9%), and the quality of the plane was of the highest standard (mesorectal plane) in 351 of 456 cases (77.0%): 173 of 223 (77.6%) in the conventional laparoscopic group and 178 of 233 (76.4%) in the robotic-assisted laparoscopic group (unadjusted difference in proportions = 1.2% [95% CI, −6.5% to 8.9%]). There was no statistically significant difference in the odds of achieving the highest-standard plane of surgery (mesorectal plane) between the groups (adjusted OR = 0.94 [95% CI, 0.56 to 1.57]; P = .14) (Table 3). Table 4 shows the complication rates up to 6 months postoperatively. Of 466 patients, 70 (15.0%) had an intraoperative complication: 34 of 230 (14.8%) in the conventional laparoscopic group and 36 of 236 (15.3%) in the robotic-assisted laparoscopic group (unadjusted risk difference = −0.5% [95% CI, −6.0% to 7.0%]). There was no significant difference between the groups (adjusted OR = 1.02 [95% CI, 0.60 to 1.74]; P = .94). The most common intraoperative complications were damage to an organ or structure, significant hemorrhage, and surgical equipment failure. Overall, 151 patients (32.4%) reported a complication within 30 days: 73 of 230 patients (31.7%) in the conventional laparoscopic group and 78 of 236 (33.1%) in the robotic-assisted laparoscopic group (unadjusted risk difference = −1.3% [95% CI, −9.8% to 7.2%]). There was no significant difference between the groups (adjusted OR = 1.04 [95% CI, 0.69 to 1.58]; P = .84). Seventy-two patients (15.5%) reported a complication after 30 days and within 6 months: 38 of 230 patients (16.5%) in the conventional laparoscopic group and 34 of 236 (14.4%) in the robotic-assisted laparoscopic group (unadjusted risk difference = 2.1% [95% CI, −4.5% to 8.7%]). There was no significant difference between the groups (adjusted OR = 0.72 [95% CI, 0.41 to 1.26]; P = .25). The occurrence of anastomotic leak was determined by the surgeon and reported as gastrointestinal complication. Of the 361 patients with an anastomosis, 40 (11.1%) reported an anastomotic leak within 6 months: 18 of 181 patients (9.9%) in the conventional laparoscopic group and 22 of 180 (12.2%) in the robotic-assisted laparoscopic group. The primary outcome measure was conversion to open surgery, based on the hypothesis that the technological advantages of the robot should facilitate rectal cancer resection and avoid the need to convert to an open operation. The sample size calculations were based on best available evidence in 2009, including the largest randomized clinical trial of conventional laparoscopic rectal cancer surgery, the MRC CLASICC trial, which reported a 34% conversion rate to open surgery.21 A 25% conversion rate from conventional laparoscopic to open surgery was assumed, giving a sample size of 400 patients to demonstrate a 50% relative reduction in the conversion rate with robotic-assisted surgery. The actual overall conversion rate was much lower, 10.1%. A similar reduction in conversion rates with time has been reported in other conventional laparoscopic rectal cancer trials: COLOR II, 16%24; ACOSOG Z6051, 11%4; and ALaCaRT, 9%.3 In our trial, the difference in conversion rates between conventional laparoscopic surgery (12.2%) and robotic-assisted laparoscopic surgery (8.1%) was not statistically significant. The statistically significant lower overall conversion in patients undergoing low anterior resection, as compared with abdominoperineal resection, probably reflects that the majority of the oncological component of the operation is performed from the perineum in the abdominoperineal approach and is less affected by the laparoscopic approach. Similarly, the higher overall conversion rates for men (as compared with women) and obese patients (as compared with underweight or normal-weight patients) probably reflect the increasing technical difficulty in these patients. The experience of the participating surgeons was also evident in the low CRM+ rate (overall, 5.7%), which was lower than previous trials studying conventional laparoscopy for rectal cancer: COLOR II, 10%; ACOSOG Z6051, 12.1%; and ALaCaRT, 7%. Pathological grading of the plane of surgery showed a good standard, with mesorectal plane surgery observed in 75.3% overall. This is lower than reported in COLOR II (88%) and ALaCaRT (87%), but similar to ACOSOG Z6051 (72.9%), and is probably due to the recognized variation in reporting between pathologists. In our trial, reporting of the pathological plane of surgery was standardized to the method described by Nagtegaal and Quirke.25. Results from the health economics analysis suggest that robotic-assisted laparoscopic surgery for rectal cancer is unlikely to be cost-saving. The mean difference per operation, excluding the acquisition and maintenance costs, was £980 ($1132) and driven by longer operating theater time and increased costs for robotic instruments. When considering robotic-assisted laparoscopic surgery as a whole, rather than just rectal cancer surgery, one has to consider the cost of purchase and maintenance of the system, the operational life, and the total utilization of the robot per year for all robotic procedures. Estimates of acquisition costs in 2017 vary between $0.6 million and $2.5 million, with maintenance costs between $0.08 million and $0.17 million per year.29 Assuming a midpoint acquisition cost of $1.55 million and midpoint maintenance cost of $0.125 million per year, with an operational life and amortization period of 7 years,30,31 the total cost of a robot would be approximately $2.425 million. Estimates for total utilization of the robot per year in 2017 vary between 819 000 and 843 000 procedures across 3919 installed systems, or 1505 procedures per robot over 7 years.29 This gives the total fixed cost of around $1611 per procedure, in addition to the variable costs. No blinding to treatment allocation was incorporated into this trial. The primary end point and the measure of mortality were certainly unaffected by this as objective end points. However, there is the potential for end points that are not completely objective to have been affected. In our pathology end points, including CRM+, we have guarded against this by carrying out a blinded central review of pathology assessments. WGS identified a new monogenic variant with disease risk in 22% of the FH+ WGS group, but only 4% had clinically confirmed abnormalities associated with the new variant. Seventy-three percent of PCPs managed the genetic findings appropriately. They recommended clinical action for 16% of patients in the FH group and 34% of patients in the FH + WGS group. Whole-genome sequencing did not cause patient anxiety or depression, but 41% of those in the WGS + FH group and 30% in the FH group reported making health behavior changes after receiving results. Health care costs in the 6 months following disclosure of results averaged $300 more per patient in the sequenced group. The further analysis of the 2016 report, which was based on data covering the years 2001-2014 and highlighted in a fact sheet, showed that the suicide rate among female veterans in 2014 was 19 per 100 000, a 62.4% increase since 2001. The figure is roughly 2.5 times higher than the rate among nonveteran US women (7.2 per 100 000). By contrast, among male US veterans, the 2014 suicide rate was 37.2 per 100 000, a 29.7% increase since 2001. The rate is 19% higher than that among US nonveteran males (25 per 100 000). But even though the gap between male and female veterans is closing slightly, men are still nearly twice as likely to take their own lives as women. When we find a patient able to take repeated poisonous doses of such a drug as opium, with perfect impunity, and when it is demonstrated that such doses are essential to his bodily comfort, or even necessary to sustain life itself, certainly there is the justifiable inference that some change in the animal economy has taken place, that a departure from normal action is set up, and that a pathological condition is established. Dr. Clouston, in the July, 1890, number of the Quarterly Journal of Inebriety, clearly recognizes this diseased condition, while Dr. Charles L. Hughes, of St. Louis, in the Alienist and Neurologist for July, 1886, makes mention of it in the following language: “The long continued use of opium or of its salts engenders a disorder of the nervous system which is entitled to distinct recognition.…”. Stress incontinence occurs when the muscles surrounding the urethra do not squeeze strongly and urine leaks out through the urethra accidentally. This often occurs during activities that increase pressure inside the abdomen, such as laughing, sneezing, or exercising. Vaginal childbirth or activities such as repeated heavy lifting can affect the function of the muscles of the pelvic floor and lead to stress incontinence. Urgency incontinence occurs when a strong urge to urinate occurs at the wrong time or place. Urgency incontinence often does not have a single clear cause. Women with neurologic conditions that affect the nerves that travel from the brain to the bladder can have urgency incontinence. Mixed urinary incontinence typically involves aspects of both stress incontinence and urgency incontinence. Incontinence may also be caused by urinary tract infections. The diagnosis of urinary incontinence begins with a detailed medical history performed by a health care professional. A physical examination may be performed to assess the anatomy of the pelvic region. A test of the urine (urinalysis) should be obtained to check for evidence of infection. A voiding diary may be used to document the amount and timing of fluid intake and urine output to aid the diagnosis. If the diagnosis remains uncertain after these initial evaluations or the symptoms do not improve after initial treatments, additional tests may be needed. Chang HJ, Lynm C, Glass RM. Urinary incontinence in older women. JAMA. 2010;303(21):2208. Lukacz ES, Santiago-Lastra Y, Albo ME, Brubaker L. Urinary incontinence in women: a review. JAMA. doi:10.1001/jama.2017.12137. Adjusted mortality curves were estimated by Cox proportional hazards modeling at the median value of each covariate. The median values for the matching factors were set as follows: (1) 0, explosives for mechanism of injury; (2) 1, yes for documented prehospital shock; (3) 2, two or more traumatic limb amputations below the knee or elbow, or 1 above the knee or elbow; (4) 1, Abbreviated Injury Scale score for head injury of 2; and (5) 1, yes for hemorrhagic torso injuries. The median values for the additional covariates were set as follows: age of 26 years, injury year of 2012, US Army DUSTOFF transport team, yes for prehospital tourniquet, and 29 minutes from injury occurrence to medical evacuation rescue. There were no patients lost to follow-up and the median survival times were 1440 minutes (24 hours) for the 24-hour survival analysis and 30 days for the 30-day survival analysis. HR indicates hazard ratio. Adjusted mortality curves by Cox proportional hazards modeling at the median value of each covariate. The median time to start of transfusion was 36 minutes after injury (interquartile range, 27-46 minutes). The median values for the matching factors were set as follows: (1) 0, explosives for mechanism of injury; (2) 1, yes for documented prehospital shock; (3) 2, two or more traumatic limb amputations below the knee or elbow, or 1 above the knee or elbow; (4) 1, Abbreviated Injury Scale score for head injury of 2; and (5) 1, yes for hemorrhagic torso injuries. The median values for the additional covariates were set as follows: age of 26 years, injury year of 2012, US Army DUSTOFF transport team, yes for prehospital tourniquet, and 29 minutes from injury occurrence to medical evacuation rescue. There were no patients lost to follow-up and the median survival times were 1440 minutes (24 hours) for the 24-hour survival analysis and 30 days for the 30-day survival analysis. HR indicates hazard ratio. Quiz Ref IDEven though prehospital transfusion was well documented, the capability to transfuse was not known with certainty for transport teams serving nonrecipients. To balance the study groups based on injury severity, nonrecipients of prehospital transfusion were frequency matched on 5 documented factors likely to have been visible to the transport teams: (1) mechanism of injury (gunshot vs explosion), (2) positive indicator of prehospital shock defined as above, but measured prior to hospital admission (yes or no), (3) type and severity of traumatic limb amputation in 4 categories (none, 1 below knee or elbow, ≥2 below knee or elbow or 1 above knee or elbow, ≥2 above knee or elbow), (4) hemorrhagic torso injury assessed by Abbreviated Injury Scale (AIS) diagnostic code (yes or no),16 and (5) severity of head injury assessed by maximum head AIS score in 3 categories (0-1, 2, ≥3) (Figure 1). The AIS score designates severity of injury by body region (head, face, chest, abdomen, extremities, and external), ranging from 0 (no injury) to 6 (not survivable). Prehospital transfusion recipients were classified into 26 strata, each with a unique matching-factor profile. Nonrecipients of prehospital transfusion with a profile matching that of 1 or more recipients were selected into each stratum, allowing the ratio of nonrecipients to recipients to vary across strata. Unadjusted associations were assessed using the χ2 test, the Wilcoxon rank sum test, and the Fisher exact test. One transfusion recipient and 13 nonrecipients (from various other strata) had missing data for the time from injury occurrence to MEDEVAC rescue; however, there were no missing data for the 4 other covariates included in the modeling, the 5 matching factors, or the prespecified outcomes. Adjusted analyses were performed only for complete cases without missing data. Balance across matched study groups was evaluated with median and multiple logistic regression adjusted for all 10 covariates simultaneously (including matching factors). Initial Cox proportional hazards models were stratified by the matched groups and adjusted for the 5 other covariates. To generate survival curves, equivalent Cox models adjusted for all 10 covariates simultaneously17 and set each covariate at its median value. The delayed-entry approach18-21 adjusted all Cox models for left truncation (a source of potential immortal time bias22) among transfusion recipients who were selected for analysis because they survived long enough to receive prehospital transfusions. Follow-up of matched and covariate-balanced nonrecipients was thus aligned to begin at the same minute after the MEDEVAC rescue that the prehospital transfusion started in the corresponding transfusion recipient with the shortest delay. Any matched and covariate-balanced nonrecipient who died on or before the earliest transfusion start time among corresponding transfusion recipients was not counted.18-21. Between April 1, 2012, and August 7, 2015, there were 502 US casualties (median age, 25 years [interquartile range, 22-29 years]; 98% male) that met the selection criteria (Figure 1) with 55 recipients of prehospital transfusion and 447 nonrecipients. Transfusion recipients had more severe injuries than nonrecipients (Table 1). Among prehospital transfusion recipients, 38 received only RBCs (1 patient received 2 U), 7 received only 1 U of plasma, and 10 received 1 U of RBCs and 1 U of plasma. All 7 prehospital transfusion recipients who received only 1 U of prehospital plasma went on to receive 1 U or more of RBCs during hospitalization. Within 24 hours of MEDEVAC rescue, 3 of 55 prehospital transfusion recipients (5%) and 85 of 447 nonrecipients (19%) died (between-group difference, −14% [95% CI, −21% to −6%]; P = .01). By day 30, 6 prehospital transfusion recipients (11%) and 102 nonrecipients (23%) died (between-group difference, −12% [95% CI, −21% to −2%]; P = .04). The 5 matching factors classified the 55 recipients of prehospital transfusion into 26 unique strata, each containing from 1 to 7 patients (median, 2 patients). There were 345 matching nonrecipients identified, with the number of matched nonrecipients per stratum ranging from 2 to 62 (median, 6 patients) (Table 1). The ratio of matched nonrecipients to recipients per stratum ranged from 0.67 to 62.0 (median, 6 patients). Matching alone did not balance the study groups17 in aggregate because of the variable numbers of transfusion recipients and matched nonrecipients within each stratum. By hour 24, 3 prehospital transfusion recipients died (5%) compared with 69 matched nonrecipients (20%) (between-group difference, −15% [95% CI, −22% to −7%]; P = .007). By day 30, 6 prehospital transfusion recipients died (11%) compared with 78 matched nonrecipients (23%) (between-group difference, −12% [95% CI, −21% to −2%]; P = .05). Regression modeling adjusted for frequency matching and additional covariates balanced the study groups with respect to evacuation and transport times, initial laboratory pH values, total hospital days among 30-day survivors, maximum AIS score, and ISS (Table 2). More prehospital transfusion recipients than nonrecipients received tranexamic acid (48/55 [87%] vs an estimated 45% of nonrecipients; P < .001). Tranexamic acid is an antifibrinolytic medication and potential confounder of the prehospital transfusion effects on survival.8,25 However, this medication was initiated during hospitalization for 150 (88%) of the 170 actual patients who received it; only 4 prehospital transfusion recipients (7%) received the initial dose before or concurrently with their initial transfusion. For 47 prehospital transfusion recipients and 76 nonrecipients who survived long enough to receive a transfusion during hospitalization, the times to initiate transfusion (regardless of location) overlapped (Figure 2). In the adjusted Cox models, only transfusions initiated within 15 minutes of MEDEVAC rescue (median, 36 minutes after injury) were associated with reduced 24-hour mortality (HR, 0.17 [95% CI, 0.04-0.73], P = .02; Figure 4A; there were 2 deaths among 62 recipients of transfusion within 15 minutes of MEDEVAC rescue vs 68 deaths among 324 matched patients with delayed treatment). For delays of only 1 to 5 minutes longer (16-20 minutes after MEDEVAC rescue), there was no significant association with survival at 24 hours (adjusted HR, 0.94 [95% CI, 0.41-2.17], P = .89; there were 10 deaths among 33 recipients of transfusion within 16-20 minutes of rescue vs 46 deaths among 278 matched patients with delayed treatment; Figure 4B). Prehospital deaths occurred only among nonrecipients, and after removing 21 of the total 41 in the sensitivity analysis to allow for possible transfusion futility, a reduced 24-hour mortality rate persisted for transfusions started within 15 minutes of MEDEVAC rescue (adjusted HR, 0.23 [95% CI, 0.06-0.96], P = .04; there were 2 deaths among 62 recipients of transfusion within 15 minutes of MEDEVAC rescue vs 47 deaths among 303 with delayed treatment). This study has several limitations. Retrospective studies cannot overcome unmeasured confounding. Although receipt of prehospital transfusion was well documented in medical records, the prehospital transfusion capability of MEDEVAC teams transporting nonrecipients was not. To enhance between-group comparability, nonrecipients were matched to prehospital transfusion recipients. Because stratification on posttreatment surrogates for severity (eg, massive transfusion, ISS) has introduced bias into both randomized and observational studies of prehospital trauma care,36 the current study used only matching and additional covariate adjustment based on documented injury characteristics that prehospital transport teams would likely observe. A sensitivity analysis was applied as a mitigation strategy for potential transfusion futility among nonrecipients reported as unresponsive to resuscitation efforts after the loss of vital signs en route. Because prehospital transfusion may have been considered futile for at least some moribund casualties, the sensitivity analysis omitted all of them to reduce mortality to the lowest extreme among nonrecipients. Despite removing more than 50% of prehospital deaths, 24-hour mortality was still significantly decreased for recipients of transfusions started within about 36 minutes after injury. In addition, although all of the 55 prehospital transfusion recipients received at least 1 U of additional blood product after hospital arrival, 112 matched nonrecipients (32%) survived longer than 24 hours without receiving any transfusion, which could cast doubt on their eligibility for prehospital transfusion. If these casualties were excluded, a higher mortality rate among the nonrecipients would further strengthen the association of prehospital transfusion with survival, but would also further reduce statistical power and precision. Alongside consistently favorable and statistically significant associations of prehospital and early transfusion with survival, the study’s modest sample size of only 55 prehospital transfusion recipients generated broad 95% CIs with upper bounds close to null, especially for 30-day survival. Early deaths prior to MEDEVAC arrival were excluded from the study to focus on casualties who could possibly have received transfusions. During this study, capability for transfusion prior to MEDEVAC arrival was nominal and no recipients were identified. It is possible that even earlier transfusion (ie, prior to the estimated 36 minutes shown in this study) may be associated with a greater likelihood of survival. More research in this area is needed. However, the study by Wang and colleagues might still have overestimated the prevalence of prediabetes in China. In a nationally representative sample of 46 239 adults in 2008, the prevalence of prediabetes was 15.5% (16.1% among men and 14.9% among women).3 In contrast, the prevalence of diabetes in the 2008 study was 9.8%, which is close to the 10.2% in the current study. Wang and colleagues used HbA1c levels as the criterion for prediabetes. Although the increased prediabetes prevalence could be due to improved sensitivity of screening with HbA1c measurement, the increase from 15.5% in 2008 to 35.7% in 2013 is suspect. Wang and colleagues used HbA1c levels of 5.7% to 6.4% to diagnose prediabetes, as recommended in the United States. However, this criterion, developed in the US population, has not been validated in a Chinese population. In addition, a study on the relationship between HbA1c level and the oral glucose tolerance test among Chinese adults found that the accuracy of HbA1c measurement for detecting diabetes defined with an oral glucose tolerance test declines with age.4 There were 49 542 Chinese participants aged more than 60 years among the total population of 170 287 in the study by Wang and colleagues (29%), suggesting oversampling of the elderly. Also, the sex ratio in the study was unbalanced (97 551 men; 57%). These imbalances could partly explain the high prevalence of prediabetes, although Wang and colleagues weighted the results. In Reply The direct comparison between the 2008 study by Yang and colleagues1 and our study might not be appropriate due to different sampling strategies, methods of data collection, diagnostic procedures, and statistical analyses. In the 2008 study, a sample of 46 239 adults aged 20 years or older from only 14 provinces and municipalities was included. HbA1c level was not used to identify undiagnosed diabetes or prediabetes. Their calculations of standardized prevalence were weighted based on the Chinese population structure in 2006. Our study included 170 287 participants aged 18 years or older from 298 surveillance points covering all 31 provinces, autonomous regions, and municipalities in mainland China. The prevalence was standardized according to the Chinese population census in 2010. Direct comparison between the 2010 study by Xu and colleagues2 and our 2013 study is more appropriate, as both were conducted within the same framework of the serial cross-sectional China Chronic Disease and Risk Factors Surveillance studies. The estimated prevalence of prediabetes in 2010 was 31.0% using only fasting glucose of 100-125 mg/dL (to convert glucose to mmol/L, multiply by 0.0555) and 2-hour glucose of 140-199 mg/dL (ie, the same criteria as the 2008 study1) and was 50.1% adding HbA1c level as a criterion for prediabetes. Both the 2010 study and our study found a prevalence of prediabetes of more than 30% regardless of whether HbA1c level was used as the criterion. Using HbA1c level increased the prevalence by almost 20% in the 2010 study, vs less than 5% in our study. The difference may be due to different methods of measuring HbA1c levels. In addition, although more female and elderly participants were included in our survey, weighted prevalences were used according to the distribution of the Chinese population census in 2010. The large sample size provided sufficient numbers of participants in each age and sex group. There are ethnic differences in the distribution of hyperglycemic categories4 that apply to both fasting glucose and HbA1c measurements. Using only fasting plasma glucose concentrations, 69% of patients with diabetes were detected in a European population, vs only 56% of Asian patients.4 Therefore, use of HbA1c level is more helpful to identify high-risk Asian patients. In the latest clinical guideline on type 2 diabetes in China,5 because of an insufficient degree of standardization of instruments and quality control of measurements, HbA1c measurement was not generally recommended. However, the guideline suggested using HbA1c level as a reference in hospitals that use a standardized HbA1c assay. In our study, a single laboratory with strict quality control was used for HbA1c measurements. The American Diabetes Association definition of prediabetes, an HbA1c level between 5.7% and 6.4%, was associated with a high risk of progression to diabetes in diverse populations, including Asian populations.6 This criterion is becoming widely accepted and used. Fifth, the consumer-driven health care market is growing rapidly and is expected to exceed $350 million by 2020. National restrictions on DTC services may drive people to seek it from other countries, just as they seek less expensive prescription drugs. Patients need easier access to high-quality, affordable health care and reliable options for obtaining DTC testing. If health care professionals are concerned about the types and quality of DTC tests, then DTC testing should be offered at appropriately accredited hospitals and laboratories with engaged directors who practice laboratory test stewardship. As Dr Gronowski and colleagues point out, tests such as over-the-counter (OTC) HIV testing and pregnancy testing have played a positive role in screening for these conditions, supporting patient privacy during the screening process, and permitting patients to screen as often as they think necessary without having to pay and wait for an office visit with their clinician. Likewise, home glucose monitoring has played an important role in routine monitoring and management of diabetes mellitus. Unlike many DTC medical tests, however, the indications and utility of OTC HIV, OTC pregnancy, and home glucose testing are easily understood by a large segment of the population. For example, it is easy to understand that an OTC pregnancy test is of greater utility to a fertile woman who is sexually active than a woman well past the age of fertility or to a man. The relative ease of understanding the test results is, in part, because of the high specificity and the binary implications of the results (ie, a positive test means the target condition is detected; a negative test means the target condition is not detected). To the Editor In the JAMA Clinical Challenge of a patient with a large adrenal tumor,1 we question the need for a positron emission tomography (PET)/computed tomography (CT) scan using 18F-fluorodeoxyglucose (18F-FDG) in establishing the need to proceed with surgery. Based on size (5.5 cm), presence of irregular margins, and a central hypodense area, the suspicion for malignancy was already high. Furthermore, absolute and relative washout were below the thresholds for adenomas of 40% and 60%, respectively, suggesting malignancy.2 With such a high pretest probability for malignancy, it is unsurprising that the PET/CT scan with 18F-FDG showed avid uptake, and we question the added value associated with this extra cost. Advocating for the use of PET/CT imaging with 18F-FDG in the evaluation of adrenal masses with clear indications for surgery will likely only contribute to unnecessary health care spending. Few studies have evaluated the value of contrast-enhanced adrenal CT in nononcologic patients, and most data lack robustness.1 Rather, its role has been to distinguish adrenocortical adenomas from adrenal metastases in patients with cancer. In a prospective study including adrenal masses in nononcologic patients, in the subgroup of masses with unenhanced CT density of 10 Hounsfield units or greater, imaging had a sensitivity of 90% and specificity of 50% for relative washout (optimal threshold value, ≤23%) and a sensitivity of 91% and specificity of 40% for absolute washout (optimal threshold value, ≤47%).2 For such masses with abnormal relative or absolute washout values and diameters of 4 cm or greater, the risk of malignancy was estimated to be 32%. We agree with O’Neil and colleagues that the presence of irregular margins and a central hypodense area increase the risk of malignancy. However, some other nonmalignant diagnoses are also possible, such as atypical adenomas, ganglioneuromas, and schwannomas. The use of PET/CT imaging with 18F-FDG enables a shift from a diagnostic adrenalectomy to tailored surgery based on an accurate preoperative diagnosis. The presence of metastatic disease usually contraindicates surgical removal of an adrenocortical carcinoma. Therefore, the exclusion of distant metastases is essential for optimal management. Although PET/CT imaging with 18F-FDG can be less sensitive than conventional imaging for small lung or liver metastases, it has an overall greater specificity and therefore can be an important complementary imaging modality. In certain cases, PET/CT imaging with 18F-FDG also can reveal occult metastases. The RBS, established in 1972, is a prospective study of 6629 adults older than 50 years residing in Southern California.5 As of 2016, approximately 1000 participants were active (the primary reason for loss to follow-up was mortality). Of those 1000 participants, 112 had routine morning spot urinary biospecimens obtained at each of 5 clinic visits that took place from 1993 to 1996 and from 2014 to 2016. One hundred of these 112 were randomly selected for this study, which was approved by the University of California, San Diego, institutional review board. All participants gave written informed consent. Samples were analyzed using high-performance liquid chromatography coupled with mass spectrometry. Limits of detection (LOD) were 0.03 μg/L for glyphosate and 0.04 μg/L for AMPA; assays were linear up to 50 μg/L. Analyses were normalized to each sample’s specific gravity, thereby accounting for dilution or concentration effects due to variability in water intake and age-related or other differences in renal function. Changes over time in the proportion of samples above the LOD were assessed using generalized estimating equation models to account for the dependency of observations in repeated measures. A 2-sided significance threshold was set at less than .05. Statistical analyses were performed using R (R Foundation), version 3.3.2. Most of the major infectious diseases affecting humans, such as smallpox, polio, and yellow fever, have required effective vaccines for their control and in some cases elimination, and so the question arises whether the HIV/AIDS pandemic can be effectively addressed without an HIV vaccine. The answer to that question is not straightforward, but needs to be addressed from both a theoretical and a practical standpoint. Theoretically, the HIV pandemic can be ended without an HIV vaccine. More than 30 highly effective anti-HIV drugs are currently available. When given in combinations of 3 or more, these medications can durably suppress the virus such that patients who are treated soon after infection and continue therapy throughout their lifetime can expect to have an almost-normal life expectancy. Importantly, effective treatment can reduce the level of virus in a person with HIV so low that it is extremely unlikely that this person will transmit the virus to his or her uninfected sexual partner. This concept is referred to as “treatment as prevention.” Therefore, theoretically, if most or all of the people living with HIV in the world could be identified, accessed, and treated, it would be possible to stop all infections and end the epidemic. People who are uninfected, but whose behavior or life situation puts them at high risk of HIV infection, can take a single pill containing 2 anti-HIV drugs and decrease the likelihood of acquiring HIV infection. This approach—“preexposure prophylaxis” or PrEP—can lower the risk of acquiring HIV through sexual activity by more than 90%, or from injection drug use by more than 70% if the medications are taken consistently.2 Accordingly, if both of these treatment and prevention modalities were effectively implemented throughout the world, the HIV/AIDS pandemic would end. However, from a practical standpoint, ending the HIV/AIDS pandemic without a vaccine is possible, although it is unlikely. Although an estimated 19.5 million of the estimated 36.7 million HIV-infected people globally are receiving anti-HIV therapy (an extraordinary accomplishment), more than 17 million people are not receiving therapy.1 This leaves a substantial treatment gap. These 17 million people can continue to infect others, allowing the pandemic to be sustained. In addition, although PrEP is highly effective in preventing acquisition of HIV among people at high risk of infection, only a very small percentage of these individuals are actually taking these medications. In the United States, it is estimated that only approximately 10% of people who could benefit from PrEP are actually receiving it,3 and this proportion is much smaller elsewhere in the world.1. The Joint United Nations Programme on HIV/AIDS (UNAIDS)  has set an ambitious target to help end the HIV pandemic.1 Called “90-90-90,” the goal for 2020 is to have 90% of HIV-infected people throughout the world know their HIV status, 90% of people diagnosed with HIV receiving anti-HIV treatment, and 90% of people who receive treatment having their virus suppressed to undetectable levels. If successful, the result would be that an estimated 73% of all people in the world with HIV would have undetectable viral levels. Since suppressed viral levels result in a marked reduction in the risk of HIV transmission to other individuals, mathematical models suggest that achieving the 90-90-90 goal would reverse the kinetics and trajectory of global HIV disease such that it would no longer be of pandemic proportions. A recent study in rural Kenya and Uganda demonstrated that implementation of community-based testing and treatment resulted in increased HIV diagnosis, initiation of antiretroviral therapy and viral suppression, and the study communities reached the UNAIDS target within 2 years.4 In addition, some entire countries have been successful in reaching the goal of 73%, largely through the efforts of the President’s Emergency Plan for AIDS Relief and the Global Fund to Fight AIDS, Tuberculosis, and Malaria. However, the global figure for achieving this goal in all countries is just 44%.1 Also, modeling studies have suggested that in certain high-prevalence regions of the world, the geographic dispersion of the infected population, would make it extremely difficult to reach them effectively with HIV treatment.5. The question also arises whether it is economically feasible to end the HIV pandemic in the absence of a vaccine. In this regard, the resource requirements to achieve such a goal are continually increasing. The 19.5 million people currently receiving anti-HIV drugs must be maintained on these medications for the rest of their lives; at the same time, anti-HIV drugs need to be provided to the 17.2 million infected, but untreated people. Furthermore, the estimated 1.8 million people who are newly infected with HIV each year1 also need to be treated. In addition, the cost of providing PrEP and other prevention services to the millions of people who are at risk for HIV infection is substantial. In 2016, UNAIDS estimated that the total investments needed for an adequate treatment and prevention response for HIV in low- and middle-income countries between 2016 and 2030 would amount to approximately $350 billion.6 Against this backdrop, a recent Kaiser Family Foundation and UNAIDS study found that donor-government funding for HIV decreased by 7% in 2016, which represents the lowest funding level since 2010.7. Despite the remarkable gains in the treatment and prevention of HIV infection, development of an effective HIV vaccine will likely be necessary to achieve a durable end to the HIV pandemic. An important question is how effective that vaccine must be. One vaccine tested in a large vaccine trial8 in Thailand reduced the risk of infection by 31%, a figure inadequate to justify licensure of the vaccine. In contrast, other vaccines used in controlling or ending global outbreaks have been much more effective. For example, the measles, polio, and yellow fever vaccines are nearly 100% effective. Given the difficulty for the human immune system to mount a protective response against HIV, it is extremely unlikely that an HIV vaccine will be as effective as those other proven vaccines. Despite extraordinary advances in the treatment and prevention of HIV infection, it is theoretically possible to end the HIV epidemic by aggressively and effectively implementing these interventions, although from a practical standpoint this goal would be difficult to achieve. Therefore, development of a moderately effective vaccine together with optimal implementation of existing treatment and prevention modalities could end the current HIV pandemic. Recent advances in HIV vaccine research provide hope that at least a moderately effective vaccine can be developed. It is critical to continue to accelerate a robust research effort in that direction while aggressively scaling-up the implementation of current treatment and prevention tools. To do anything less would lead to failure, which for HIV is not an option. 1. Improve surveillance of possible opioid addiction. No current information systems enable real-time assessment of the numbers, patterns, or trends of new opioid addiction. This makes it impossible to determine the trajectory of the epidemic, identify areas in which opioid addiction is worsening and intervene, and learn lessons from areas in which incidence is decreasing. By using Prescription Drug Monitoring Program (PDMP) and other data systems to identify trends and patterns of possible new addiction (eg, newly filled prescriptions for ≥30 days), aggregate data could be used in real time to identify factors associated with addiction. New cases of opioid use disorder may be declining, while deaths among heroin users may be increasing due to spread of illicitly synthesized fentanyl; however, it is not possible to know until the incidence of opioid addiction is tracked. 2. Improve reporting of and response to opioid-related overdoses and fatalities. Real-time data on overdoses from syndromic and other surveillance systems can enable rapid response to changing patterns of opioid use. The timeliness and specificity of testing for drugs involved in opioid-related fatality reports differ from state to state. More reliable information on the specific drugs involved in deaths would better inform public health and law enforcement interventions. Substantial improvements in the quality and timeliness of medical examiner and coroner work, including access to data from PDMPs, could greatly improve data timeliness and completeness. Improved training and increased funding of coroners and medical examiners could facilitate more accurate and timely identification of drugs involved in overdose deaths. Fatal and nonfatal overdoses involving prescribed medications should trigger an automatic report to the patient’s health care professionals to facilitate appropriate medical response and to state medical boards, and people who have survived an overdose could be linked to treatment. This feedback could foster more cautious prescribing and improve regulation of prescribing practices. 3. Promote more cautious prescribing for acute pain. Opioids are essential medicines to treat severe pain after surgery or serious injury, but they are too frequently prescribed for pain that could be treated with nonsteroidal anti-inflammatory medications (eg, molar extractions in adolescents). When opioid use is unavoidable, dosage should be as low and duration as brief as possible3; physiological dependence on and tolerance to opioids can develop in as little as 1 week. Patients taking short courses of opioid medication may experience withdrawal symptoms, including worsening of pain upon discontinuation; this may lead to continued use. According to a recent study, 1 in 5 patients who had been prescribed opioids for 10 days became long-term users.4 Another study found that the quantity of pills prescribed for postsurgical acute pain could be reduced 53% and that less than 1% of patients required refills.5 The Centers for Disease Control and Prevention (CDC) recommends that when opioids are prescribed for acute pain, “[t]hree days or less will often be sufficient; more than 7 days will rarely be needed.”3 The US Food and Drug Administration (FDA) should revise opioid labels to be consistent with the CDC recommendation. Adding duration of use to opioid labels would send a clear message to prescribers and patients that risks increase when opioid use continues past 3 days. 4. Change labeling for chronic pain and greatly restrict or eliminate marketing of opioids for this indication. The risks of opioids are likely greater than the benefits for common chronic conditions (eg, low back pain, fibromyalgia). However, patients with chronic non-cancer-related pain have been the target market for opioid manufacturers and account for much of the increase in opioid consumption in the United States during the past 20 years. The FDA should narrow on-label indications and halt marketing of opioids for low back pain and other conditions for which risks of use outweigh potential benefit. This could help to discourage clinicians from initiating long-term opioids, but would not prohibit clinicians from continuing off-label prescribing of opioids to stable patients with chronic pain. Compassionate care for patients with chronic pain is not jeopardized by more cautious prescribing. 5. Increase insurance coverage of and access to nonopioid and nonpharmacological management of pain. Chronic pain is a serious and potentially disabling problem for millions of people in the United States. Opioids are likely less effective and certainly more dangerous than other modalities of chronic pain management. The Centers for Medicare & Medicaid Services should ensure full reimbursement for nonprescription analgesics, such as acetaminophen and nonsteroidal anti-inflammatory drugs, for Medicare Part D and Medicaid beneficiaries. This would remove a financial disincentive for patients to use these medications. Easier access to and low or no copayments for physical therapy and other nonpharmacological pain management modalities could potentially reduce medication use and improve patient functionality and outcomes. 6. Interrupt the supply of heroin and illicitly produced synthetic opioids and improve coordination between legal and public health authorities. Interdiction is critically important to increase the cost and reduce accessibility of opioids. As with tobacco and alcohol, if heroin and illicitly produced synthetic opioids such as fentanyl are more expensive and more difficult to obtain, use should decrease.6,7 The legal system can also implement programs such as treatment as an alternative to incarceration, and correctional facilities can provide opioid agonist treatment for addicted inmates during detention and linkage to treatment services on release. These interventions are more likely than criminal sentences for low-level drug users to reduce both illicit opioid use and related crime. 7. Identify possible opioid addiction early and link individuals to treatment. Early identification and treatment of opioid-addicted individuals reduces the risk of overdose, psychosocial deterioration, transition to injection opioid use, and medical complications. Medically assisted treatment (eg, methadone, buprenorphine) should be routinely offered in primary care, emergency departments, and hospital inpatient services to increase treatment uptake, as well as in the criminal justice system, with careful attention to continuity on discharge. States receiving federal funding for their PDMPs should be incentivized or required to mandate prescriber checking of and timely data provision to PDMPs for all opioid prescriptions of more than 3 days’ duration, and state health officials should identify opioid-addicted patients as early as possible and facilitate referral to treatment. The federal privacy law known as 42 CFR Part 2 (Confidentiality of Substance Use Disorder Patient Records) should be amended so that opioid addiction can be treated like other medical conditions, improving patient safety and continuity of care. 8. Expand low-threshold access to opioid agonist treatment, particularly with methadone and buprenorphine. A substantial proportion of patients who would benefit from buprenorphine or methadone treatment will receive this treatment only if it becomes more attractive and accessible than either prescription or illicit opioids. In France, opioid overdose deaths decreased 79% six years after widespread prescribing of buprenorphine.8 Barriers to accessing buprenorphine in the United States include federal limits on the number of patients a clinician can treat, a required 8-hour training course, and inadequate integration of buprenorphine into primary care treatment. Access to buprenorphine could be expanded if the federal government removed regulatory barriers and required all federally qualified health centers to offer buprenorphine treatment. It seems illogical and perhaps ironic that buprenorphine, the one opioid that appears to be safer than commonly prescribed opioid analgesics and heroin, is the only one with such barriers to prescription. 9. Implement harm reduction measures for current users, including access to clean syringes and naloxone. Access to clean syringes can prevent injection-related infectious diseases, and access to naloxone can reduce fatal overdoses. The federal government should continue to assist state and county efforts to make naloxone and clean syringes more widely available, and the FDA should accelerate its efforts to help drug manufacturers pursue approval of an over-the-counter naloxone product. 10. Consider removing ultra-high-dosage-unit opioid analgesics from the market. Formulations of opioids that exceed 90 morphine milligram equivalents per day when taken as directed are dangerous and should be removed from the market. For example, a patient directed to take 1 oxycodone 80-mg tablet twice a day is consuming the equivalent of 240 mg of morphine, far exceeding a dosage associated with a greatly increased risk of death. Because only 1 pill is taken at a time, the patient and prescriber may not appreciate that this is an extremely high dose. An individual who takes a single pill that is unused or diverted from a prescription supply could experience a fatal overdose. Opioids are available in liquid preparations, patches, sublingual forms, and suppositories for patients who have difficulty swallowing extra pills. Existing public health strategies are meaningful and necessary, but an emergency declaration could be a turning point for a national surge response. Current response efforts include patient and prescriber surveillance, reduced medical prescribing, and counseling or treatment for persons at risk or already addicted. The US Food and Drug Administration (FDA) and National Institutes of Health urge research and development of less addictive painkillers, while CDC calls for greater physician and pharmacist education to avoid inappropriate prescribing. US Customs and Border Protection agents are working to restrict the flow of illicit opioids, while law enforcement is targeting illicit drug dealers and unethical physicians. Emergency medical technicians and family members are administering life-saving naloxone with fewer liability concerns due to state Good Samaritan laws. Jails and prisons are another focal point for public health interventions. Significant numbers of prisoners either used opioids in committing their crimes or were incarcerated with an opioid-related use disorder. Effective interventions would provide expanded treatment to addicted prisoners by overriding Medicaid provisions that terminate benefits for incarcerated populations. Greater use of drug courts that divert opioid users from punitive to therapeutic settings is also essential. As of October 1, 2016, more than 25 000 medical device reports indicating large inaccuracies with blood glucose levels measured with the CGM device had been filed with the FDA since the start of 2015,5 shortly after FDA approval of the most recent software and sensor combination currently used by the Dexcom G5. By June 30, 2017, the number of reports of inaccuracy had increased to more than 40 000. Even though the average error of readings had steadily improved through successive generations of the device, these reports suggest that sporadic large errors still occur. Many of the reports lack detail, and approximately one-third of the concerns about inaccurate readings were attributed by the manufacturer to user error. Nonetheless, there have been detailed reports of unexplained inaccurate readings associated with loss of consciousness, seizures, car crashes, hospitalizations, intensive care unit stays, and deaths. Individually, many of the concerns raised in the reports might be regarded as unique, incomplete, or inconclusive; however, taken together, these reports represent a safety signal requiring investigation (100 examples of reports of Dexcom CGM inaccuracy appear in the Supplement). As part of the application submitted to the FDA for relabeling of the device, no new clinical evidence was introduced to address the reports of inaccuracy from numerous users. The only clinical data considered regarding the accuracy of the device were the same data that had been used originally to gain approval for using the device for diabetes treatment decisions only with verification. The data on accuracy were based on only 44 adult patients with type 1 diabetes, 7 with type 2 diabetes, and 79 pediatric patients with type 1 diabetes.6,7 Estimates of device accuracy using such small numbers of patients observed in an in-clinic setting can be overly optimistic. For example, data from the pivotal study6 for adult users, showed that when the CGM reading was greater than 80 mg/dL, there were no occurrrences in which the true blood glucose was below 60 mg/dL.3 In contrast, the Supplement includes 20 reports in which CGM values of 115 mg/dL or greater masked hypoglycemic blood glucose readings of 45 mg/dL or less that were corroborated by medical personnel, the altered consciousness of the patient, or both. An FDA advisory panel was convened in July 2016 to provide recommendations. The panel was not provided information about the large number of reports of device inaccuracy. Instead, panelists (1) heard testimonials for the device from some current users and physicians during a public hearing; (2) were presented with 2 sponsor-originated computer simulations ultimately determined unconvincing by FDA reviewers; and (3) were presented with the original clinical data. The panel also received a petition signed by users who had been told that labeling the device for nonadjunctive use was necessary to permit the CMS to provide reimbursement coverage, and would hasten the development of an artificial pancreas device. The panel voted 8-2 to approve the label changes.8. Patients with diabetes are heterogeneous. They differ in their susceptibility to hypoglycemia; the frequency, speed, and extent of glycemic excursions; their counter-regulatory responsiveness; and their degree of glycemic awareness. The extent to which a CGM susceptible to intermittent accuracy can safely replace, instead of complement, blood glucose meters for insulin dosing is likely to differ for each patient. In particular, the number of confirmatory blood glucose readings needed to use a CGM safely will vary between individuals. The CMS and other payers will need to recognize the need for more than just 2 calibration test strips per day for many of the patients using CGMs for making diabetes decisions. One Sunday in early 2017, my anesthesia coresident Will (not his real name) was brought to the emergency department of our hospital unresponsive. He was quickly triaged, intubated, and transferred to the intensive care unit (ICU), where a hypothermia protocol was initiated. I couldn't visit Will for two days. Those days came like rust, slow and hard. I went to see him with another resident. We met Will’s family for the first time under the worst of circumstances. I went expecting to offer comfort, prayers, and time. Instead, Will’s family gave me comfort, prayers, and all the time I needed. The ICU is a place anesthesia residents visit daily, no more worrisome than the hospital cafeteria. However, standing in Will’s room, I felt out of place and vulnerable. I wasn’t a physician here, and Will wasn’t my patient. This was my friend and I was a visitor. Years of anesthesia training will never prepare you for the moment when you see a loved one intubated. In that moment, it all seemed new to me: the intravenous (IV) infusions, the ventilator, the electroencephalogram (EEG) leads stuck to Will’s head like a skullcap. An endotracheal tube had never been so invasive. I spoke to Will and let him know I was there. Then I left, dumbfounded. There was a certain pressure in the room I couldn’t shake. I realized later it was fear. Fear for Will. Fear for his family. As the days passed, friends and family from across the country came like fireflies to light up Will’s bedside. For his coresidents, our days were punctuated by visits to Will. No matter how physically spread out we were, on different rotations at different hospitals, we all made the time. The nights were often the best time to visit. When the operating rooms had been lulled to sleep, I visited the ICU to watch Will’s chest rise and fall. And when we were alone, after Will’s family had fought back sleep for as long as they could, I would read him short stories, hoping somehow, through the fog of sedation, Will was hearing a familiar voice. The days morphed into weeks; the sprint became a marathon. The first time I saw the chairman of our department in scrubs was the day he joined our associate program director at Will’s bedside. Will needed an operation, and the two of them would perform the anesthesia. I drew strength from watching them transport Will to the operating room. They were taking care of their own. When they left, another resident and I stayed with Will’s family. Despite having taken countless patients to the operating room, this was the first time I saw what was left behind. The room looked empty without the hospital bed. A room full of worry, a room not ready for the separation. The weeks continued. The infusions, ventilator, and EEG monitor slowly disappeared. The sedation was turned off, and everyone waited. Will opened his eyes and moved his limbs, but the fog persisted. He was transferred to the general medicine floor and soon afterward left the hospital for a long-term care facility. A month later, surrounded by his family, Will died. Will overdosed on fentanyl; one of many physicians with substance use disorder whose presenting symptom is a fatal or near-fatal overdose.1 Smart, funny, charming, Will had a tireless work ethic; no one would have suspected that Will had a drug problem. His addiction slipped by everyone, as is too often the case. If this could happen to Will, it could happen to any physician. Substance use disorder is not limited to anesthesia. As a community, we must examine the factors driving increasing rates of substance use disorder among all practicing physicians.2. Physicians are not immune to stress and fatigue, although we sometimes pretend this is the case. While many are proficient at masking stress and fatigue, we often do a poor job of managing them. This manifests in serious ways: burnout, depression, and substance use disorder. Estimates suggest that more than half of practicing physicians in the United States are affected by professional burnout, and our profession has one of the highest suicide rates.3,4 This is unacceptable. The medical community must acknowledge that these tragedies are a direct product of the environment and culture we have created. Just as firefighters are most vulnerable to the wildfires they fight, physicians are most vulnerable to the opioid epidemic we fight. Trainees may be protected by ACGME guidelines and requirements, but what about practicing physicians? Who protects them? It is time to consider whether it is effective for the medical community to continue to self-regulate in this regard. The stakes are too high to continue to get this wrong. And unfortunately, the medical community is too fatally entangled to see it. Seeking the guidance of an external agency to detect, manage, and treat physician burnout and substance use disorder could be a major step toward promoting physician wellness and ensuring patient safety. Without guidelines and enforcement from an independent group to manage stress, fatigue, and general wellness (sleep, exercise, diet) what will remain is a patchwork of unconnected suggestions. Physician wellness must become a priority. We are much better at fighting for our patients than for each other. This must change, and we cannot do it alone. We celebrated Will’s twenty-six years the same way he lived his life, filled with love. As a profession, we should acknowledge every physician’s death as our program did Will’s, a death in the family. The ease of accepting it as another statistic must be replaced by the urgency to act. As a profession, now is the time to call a family meeting. We must look out for each other, because that’s what families do. In this randomized double-blind clinical trial, adult patients were enrolled during an ED visit for acute extremity pain. Patients received a single dose of an oral combination analgesic (ibuprofen and acetaminophen, oxycodone and acetaminophen, hydrocodone and acetaminophen, or codeine and acetaminophen) and were asked to rate their pain intensity using a verbal numerical rating scale (NRS) from 0 to 10 at 1 and 2 hours following ingestion. The Albert Einstein College of Medicine institutional review board provided ethical oversight and study approval. All participants provided written informed consent in either English or Spanish. Data were collected between July 2015 and August 2016. The protocol and statistical analysis plan as well as additional post hoc analyses appear in Supplement 1. This study was conducted in 2 EDs of the Montefiore Medical Center. The Moses division is an urban teaching hospital with more than 100 000 adult visits annually, and is located in the west Bronx, New York. The Weiler division is a community hospital with more than 70 000 adult visits annually, and is located in the east Bronx, New York. Salaried, trained, full-time, bilingual (English and Spanish) research associates staffed the ED 24 hours per day, 7 days per week during the accrual period. After randomization, all patients rated their pain immediately before taking the study analgesic and again both 1 and 2 hours after taking the medication while remaining in the ED. Patients discharged prior to 2 hours were reached at the 2-hour time point via a previously confirmed cell phone number. Patients who required rescue analgesics (based on the discretion of the ED attending physician) received an unblinded 5-mg dose of oral oxycodone, which could be administered at any point during the 2-hour study period. Additional analgesia could also be administered based on the discretion of the ED attending physician. A research pharmacist performed the stratified randomization in blocks of 8 using an online randomization plan generator. The pharmacist masked the analgesics by placing them into identical unmarked opaque capsules, which were packed with small amounts of lactose to equalize weight and then sealed. The pharmacist created research packets, each with 3 tablets containing the masked investigational medication. Research packets were removed by nurses from the Pyxis automated medical dispensing system located in the ED, and administered to the study patients. The randomized allocation schedule could only be accessed by the research pharmacist, who had no role in dispensing the medication. Pain intensity was assessed using an 11-point NRS in which a score of 0 indicates no pain and a score of 10 indicates the worst possible pain. The NRS is commonly used in EDs for assessing initial pain at triage and changes in pain levels during evaluation and treatment. The primary outcome was the between-group difference in mean change in NRS pain score among patients receiving 1 of the 4 combination analgesics, measured from the time before ingestion of the study medication to 2 hours later. Secondary outcomes included between-group differences in mean NRS scores at 1 hour and responses to a 4-point Likert scale rating pain as none, mild, moderate, or severe. The data collection instrument went through several iterations and the Likert scale was not included in the final data collection instrument. The minimum clinically important difference was defined as a mean NRS pain score of 1.3 based on the standard previously derived and independently validated definition.15-17. An intention-to-treat analysis was performed. All patients who were enrolled and met inclusion criteria were analyzed in the groups to which they were randomized. Those with missing NRS data (1 per group) had their missing values calculated by imputation. Details of the imputation analysis appear in Supplement 2. The primary analysis was a 1-way analysis of variance testing the null hypothesis that there is no difference in the effect of the medications on mean change in pain from baseline to 2 hours with a significance level of .05. The Bonferroni method was used to adjust the overall significance level of .05 to account for multiple comparisons when all pairwise mean differences in pain were compared, resulting in 99.2% CIs. Four patients were missing 1, 2, or 3 NRS scores (Figure). Multiple imputation using chained equations was used to keep these patients in the intention-to-treat analysis. In a post hoc analysis, NRS scores at 2 hours were imputed for patients who received rescue medication. This was done to address bias that could be introduced if receipt of rescue medication differed by treatment group. If the distributions differ, the 2-hour NRS scores would reflect the combined effect of the initial and rescue analgesics, thus attenuating differences between treatment groups. The imputation model included baseline and 1-hour NRS scores, receipt of rescue analgesia, language of interview, age, sex, site, ethnicity, treatment group, diagnosis, and nonpharmacological interventions. For all analyses, we used SPSS Statistics version 22 (IBM) and Stata version 14.0 (StataCorp). During a 13-month period beginning in July 2015, 416 patients were randomized (Figure). Of these, 5 patients received nonopioid analgesia within the past 8 hours (an exclusion criterion) and were inadvertently randomized and later excluded from the analysis. Of the remaining 411, 48% were female, 60% were Latino, and 31% were black. Baseline characteristics were similar in all groups (Table 1). Baseline pain intensity was initially high (mean NRS pain score, 8.7 [SD, 1.3]) and did not differ between groups (Table 2). Pain intensity declined over time in all treatment groups (Table 2). At 2 hours, the mean NRS pain score decreased by 4.3 (95% CI, 3.6 to 4.9) in the ibuprofen and acetaminophen group; by 4.4 (95% CI, 3.7 to 5.0) in the oxycodone and acetaminophen group; by 3.5 (95% CI, 2.9 to 4.2) in the hydrocodone and acetaminophen group; and by 3.9 (95% CI, 3.2 to 4.5) in the codeine and acetaminophen group. The overall test of the null hypothesis that there is no difference in change in pain by treatment group from baseline to 2 hours (the primary outcome measure) was not statistically significant (P = .053). There was also no significant difference at 1 hour (P = .13) (Table 2). Seventy-three patients (17.8%) received rescue analgesics within the 2-hour period (Table 4). The distribution of receipt of rescue analgesia was not statistically significant, but the estimates varied by as much as 9% (oxycodone and acetaminophen vs codeine and acetaminophen). Results of the analysis with multiple imputations of the NRS pain scores for patients who received rescue analgesics were nearly identical to the analysis without imputation (eTable 1 and eTable 2 in Supplement 2). There were no clinically important or statistically significant differences in efficacy when these post hoc analyses were performed. The combination of nonopioids makes clinical sense because of the potential to increase analgesic efficacy through different modes of action.23 However, there are relatively few studies of the relative efficacy of combination nonopioid oral analgesics vs oral opioids9-12; all were postoperative or dental studies that compared a combination of ibuprofen and acetaminophen vs codeine and acetaminophen. None found any dose of codeine (30 mg or 60 mg) to be superior to the combination of 400 mg of ibuprofen and acetaminophen in doses ranging from 325 mg to 1000 mg. One ED-based study compared a combination nonopioid vs oxycodone. All patients received ibuprofen and acetaminophen. There were 3 groups in the study (group 1 received 1000 mg of acetaminophen, 400 mg of ibuprofen, and 200 mg of thiamine; group 2 received 1000 mg of acetaminophen, 400 mg of ibuprofen, and 60 mg of codeine; and group 3 received 1000 mg of acetaminophen, 400 mg of ibuprofen, and 10 mg of oxycodone). At 30 minutes, there was no difference in analgesic efficacy among the groups.24 The evidence from these studies suggests that the lack of greater pain reduction with opioid analgesics over the combination of ibuprofen and acetaminophen found in the current study may generalize beyond the treatment of acute extremity pain. In contrast to earlier research, the current study offered a direct comparison of the most commonly prescribed oral opioids used in the ED and a nonopioid combination. In light of the substantial increase in prescription opioid-related overdoses and deaths, the widespread use of oral opioids has been questioned.26 Overuse and misuse of opioid analgesics in the community is an important contributor to the opioid epidemic; however, the current study only focused on treatment in the ED. If a nonopioid combination analgesic provides comparable pain relief to that obtained by oral opioids commonly used in the ED, it is possible that physicians may be more likely to discharge patients and prescribe the same nonopioid combination analgesics. In addition, patients might be more accepting of that analgesic so long as it provides ample pain relief when used in the ED. This change in prescribing habit could potentially help mitigate the ongoing opioid epidemic by reducing the number of people initially exposed to opioids and the subsequent risk of addiction, as shown in a recent study13 that found long-term opioid use was significantly higher among patients treated by high-intensity ED opioid prescribers than among patients treated by low-intensity ED opioid prescribers. This study has several limitations. First, the follow-up time was limited to 2 hours. Although a more prolonged follow-up would have provided additional information, the goal was to determine if a single dose of an analgesic would provide superior pain relief for patients while in the ED. In addition, the enrolled patient population included those generally seen in a fast-track setting, in which patients are discharged quickly with minimal or no testing. Whether the duration of analgesia differs among the analgesics is unknown, although the half-lives are similar and range from approximately 3 hours to 4 hours. Second, the oral opioids used in this study may be dispensed as 1 or 2 tablets at a time. We chose to administer the equivalent of 1 tablet. There are no nationally representative data sets that allow estimation of usual analgesic dose in the ED setting. For example, the National Hospital Ambulatory Care Survey2 has medication and route information but not dose information. The dosage used in this study represents common practice among physicians from a variety of training programs and institutions. The dose of opioid chosen also reflects prior studies,3-5 which have shown effective pain relief at the identical dosages used in the current study. In addition, there was concern that many physicians would not refer patients for enrollment if it meant administering higher doses of opioids because most patients were expected to be discharged quickly and the rescue analgesic was predefined to be 5 mg of oxycodone. Fourth, adverse effect information was not collected. Such information could influence the choice of analgesic prescribed, especially if one group had significantly more adverse effects than another group. Significant adverse effects were not expected to occur during the 2-hour follow-up of this study. However, a similar ED-based study lasting 90 minutes had an incidence of adverse events of 1.6% in the codeine group, 3.3% in the nonopioid group, and 16.9% in the oxycodone group, with lightheadedness accounting for 70% of adverse events in the oxycodone group. In that study, the amount of oxycodone and codeine administered was double the dose used in the current study and thus is not directly comparable.24. Spending amounts are reported and annualized percent changes in spending calculated using inflation-adjusted 2015 dollars. Values in parentheses indicate uncertainty intervals. Other neurologic disease actually had the fifth-largest increase in health care spending but was omitted from Figures 1 and 2 because it is a composite category made up of disparate neurologic diseases not included in the 6 other categories tracking spending on other neurologic diseases. Gray cell indicates values could not be calculated for that condition in that age group (no prevalence data). Spending amounts are reported and annualized percent changes in spending calculated using inflation-adjusted 2015 dollars. Values in parentheses indicate uncertainty intervals. Top, Spending changes that would have occurred in the presence of a constant population size and structure. Bottom, Spending changes that would have occurred if age-specific disease prevalence rates remained constant. These spending changes exist because of changes in service utilization, price, and intensity. Gray cells indicate values could not be calculated for a condition in that age group (no prevalence data). Each colored bar corresponds to 1 of 5 factors and reflects the amount of spending change associated with that factor. Bars to the left of the black vertical line (no change) indicate factors associated with decreased spending; to the right of the line, factors associated with increased spending within that type of care. The sum of the 5 bars equals the total spending change, 1996 through 2013, indicated with a black square marker. Error bars indicate uncertainty intervals. Other neurologic diseases actually had the fifth-largest increase in health care spending but was omitted from this figure because it is a composite category made up of disparate neurologic diseases not included in the 6 other categories tracking spending on other neurologic diseases. The dental care category was omitted from the figure because none of the spending on these 6 health conditions is associated with dental care. Each colored bar corresponds to 1 of 5 factors and reflects the amount of spending change associated with that factor. Bars to the left of the black vertical line (no change) indicate factors associated with decreased spending; to the right of the line, factors associated with increased spending within that type of care. The sum of the 5 bars equals the total spending change, 1996 through 2013, indicated with a black square marker. Error bars indicate uncertainty intervals; absence of error bars around a marker indicates zero change. Condition-specific graphs for all health conditions included in this study are included in the eResults in the Supplement. Each colored bar corresponds to 1 of 5 factors and reflects the amount of spending change associated with that factor. Bars to the left of the black vertical line (no change) indicate factors associated with decreased spending; to the right of the line, factors associated with increased spending within that type of care. The sum of the 5 bars equals the total spending change, 1996 through 2013, indicated with a black square marker. Error bars indicate uncertainty intervals. Each colored bar corresponds to 1 of 5 factors and reflects the amount of spending change associated with that factor. Bars to the left of the black vertical line (no change) indicate factors associated with decreased spending; to the right of the line, factors associated with increased spending within that type of care. The sum of the 5 bars equals the total spending change for each period, indicated with the black square marker. Error bars indicate uncertainty intervals. Estimates of personal health care spending and service volume were extracted from the US Disease Expenditure 2013 project.4 That study estimated US health care spending and service volume for each year from 1996 to 2013 and synthesized information on health care spending from 183 sources, leveraging the strength of each source to produce modeled estimates of spending and volume disaggregated by 155 health conditions, 38 age and sex groups, and 6 types of care: ambulatory, inpatient, emergency department, dental, retail pharmaceutical, and nursing facility care. Several adjustments, including adjustments for comorbidities and adjustments to account for the difference between charge and payment data, were applied to improve accuracy of the estimates. These adjusted data track spending associated with each disease rather than simply spending associated with the primary diagnosis appearing in the raw data. Spending estimates were also scaled to the official estimate of US health spending contained in the National Health Expenditure Accounts, which ensured that no spending was counted twice. All spending estimates from the project data set were adjusted for inflation and expressed in 2015 dollars. Spending on patients 85 years and older was combined with spending on patients aged 80 to 84 years to create 36 age and sex groups and to match the age and sex categories for our other data sources. More detailed information on the methods used in the Disease Expenditure project is reported in the eMethods in the Supplement and has been published.4,9-11. Population and epidemiologic data, including prevalence and incidence for each health condition, were extracted from the Global Burden of Disease (GBD) 2015 study.12 The GBD study estimated prevalence, incidence, deaths, and other epidemiologic metrics by age and sex groups for 315 conditions, for years 1990, 1995, 2000, 2005, 2010, and 2015. To estimate disease incidence and prevalence for each age and sex group in the United States, 1604 data sources were used, including hospital data, claims data, and surveys. A Bayesian meta-regression tool developed specifically for the GBD, DisMod-MR, was used to generate the modeled estimates of prevalence and incidence.13 GBD health conditions were aggregated to reflect the health conditions used in the Disease Expenditure project, with more details provided in the eMethods in the Supplement. To interpolate disease incidence and prevalence for the years that GBD does not report, log-linear interpolation was used for each age and sex group. Interpolation introduced additional uncertainty that is not included in estimates, although for most conditions included in this study, prevalence and incidence rates remained relatively constant across the study, and substantive short-term variations in the disease prevalence or incidence associated with the most expensive health conditions is unlikely. The relationship between changes in population size and age structure and spending growth were explored by comparing unadjusted spending growth rates with age- and sex-standardized growth rates. Age and sex standardization is commonly used to compare health outcomes that are dependent on the age structure of a population.14 To standardize 1996 spending to reflect the 2013 population profile, spending per person was calculated for each of the health conditions and for 36 age and sex groups in 1996, and those fractions were multiplied by the 2013 age- and sex-specific population estimates. The annualized rate of change was then calculated between adjusted 1996 values and the observed 2013 values. This procedure estimated changes in spending that would have occurred even if the 1996 and 2013 population size and age and sex structure were the same. Case-standardized growth rates were also measured. Spending per prevalent case was calculated for each health condition and for 36 age and sex groups in 1996, and those fractions were multiplied by 2013 age- and sex-specific prevalent case estimates. Using these standardized 1996 estimates, annualized rates of change were estimated to report the change that would have occurred even if the number of prevalent cases in 1996 and 2013 was the same. For injuries and cancers, spending per incident case was calculated, because spending for these conditions is associated more with incidence than with prevalence. Quiz Ref IDFor each of the 155 health conditions, 36 age and sex groups, 6 types of care, and 18 years of the study, 5 variables were constructed: (1) the total US population, (2) the fraction of the population living in each age and sex group, (3) the disease prevalence or incidence, (4) service utilization, and (5) service price and intensity. Disease prevalence was used for all health conditions except injuries and cancers, for which incidence was used. Service utilization and service price and intensity were measured differently for the different types of care (Table). For ambulatory, emergency department, and dental care, service utilization was measured as the mean visits per prevalent or incident case, while service price and intensity was the mean spending per visit. For inpatient and nursing facility care, service utilization was the mean bed-days per prevalent case, while service price and intensity was the mean spending per bed-day. For retail pharmaceuticals, service utilization was the mean number of purchased prescriptions per prevalent case, while service price and intensity was the mean spending per purchased prescription. Changes in service intensity included the use of new technology. The final data set included yearly estimates of all 5 factors for 1996 through 2013. Because the data from the Disease Expenditure project and interpolated GBD data are each complete without missing values, no additional data estimation or imputation were needed. Because not all health conditions exist for all age and sex groups and types of care, the 5 factors were measured for 21 713 unique combinations of condition, age and sex, and type of care, for each year. Five conditions (diphtheria, intestinal infectious diseases, measles, vitamin A deficiency, and tetanus) and several outlier cases were excluded because the related prevalence data were too low to allow for accurate estimation. To estimate uncertainty, 1000 draws of estimates were extracted from both the Disease Expenditure  and GBD databases. Each draw represents 1 statistical estimate, and the variation of these draws captures uncertainty associated with these input data. The measurement of rates of change, standardized rates of change, and the decomposition analysis were all completed on each of the 1000 draws independently so that data uncertainty was propagated throughout the entire process, and the reported uncertainty intervals (UIs) reflect all sources of uncertainty in both the input data estimation and the deterministic decomposition calculation. The reported estimates are the mean and the 2.5th and 97.5th percentiles of the 1000 estimates. More details about implementation of the 5-factor decomposition, including code, are reported in the eMethods in the Supplement. These analyses assessed 155 health conditions and tracked a $933.5 billion increase in annual US personal health care spending. Figure 1 shows health care spending increases and growth rates between 1996 and 2013. During this period, inflation-adjusted spending on inpatient, ambulatory, retail pharmaceutical, nursing facility, emergency department, and dental care increased at an annualized rate of 3.5%, from $1.2 trillion to $2.1 trillion. Across all health conditions, the greatest annualized growth rates were in emergency department care and retail pharmaceutical spending, at 6.4% and 5.6%, respectively. The condition with the greatest absolute increase in spending was diabetes, which increased at an annualized rate of 6.1% (UI, 5.3% to 7.0%), or $64.4 (UI, $57.9 to $70.6) billion. Most of this increase was related to increases in retail pharmaceutical spending. Low back and neck pain had the second-largest increase in health care spending, increasing at an annualized rate of 6.5% (UI, 5.3% to 7.7%), or $57.2 (UI, $47.4 to $64.4) billion between 1996 and 2013. In absolute terms, this increase was concentrated in ambulatory care and inpatient care. Spending increases on the treatment of hypertension ($47.6 [UI, $41.7 to $53.7] billion), the treatment of hyperlipidemia ($41.9 [UI, $37.7 to $45.4] billion), and depressive disorders ($30.8 [UI, $25.3 to $36.8] billion) were also notable. Figure 2 shows age- and sex-standardized health spending increases and growth rates (top panel). This panel shows spending factors not related to demographic changes. After controlling for population size and age structure, the increase in annual spending was $573.8 (UI, $570.6 to $577.3) billion. Even after adjusting for population size and age structure, diabetes and low back and neck pain remained the conditions with the largest increases in annual spending, although the increases and annualized growth rates decreased. Falls, the health condition with the seventh largest increase in annual spending, was no longer among the 10 health conditions with the highest growth after adjustment for age and sex. Figure 2 also shows case-standardized health spending increases and growth rates (bottom panel). Because differences in prevalence over time are controlled for, this panel reports growth associated with changes in health system service variables such as service utilization, price, and intensity. Diabetes went from showing the biggest change in spending to the fourth biggest change. After controlling for the number of prevalent cases, the health conditions with the largest growth in spending were low back and neck pain, treatment of hypertension, and treatment of hyperlipidemia. The 5-factor decomposition assessed 150 health conditions and tracked a $931.4 billion increase in US personal health care spending. (Diphtheria, intestinal infectious diseases, measles, vitamin A deficiency, and tetanus were removed because they did not have sufficient amount of prevalent cases to compete the decomposition.) The increase in population size was associated with a 23.1% (UI, 23.1% to 23.1%) increase in spending between 1996 and 2013, while population aging was associated with an 11.6% (UI, 11.4% to 11.8%) increase in spending (Figure 3). Change in disease prevalence or incidence was associated with a reduction in spending overall (2.4% [UI, 0.9% to 3.8%]), as was change in service utilization (2.5% [UI, −7.8% to +2.9%]). However, the relationship varied by condition category. Within cardiovascular diseases, decreases in prevalence were associated with a 20.7% (UI, 16.5% to 26.2%) reduction in spending, while among all other conditions together, decreases in prevalence or incidence were associated with a 1.1% increase (UI, −0.9% to +3.7%). Relationships between changes in utilization and cost growth also differed by type of care. Decreases in bed-days were associated with a 45.9% (UI, 40.7% to 51.9%) reduction in spending in the inpatient setting, whereas utilization increases were associated with a 67.1% (UI, 48.9% to 85.6%) increase in spending on pharmaceuticals. In contrast, increases in service price and intensity were associated with a 50.0% (UI, 45.0% to 55.0%) increase in spending between 1996 and 2013, although this association varied by type of care and health condition. Five-factor decomposition estimates reported in this article can be interactively explored at http://vizhub.healthdata.org/dex/. Figure 4 shows that annual spending on ambulatory care increased by $324.3 billion between 1996 and 2013, the most of any of the 6 types of care, from $381.5 billion in 1996 to $706.4 billion in 2013. The colored bars of Figure 4, Figure 5, Figure 6, and Figure 7 show the results of decomposition analyses associating each of the 5 factors with annual spending by type of care between 1996 and 2013. Increases in service utilization (more visits per case) were associated with a $115.5 (UI, $73.8 to $158.9) billion increase in ambulatory care spending. Greater service price and intensity (more spending per visit) was associated with an increase of $92.5 (UI, $53.7 to $130.7) billion in ambulatory care spending. Population size was associated with an $88.6 (UI, $88.4 to $88.8) billion increase in ambulatory care spending and age structure with a $33.7 (UI, $32.1 to $35.0) billion increase. Reductions in the number of prevalent cases between 1996 and 2013 for some expensive health conditions, such as ischemic heart disease and hypertension, were associated with a $6.0 billion reduction (UI, −$15.1 to +$3.1 billion) in ambulatory care spending. Annual spending on inpatient care increased by $258.1 billion between 1996 and 2013 (Figure 4), from $438.0 billion in 1996 to $697.0 billion in 2013. Like ambulatory care, population size and population age structure were associated with increases in inpatient spending, and disease prevalence or incidence with a slight reduction in spending. Service utilization and service price and intensity had opposite relationships with inpatient spending increases during this period. Reductions in inpatient bed-days per prevalent or incident case were associated with a reduction of $201.1 (UI, $178.3 to $227.1) billion in annual spending, whereas increases in spending per bed-day were associated with a $333.7 (UI, $315.2 to $350.9) billion increase in annual inpatient health spending. Figure 5 shows that the 5 factors were related to the individual health conditions in distinct ways. Within diabetes, which experienced the largest spending increase, increases were concentrated within retail pharmaceuticals ($44.4 [UI, $38.7 to $49.6] billion). Within retail pharmaceuticals, all 5 factors were associated with spending growth: there were more people, the people were generally older, age-standardized diabetes prevalence increased, there were more retail pharmaceuticals prescribed per prevalent case, and the mean spending per pharmaceutical increased. Across these 5 factors, service price and intensity was associated with the largest increase ($20.0 [UI, $12.1 to $27.5] billion). All 5 factors were also associated with increased diabetes spending in ambulatory care settings. The other 4 panels of Figure 5 show a diverse set of factors related to increases in spending on different health conditions. Increases in depressive disorder spending was most strongly associated with increases in spending on ambulatory care and retail pharmaceuticals, and all 5 factors were related to increases in spending for each type of care. Increases in spending on other neurologic disorders were most strongly associated with increases in the utilization of ambulatory care. Despite reductions in prevalence, there were still net increases in spending on retail pharmaceuticals for hyperlipidemia. In addition, increases in utilization were associated with increases in spending on ambulatory care and retail pharmaceuticals for the treatment of hypertension. The relationship of the 5 factors with spending for each type of care for each of the 150 conditions is reported in the eResults in the Supplement. Figure 6 demonstrates that patterns in spending changes and the factors associated with these changes differed by broad age category. Service utilization was associated with much larger increases in the 65 years and older age group ($42.1 [UI, $33.5 to $51.6] billion of the $74.3 [UI, $70.9 to $77.5] billion increase in pharmaceutical spending), compared with ages younger than 65 years ($33.9 [UI, $20.4 to $47.2] billion of the $100.4 [UI, $97.2 to $103.7] billion increase). Service utilization was also associated with the largest spending increases within ambulatory care among people 65 years and older ($50.6 [UI, $38.1 to $63.0] billion of the $101.6 [UI, $95.9 to $107.6] billion increase in ambulatory spending) compared with the younger age group ($64.8 [UI, $34.8 to $97.7] billion of the $222.7 (UI, $216.9 to $228.4) billion increase). Spending on nursing facility care was more strongly related to overall spending changes for the older age group, whereas emergency care and dental care were more strongly related to overall spending changes for the younger age group. Figure 7 shows how the 5 factors were associated with spending within each type of care over different periods. From 1996 to 2002 and from 2002 to 2008, spending increases were most associated with growth in service price and intensity ($259.1 [UI, $201.5 to $311.1] billion and $198.9 [UI, $169.6 to $229.7] billion, respectively). The relationships between the 5 factors and spending within different types of care changed somewhat from 2008 to 2013. Service price and intensity was negatively associated with spending on ambulatory care ($23.1 [UI, −$41.2 to +$1.6] billion), nursing facility care ($6.7 [UI, $1.5 to $18.7] billion), and prescribed retail pharmaceuticals ($4.4 [−$15.6 to +$6.6] billion). Quiz Ref IDThis study measured how 5 fundamental factors were collectively associated with a $933.5 billion increase in annual US health care spending between 1996 through 2013. Although population size and age were associated with increased spending for most health conditions and types of care, the other 3 factors had varying relationships with spending, depending on the health condition and type of care. Across all types of care and health conditions, increases in service price and intensity had the strongest associations with the total spending increase. Service utilization was associated with increases in spending on ambulatory care and retail pharmaceuticals but with reductions in spending on inpatient care. The association of disease incidence and prevalence with spending growth was minor overall but varied by condition. For example, the increasing prevalence of diabetes was associated with more health care spending on this condition, while reductions in the prevalence of cardiovascular diseases were associated with decreased health care spending. This study adds to existing literature that has measured factors leading to increases in health care spending.23 These existing studies, which typically focus on 1 or 2 factors,24 have reached varying conclusions about which factors are associated with increases in health care spending.5,6,8,25 Some studies have suggested that spending increases are primarily attributable to changes in the spending per treated case,5,7,26 often pointing to technology changes as a significant contributor to spending increases. The research presented here tests and affirms these findings, demonstrating that service price and intensity have had the strongest association with increasing health care spending between 1996 and 2013 of the factors examined, associated with to a 50.0% (UI, 45.0% to 55.0%) increase overall. This increase was greatest in inpatient care and was an especially large contributor to increases in spending on retail pharmaceuticals for diabetes and on inpatient care for low back and neck, among others. The present study differs from prior studies in its ability to separate increases in service utilization from service price and intensity and to assess these relationships independently for each health condition and type of care. The relationship between changes in service utilization and spending has been more variable. Service utilization in an inpatient setting has decreased between 1996 and 2013, meaning there were in general fewer inpatient bed-days per prevalent or incident case. This is observed for many health conditions, including cardiovascular conditions, labor and postpartum care, and low back and neck pain. This likely corresponds to encouragement from insurers and other groups to minimize the number of inpatient days associated with each admission.27 However, concurrent with reductions in inpatient days, there have been increases in inpatient prices and intensity of services. This relationship is somewhat tautological, because for many cases the number of inpatient services may have been compressed into a smaller number of days, thus leading to more spending per day. For most health conditions, however, the increase in inpatient spending associated with increases in price and intensity was greater than the reduction in spending associated with reductions in inpatient utilization, yielding a net increase in inpatient spending. The reduction in inpatient utilization has also occurred simultaneously with increases in ambulatory care and pharmaceutical utilization and in spending in these settings. This substitution may reflect attempts to encourage less costly modalities of care, as has been observed elsewhere.28. In addition to increases in cost per case, existing research also has  pointed to changes in disease prevalence or changes in demography as underlying factors fueling increases in health care spending.7,25 The finding in the present study that changing prevalence and incidence are less strongly associated with spending increases—and in some cases are associated with spending decreases—may be explained by the separation of changes in population size and aging from changes in disease prevalence or incidence. In addition, the protracted associations between disease prevalence and spending is also likely attributable to the fact that this study measured the associations with changes in clinical prevalence, which is measured by the GBD project. Because clinical disease prevalence or incidence is used, increases in spending due to changes in health care use will correspond to increases in service utilization. Previous research has underscored the importance of using clinical prevalence rather than treated prevalence, because an increase in spending attributed to treated prevalence might actually be caused by more people receiving care.26Quiz Ref ID Despite disease prevalence or incidence having a relatively small relationship with spending growth across all diseases, changes in prevalence or incidence were associated with sizable increases or decreases for a number of specific health conditions. Diabetes was one health condition for which increases in disease prevalence were associated with increases in health care spending, whereas cardiovascular diseases such as ischemic heart disease and cerebrovascular disease had reductions in disease prevalence between 1996 and 2013. These findings have a diverse set of policy implications. For example, the finding that service price and intensity are major contributing factors to spending growth may suggest that policy efforts focused on reducing these factors could hold promise for reducing health care spending. Condition-specific analyses may point to particular health conditions to which attention could be turned, such as the rising price of pharmaceuticals for diabetes. In addition, the estimates developed here could also be used for forecasting the potential effect of policies that alter particular factors associated with cost growth, although such analyses were beyond the scope of this project and could be considered only exploratory. Quiz Ref IDThis study has several limitations. First, the expenditure estimates from the Disease Expenditure project were not separated by payer. Because public payers often pay lower prices than private or out-of-pocket payers, performing this analysis by payer could yield important insights. Second, data on spending and disease burden were only captured at the national level. Understanding subnational variation in health care spending would provide critical evidence for policymakers acting at state and local levels. Third, the data used for this study did not allow service price and intensity to be disaggregated into the individual components of changing service prices, intensity, and technology. To measure these distinct factors (which were aggregated in the present study), data tracking specific services or treatments—or data tracking relative-value units—would be needed for each health condition, age and sex group, type of care, and year of this study. This warrants further research. Fourth, estimates produced by the Disease Expenditure project only extended to 2013. The health care system is dynamic, and patterns may have changed in the years since. Specifically, pharmaceutical spending increased substantially in 2014 and 2015,1,31 and the gradual implementation of the Affordable Care Act may have contributed to other changes during this time. These data did not capture more recent trends, but future updates to the Disease Expenditure project aim to remedy this. The resident looked worried and the intern was nervous. They were concerned about an elderly patient with low cardiac output heart failure. We reviewed our plans for the day, and as we entered his intensive care unit (ICU) room, it was obvious that our patient was scared. He had a rapid heart rate and an irregular pulse. His extremities were cool to the touch despite two vasopressors supporting his blood pressure. His wife sat attentively next to him. We asked him how he felt. We discussed his heart rate, blood pressure, urine output, his ever-changing medications and even the remote and unlikely possibility of a heart transplant. We assured him that he was slowly improving. They were aware of our assessment and plans. Next, we attempted to understand the patient’s longer-term concerns in the context of his illness. Given his precarious health situation, we knew that the conversation needed to be handled with great understanding and empathy. These are difficult matters to talk about. We often need to provide patients with the suspected pace of their disease to help them prioritize their remaining time. Rather than provide a definitive number of weeks or months that we believe a patient might be expected to live, I try to anchor a general time frame to a well-known calendar event. For example, if I am caring for a patient in January with a high probability of less than six months to live I might say, “This will likely be your last Fourth of July.” If a patient gains strength from their faith, I would consider saying, “This may be your last Easter (or Passover or Ramadan).” In my experience, this approach conveys sensitivity to the patient’s situation and concern for the important events in his or her life. It also reduces the chance of the patient or family fixating on the exact date. No one wants to have this serious and difficult conversation and then argue with the patient or their family about “the number.”. At the bedside, we were prepared, attentive, and respectful. We were not surprised when the patient asked, “Well how long do I have?” We acknowledged the unknowable aspect of this question. He persisted. He wanted an answer and so did his wife. The patient had a poorly functioning left ventricle with a low ejection fraction. His blood pressure required pharmacological support. His urine output was low. His prognosis was poor. “This was probably your last Christmas,” I said to them—it was January. They were quiet. There were a few more comments and some attempt to reduce the sadness in our assessment. We promised to return later to address any new questions or concerns. I was proud of the team. I thought we all communicated accurately, considerately, and humbly. Later in the afternoon, I returned to the patient’s room and I was confronted by an annoyed patient and an indignant and furious wife. They were incensed that I had provided a “bookend” to his life. How could I be so insensitive to them? Miracles can happen. They were so livid that they threatened to report me “to higher authorities.” I was speechless. My first impulse was to flee, but I immediately apologized: “I am so sorry that I upset you.” They were not satisfied. They wanted me to recast my assessment. I listened to them and faced their anger. It was difficult. I immediately knew and acknowledged that I made a huge mistake linking his health to his past instead of framing his health and connecting him to events in his future. I apologized, and I reassured them that I had not intended to offend them. I wanted to say that we had intended to help them prioritize their time and not demoralize them but that seemed argumentative, so I did not say it. How did our conversation go so badly? I needed to think this through and even more I needed to “feel it through.” After a few more attempts at apologies, I promised to return. It is not often that I find myself leaving a patient in order to regroup but that is exactly what I did. I went to the team room. The residents and I replayed what had just happened. We spoke about the absolute need for honesty, clarity, and transparency when speaking with our patient. Based on the data, our assessments were accurate, but still we could be wrong and perhaps my discussion could have been more nuanced. I could have remained honest and truthful but less certain. One resident suggested an approach that included a sentiment like, “I am sad to give you this news. I hope you understand that I am trying to help you plan for an uncertain future.” That was good. Another resident suggested that the patient’s and family’s anger was understandable because the past attending physicians had not focused on the patient’s life outside the hospital. No one had really discussed how ill he was and how difficult his recovery would be or what he might expect in the weeks and months ahead if he made it out of the hospital. No wonder they were angry. I was telling them that he would likely die sometime in the next year. What would I do now?. I thought about other times when anger seemed to surprise and overwhelm me. I realized that this patient, his wife, and I experienced a break in our empathetic relationship. What made it so difficult was that sadly and unknowingly, I had triggered the break. I went back to the patient’s bedside not really sure what would happen or what I would say. They looked a bit forlorn and I was apprehensive. In retrospect, we were anxious to begin our healing process and revive our connections. The patient and his wife started the conversation by apologizing to me! I was shocked but I listened and then I acknowledged for being out of step with the daily hardships during the hospitalization and their fears. They said that although they appreciated my attention to their future, what they needed from me was to focus on their present. They were right. His future was either too scary, too far away, or simply irrelevant to their current daily struggle. I was too focused on helping them plan for his future, but what they wanted was hope to successfully overcome their present problems and the courage to confront their fears. In that moment, I realized they needed me to be hopeful and display confidence. I had mistakenly allowed my needs to help them plan for their future trump their needed hope. In fact, the team was hopeful. We emphasized his slow improvement and expressed optimism that he would survive his present situation. We focused on his medications, tests, and treatments, so supporting their hopefulness became a high priority. Although we knew that hope alone would be insufficient to plan for or assist them in organizing their future, we also knew it could provide comfort and help them get through their day. We never recanted our assessment of his future, and we agreed to be honest if they brought it up again; however, we remained firm in our belief that he would continue to improve. They did not bring it up again. What did I learn from this incident? I was reminded that there is an important balance between medical knowledge and my judgment and the need to bolster a patient’s grit and courage. Without attention to this balance, we risk a break in our empathetic relationship with patients. For me, the lost connection occurred when my focus shifted from the actual needs of the patient to what I thought was important to the patient. Despite my best intentions to help this patient and his wife plan for their future, I was not empathetic to his daily struggles and their fear of the future. He and his wife needed me to be their advocate not only for his health but for his hope too. The resident and I discussed this patient every day. At the end of our rotation, we talked about the “takeaway” lessons. More than just discussing disease pathogenesis, pharmacology, and procedures used to correct heart failure, we talked about remaining empathetically connected to patients and understanding their needs. We talked about the need to repair relationships when we experienced a rupture in our connections with patients. And I was reminded of the risk of losing sight of my patients’ need for hope. Even though emergency medicine physicians provide only a comparatively small proportion (4.7%) of the total number of immediate-release opioid prescriptions dispensed in the United States,3 the emergency department (ED) setting is where many patients frequently receive their first opioid treatment. Patients with moderate to severe pain are often sent home with a prescription for an opioid medication if they were successfully treated with an opioid medication in the ED. In addition, the proportion of adult ED patients prescribed an opioid medication has increased significantly during the current opioid epidemic. From 2001 to 2010, the percentage of overall ED visits in the United States (both visits related to pain and those not related to pain) in which any opioid analgesic was prescribed increased from 20.8% to 31.0%.11 Of the several types of opioids prescribed, hydrocodone and oxycodone had the greatest increases in ED discharge prescriptions during this period.11 This increase in prescribed opioids may have long-term consequences for ED patients. A recent study by Hoppe et al7 found that among opioid-naive ED patients in Colorado who filled an opioid prescription for a mild to moderate acute pain condition (eg, abdominal, pelvic, back, chest, dental, extremity, head, or neck pain) that was not expected to result in recurrent chronic pain, 17% were still taking an opioid medication 1 year after their initial ED evaluation. Increased long-term opioid use among ED patients also may be related to the specific prescribing pattern of the treating emergency physician. Barnett et al12 found wide variation in the rates of opioid prescribing among emergency physicians and an association between these patterns and risk of long-term opioid use. Long-term opioid use was significantly greater among patients treated by high-intensity prescribing emergency physicians compared with patients treated by low-intensity prescribers. However, like other medical care settings, the treatment of acute pain in the ED is based mostly on tradition and experience with only limited clinical evidence directly comparing opioid vs nonopioid medications for acute pain management.13 In addition, there are significant gaps in research demonstrating the effectiveness of opioid medications for providing adequate analgesia.14. Despite the limited contribution of emergency physicians in the provision of dispensed prescription opioid medications, the ongoing opioid epidemic will require innovative evidence-based public health strategies in the ED as well as other acute care settings.15 Volkow and Collins15 from the National Institutes of Health recently suggested the following areas of scientific research to mitigate the opioid epidemic: (1) developing better overdose-reversal interventions to counteract the very–high-potency synthetic opioids; (2) finding new treatment strategies for opioid use disorders and addiction; and (3) finding nonaddictive treatments for chronic pain. However, their assessment did not consider the prevention of new cases of opioid addiction among patients treated for acute pain. Preventing new patients from becoming addicted to opioids may have a greater effect on the opioid epidemic than providing sustained treatment to patients already addicted to opioids, in whom it may take many years to achieve recovery. Part of the overall strategy will also require a reduction in number of prescribed opioids through the development of nonopioid treatment approaches for both acute and chronic pain management.16-18. The combination of ibuprofen and acetaminophen with different mechanisms of action apparently provides additive analgesic effects while reducing the short-term risk for adverse effects. Except for the treatment of dental pain, this combination has not been extensively studied in the United States but has been used in New Zealand, Australia, and Europe.20,21 Nevertheless, prior evidence of the effect of combined ibuprofen and acetaminophen on acute pain compared with opioid medications is limited.13 Important and novel features of the trial by Chang et al19 were the use of standard doses of common medications for patients with moderate to severe pain (mean pain score of 8.7 on the 0 to 10 numerical rating scale) and the use of a standard pain score that is familiar to clinicians in the ED setting. These aspects allow a certain degree of generalizability to other EDs and other acute medical care settings. In addition, the reduction of overall prescribed opioid medications may also limit nonmedical use of prescribed opioid medications illicitly obtained from friends and family members.3. An important limitation of the trial by Chang et al19 is that the effect of the ibuprofen and acetaminophen combination was assessed only in the ED. Typically, treatment regimens that provide adequate pain reduction in the ED setting are used for pain management at home. However, the study did not address whether the combination of ibuprofen and acetaminophen would provide similar pain reduction after the initial clinical assessment and treatment of acute pain in the ED setting. In addition, a blanket approach restricting opioid medications may adversely affect certain patients who achieve better pain reduction from opioid vs nonopioid medications. Stemming the opioid addiction crisis will also require reexamination of the long-standing assumptions that opioids are superior to nonopioids in most clinical situations requiring management of moderate to severe pain. Genuine efforts should be made to reduce overall opioid prescribing in the ED setting while still providing adequate pain relief. The trial by Chang et al19 provides important evidence that nonopioid analgesia can provide similar pain reduction as opioid analgesia for selected patients in the ED setting. The demonstrated effectiveness of the ibuprofen and acetaminophen combination for moderate to severe pain may also translate to outpatient management and other clinical settings of patients with acute pain. However, this will require future investigations. Dieleman et al report that, after adjusting for price inflation, health care spending on inpatient, ambulatory, retail pharmaceutical, nursing facility, emergency department, and dental care increased by $933.5 billion between 1996 and 2013, from $1.2 trillion to $2.1 trillion. Changes in service price and intensity accounted for 50.0% (or an estimated $583.5 billion) of the spending increase, but the association between the 5 factors and spending varied by type of care and health condition. Population growth (23.1% increase, an estimated $269.5 billion spending increase) and aging of the population (11.6% increase, an estimated $135.7 billion spending increase) combined represented more than 33% of the spending increase. Changes in disease prevalence or incidence were associated with spending reductions (2.4% decrease, an estimated $28.2 billion decline in spending), and changes in service utilization were not associated with a statistically significant change in spending. The authors also report that across all health conditions, the greatest annualized growth rates were associated with emergency department care (6.4%) and retail pharmaceutical spending (5.6%). The findings from this report have important implications for US policy, payers, hospitals and clinicians consumers, and others. First, the study again underscores that the United States is on an unsustainable growth path in terms of health care costs and must get costs under control, and highlights several of the potential levers. In terms of service price and intensity representing the major driver of costs, payers and hospitals and clinicians across the private and public sector must work to control increases in prices. Some states, such as Maryland and Vermont, have approached this through multipayer payment models at the state level. Maryland currently uses all-payer rate setting and global budgets for hospital care and will expand that to all hospitals, clinicians, and post–acute care services by January 1, 2019.2 Another approach would be payers, hospitals, and clinicians partnering at a local level to control price increases—for example, to establish a contract that minimizes price increases but that allows the hospitals and clinicians to be financially successful by meeting quality metrics and lowering total cost of care (eg, an accountable care–type contract). The other components in service intensity that drive higher prices are scientific and technological advancements. The question is which of these advancements are truly associated with better health outcomes and which are expensive but deliver no or very limited patient benefit. Research- and evidence-based coverage policy can help address the technology component of costs. Another intervention that could decrease price would be competition for consumers based on transparent pricing and quality metrics for episodes of care or specific reference-priced services. This has worked for some high-end procedures (eg, orthopedic) and imaging3,4 but still has not become common or widespread, and data are limited that this approach will be broadly successful. The recently announced Centers for Medicare & Medicaid Services Innovation Center request for information solicited comments on these types of models.5. The report by Dieleman et al highlights that more than one-third of the annual increase in spending—that attributable to aging and growth of the population—is likely not modifiable, making it even more important to focus on the two-thirds of spending increases that is potentially modifiable. Disease prevalence and incidence vary by condition. For cardiovascular disease, the United States had decreased prevalence, but this was more than offset by service intensity and utilization. For diabetes, the increased prevalence, clearly related to the epidemic of obesity, is driving a portion of the increased costs. For instance, diabetes was the condition with the greatest absolute increase in spending (annualized rate of 6.1%, an estimated $64.4 billion spending increase), with most of this increase related to increases in retail pharmaceutical spending. Various prevention initiatives related to diabetes may reduce the prevalence and the related spending increases, although the prevalence of obesity is not declining, and obesity remains the single most important risk factor for type 2 diabetes. In terms of categories of spending, Dieleman et al determined that spending on ambulatory care, including outpatient hospital and emergency services, and pharmaceuticals were some of the primary drivers of increase in cost. For outpatient services, there remains a significant payment differential between procedures performed at outpatient hospital centers (eg, a colonoscopy performed by a hospital-employed gastroenterologist) and those performed at physician offices or ambulatory surgical centers, with the former being far more expensive than the latter. Congress recently passed a statute that any physician group purchased by a hospital can no longer charge Medicare higher rates.6 This only applies to new acquisitions. Many private payers are also putting into place these “site neutral” payment policies. In terms of limitations, Dieleman et al note that their data end in 2013. Much delivery system change has occurred in the last 4 years, so updating this analysis with more recent data would be informative. In addition, the analysis was based on national data, but many of these trends may vary in state and local markets. The associations are also in the context of many other changes. For example, overall service utilization did not play a large role in increased spending, but these data were based on a period when many of the interventions (eg, accountable care organizations and new payment models) primarily have focused on decreasing utilization. So the question remains what the trends might have been without some of these interventions. Three key principles emerge from this report. First, the findings reported by Tricco et al7 highlight the importance of exercise, especially for older individuals who are habitually sedentary or those who are inactive due to illness. Regular physical activity and exercise have numerous health benefits. Clinicians can encourage exercise and prescribe exercise regimens12 for older patients with chronic and acute conditions and for patients before and after surgery. Clinicians should focus on patients at highest risk for falls, but there is no reason not to encourage most older patients to exercise. As the report emphasizes, anyone increasing physical activity—and especially very old sedentary older adults—should begin at low intensity and duration, understanding that it is better to increase exercise levels gradually rather than begin at too high a level, only to become discouraged and stop. Walking, balance, and strength training are all valuable but most important is to find an exercise that a person will do regularly so that it becomes a habit. It is also important to recognize that fall risk may increase when a person who is sedentary begins exercising regularly, thus spending more time at risk for falling. Advising patients to start exercising slowly and carefully usually is effective, but for frail people, as well as those reluctant to initiate exercise, it may help to recommend personal trainers or physical therapists, especially when the risks are higher. Second, Tricco et al7 also found that combining exercise with other interventions was associated with lower risk of falls, especially when tailored to patients’ needs. For instance, vision assessment was shown to be associated with reduced fall risk and can be promoted by primary care physicians and in public health messaging. Visual impairment among older patients is highly prevalent, usually has an insidious onset, and can easily escape detection, but most important, often can be corrected. Tailoring care to prevent fractures is also worthwhile because vitamin D and calcium intake may reduce fracture risk, as may osteoporosis treatment plus vitamin D and calcium. In addition, for persons who have sustained falls, tailored recommendations can combine exercise and environmental assessment and modification along with multifactorial treatment. A third general principle is the need to implement proven approaches. Individuals, health care systems, and even whole communities will need to change established behaviors and practices that have persisted over long periods. In addition to literature on ways to promote and accomplish behavior change for individuals’ health, there is an emerging knowledge base from the field of implementation science14,15 on ways to promote change within health organizations and communities. Knowledge about effective implementation can inform health care entities aspiring to become learning health systems—ie, organizations in which research improves practice and practice informs research.16 Pragmatic clinical trials will likely accelerate change with research studies embedded in everyday health care system settings and the generation of practical evidence that encourages the implementation and spread. The systematic review by Tricco et al7 reported in this issue of JAMA provides evidence that knowledge and practices to reduce fall risk among older persons are available, especially by promoting regular and safe exercise. Using advances from implementation science could help minimize delay in creating sustained system- and community-wide programs to prevent falls. Direct-to-consumer campaigns may be useful to increase awareness about potentially devastating consequences of falls and ways to prevent them. Clinicians and health care systems should evaluate how best to deliver interventions to reduce falls among their patients and increase efforts to achieve effective fall prevention among older persons. For the children of women without epilepsy, the rate of major congenital malformation was 2.51%. Random-effects modeling was used to calculate the rates when there was a significant level of variance across the studies from which the data were drawn. For the antiepileptic drug exposure groups, valproate was associated with the highest prevalence of major congenital malformation (10.93% [95% CI, 8.91%-13.13%]; 235 of 2565 children). Major congenital malformations were reported in 96 of 4195 children (2.31% [95% CI, 1.87%-2.78%]) exposed to lamotrigine. Phenobarbital was associated with higher rates of cardiac malformations vs levetiracetam (RD, −0.02 [95% CI, −0.04 to 0]), lamotrigine (RD, −0.02 [95% CI, −0.04 to 0]), and phenytoin (RD, −0.03 [95% CI, −0.05 to 0]). Valproate exposure was associated with a higher rate of neural tube defects vs carbamazepine (RD, −0.02 [95% CI, −0.02 to −0.01]), lamotrigine (RD, 0.01 [95% CI, 0.01 to 0.02]), and levetiracetam (RD, 0.01 [95% CI, 0.01 to 0.02]). Valproate was associated with a higher rate of cardiac malformations vs phenytoin (RD, 0.02 [95% CI, 0.01 to 0.04]), lamotrigine (RD, 0.02 [95% CI, 0.01 to 0.02]), and levetiracetam (RD, 0.02 [95% CI, 0.01 to 0.03]). There was evidence of a dose-response relationship for valproate, but a similar relationship was not found for other antiepileptic drugs.1. The stated aim of GOLD is to raise awareness of and improve prevention and treatment of COPD.6 GOLD is composed of an international network of health care professionals and public health officials with global expertise. GOLD was initially launched in 1997 and involved collaboration with the National Institutes of Health, National Heart, Lung, and Blood Institute, and World Health Organization. The committee reviews published literature to determine GOLD recommendations. The committee does not make any recommendations for therapies unless at least 1 major regulatory agency has approved such therapies. The GOLD board of directors and national leaders reviewed the recommendations, and 10 outside experts performed an external review. There was no mention in the report about how conflict of interest was managed. This summary focuses on screening for COPD. The guideline discussed here (Table) comes from the 2017 report, the fourth major revision of GOLD, which includes information reviewed by the science committee between 2015 and 2016.6 Three main recommendations were made with respect to COPD screening.2,5,6 First, similar to prior GOLD reports and other guidelines, screening asymptomatic patients for COPD with spirometry is not recommended.5-7 Studies of pulmonary function screening conducted in Australia and Sweden showed a sensitivity of only 50%.5 Furthermore, there are no data that screening improves morbidity, mortality, or quality of life. The lack of evidence comes more from an absence of studies than from the presence of negative studies. Among the limited evidence on this subject were a handful of studies evaluating whether screening affects smoking cessation. Four of 5 studies on this topic showed that screening does not affect smoking cessation.5,6. The goal of COPD screening is to identify patients with COPD at a stage at which identification can provide opportunities for treatment to limit symptoms and exacerbation frequency and improve quality of life. To date, evidence does not exist that supports this benefit.9 In the general population, lung function does not predict mortality independent of a history of smoking, and there are no data on how lung function testing could be used to aid in treatment decisions or improve outcomes for patients with COPD diagnoses prior to symptom onset. The theoretical benefit of early diagnosis motivating smoking cessation has also generally not been supported. The potential harms of population screening include patient- and system-level burden of both spirometry assessments and false-positive and false-negative results.9 One systematic review concluded that “hundreds of patients” would need to be screened with spirometry to avoid a single exacerbation.2. A 50-year-old woman with chronic pain and recurrent infections from common variable immunodeficiency presented to a new primary care physician for management of her pain medications. Her pain was related to multiple vertebral fractures due to chronic steroid use for an inflammatory polyarthritis that was not responsive to hydroxychloriquine and methotrexate. Her pain medication regimen (methadone, 20 mg [3×/d]; immediate-release morphine, 30 mg [5×/d]; gabapentin, 1200 mg [2×/d]; duloxetine, 60 mg/d; and celecoxib, 200 mg [2×/d]) helped her independently complete instrumental activities of daily living. She reported no adverse effects (eg, somnolence or constipation). A comprehensive urine drug screen using immunoassay and mass spectrometry was ordered (Table 1). Urine drug screen results showed the expected morphine and methadone but also hydromorphone and codeine. Hydromorphone is a minor metabolite of morphine (found at ≤10% of the morphine urine concentration)7 that can be seen in patients prescribed higher daily doses of morphine and detected when morphine concentrations reach 10 000 ng/mL on assay.7 Codeine is not a metabolite of morphine, but it can be an impurity in the production of morphine (estimated at 0.04%-0.5% of the concentration of morphine).4 The patient’s hydromorphone and codeine urine concentrations are consistent with these findings. It is possible that the patient had an additional source of hydromorphone or codeine, which cannot be determined from the urine result. If concentrations were higher than the 10% or 0.5% of the morphine concentration, the patient could be taking hydromorphone or codeine. Screens using only immunoassay for common drugs of abuse are an alternative to mass spectrometry–based screens designed for pain management. Immunoassay methods are less expensive (Medicare reimbursement range, $20.22- $107.85)1 and allow physicians to quickly determine whether commonly abused drugs are absent and whether opiates are present; however, they are subject to false-positive and false-negative results, which vary based on the drug, drug class, and the assay used. Most semisynthetic opioids (ie, hydrocodone, oxycodone) and synthetic opioids (ie, fentanyl, methadone) are not reliably detected by an immunoassay designed to detect opiates. Immunoassays specifically designed to detect oxycodone show sensitivities and specificities at approximately 99%, while those specifically designed to detect synthetic opioids are approximately 95%.8 Mass spectrometry is used to confirm immunoassay results and has been recently advocated as a first-line test in chronic pain management.8. After contacting her prior physician and reviewing the prescription drug-monitoring program (a statewide electronic database that collects information on dispensed controlled substances), there was no concern for misuse or abuse. An opioid consent and contract,9 which detailed the risks and benefits of opioid therapy and outlined expectations of the patient, was reviewed and signed by the patient. Although her daily opioid dosage was greater than 200 morphine milligram equivalents, the benefit of pain control that promotes independence was considered to outweigh the potential harms of therapy. The regimen was continued with a plan for a multidisciplinary approach to treating her pain and slowly tapering the opioids. The ASHG statement appeared 1 day after publication in Nature of a study wherein scientists at the Oregon Health & Science University reported that they had succeeded for the first time in the United States in correcting the pathologically mutated DNA of 1-celled human embryos using CRISPR/Cas9. Their method, they said, resulted in production of 42 of 58 embryos (72%) that were free of the targeted mutation in every cell, without creation of any new mutations. The remaining 16 had unwanted DNA additions or deletions. (Some of the main conclusions of the study have since been challenged by other researchers in a bioRxiv preprint.). However, the NASEM advocated leaving the current regulatory processes in place as far as oversight of preclinical genome editing research. Kelly Ormond, MS, CGC, lead author of the ASHG statement, said that her group and the NASEM were basically “in the same place,” with regard to germline editing, but the major difference was the ASHG group’s advocacy of using federal funds to finance the research. “We felt this very strongly despite the obvious political issues,” she said. “We don’t anticipate changing the funding landscape, but we all felt we needed to make a statement.”. A survey published recently in Science illustrated just how crucial knowledge is in shaping attitudes about the somatic and germline gene-editing technology. It asked respondents 9 factual questions about genome editing to gauge their familiarity with the subject. Those able to answer 6 or more questions expressed much higher levels of support for therapeutic gene editing (76%) than those unable to answer any questions (32%). For enhancement, the more informed were more accepting than the uninformed by a 41% to 19% margin. The drug is a recombinant immunoglobulin G1 monoclonal antibody that binds to vascular endothelial growth factor (VEGF). By blocking VEGF’s interaction with its receptors, bevacizumab-awwb prevents the development of new blood vessels that solid tumors need to thrive. Bevacizumab-awwb and bevacizumab are from different manufacturers that use their own unique cell line and proprietary processes. Differences between the 2 drugs aren’t clinically meaningful in terms of safety, purity, and potency. This study was conducted using the French National Health Insurance claim database known as SNIIRAM (Système National d’Information Interrégimes de l’Assurance Maladie). This system covers the entire French population with various health insurance plans based on individuals’ employment status. The general health insurance plan covers employees in the industry, business, and service sectors; public service employees; and students, accounting for approximately 88% of the French population. Only claims reimbursed from the general health insurance plan were considered because of their availability and quality. This database contains individual, anonymous data on sociodemographic characteristics (eg, sex, age, and date of death) and all medical claims since 2008, including dispensed drugs with date of delivery, laboratory tests, and outpatient medical care. The database also contains medical information on the presence of any serious and costly long-term disease giving entitlement to 100% health insurance coverage based on the French National Health Insurance list of 30 eligible chronic conditions,15 with information on diagnosis encoded in the International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10) and date of disease onset. Information on affiliation to the Complementary Universal Health Insurance scheme, a system providing free access to health care for people whose annual income is lower than 50% of the poverty threshold in France, is also available. An anonymous, unique identifier for each individual links SNIIRAM information to the national hospital discharge database (Programme Médicalisé Des Systèmes D’information [PMSI]). The PMSI includes information about every admission at a public hospital or in a private clinic in France since 2006, either for an inpatient stay or ambulatory care. Diagnoses (recorded using their ICD-10 codes) and treatments, either medical or surgical, provided during hospital stays are available. The SNIIRAM and PMSI databases have been described and used for epidemiological research previously.16-20. Quiz Ref IDTo avoid potentially confounding factors that may have increased the risk of lymphoma, organ transplant recipients, HIV-infected patients, and patients with a history of cancer including lymphoma were excluded (Figure). Individuals with a diagnosis of cancer or lymphoma within 3 months following their IBD diagnosis were also excluded to minimize the possible inclusion of nonincident cases. In addition, these patients follow different therapeutic management strategies.21,22 Furthermore, patients with a single hospital discharge diagnosis of IBD and no pharmacy claim for IBD medication, who were considered to have an uncertain IBD diagnosis, were excluded. Patients were followed up from January 1, 2009, or the date of their IBD diagnosis, whichever occurred first, until the first of the following events: death, any cancer diagnosis, or December 31, 2015. Anti-TNF agents included infliximab and adalimumab. Infliximab is administered at hospitals or private clinics, whereas adalimumab is dispensed by pharmacies. Thiopurines (azathioprine and 6-mercaptopurine) are dispensed by pharmacies, which are authorized to deliver 1 month supply of treatment. Drug exposures were assumed to start the day of the drug infusion or delivery. Patients who received infliximab were considered as being exposed during the 2 months following the infusion, whereas those who received adalimumab or thiopurines were considered as being exposed for 1 month following the delivery. Cox proportional hazard regression models were used to estimate crude and adjusted hazard ratios (aHRs) and their 95% CIs comparing the risk of lymphoma according to treatment exposure treated as a time-dependent covariate. HRs comparing patients of each exposure group (thiopurine monotherapy, anti-TNF monotherapy, or combination therapy) with unexposed patients (either never exposed or previously treated but no longer exposed) were first estimated. Then, HRs associated with (1) exposure to anti-TNF monotherapy vs thiopurine monotherapy; (2) exposure to combination therapy vs thiopurine monotherapy; and (3) exposure to combination therapy vs anti-TNF monotherapy were estimated. In multivariable analysis, models were adjusted for baseline and time-dependent covariates. Various sensitivity analyses were conducted using alternative assessment criteria for exposure and outcome, especially in case of occurrence of IBD treatment switches or withdrawals during follow-up. First, to exclude incipient lymphoma events, a lag period of 3 or 6 months following the initiation of each line of thiopurines and/or anti-TNF agents was introduced. Follow-up during these lag periods did not account for person-years in the exposed nor the unexposed groups. Second, analysis was restricted to the first line of IBD treatment by censoring follow-up time after any treatment switch or withdrawal. Third, to account for a potential persisting risk of lymphoma after treatment discontinuation, exposure time was extended for 3 or 6 months following treatment switch or withdrawal. In addition, an analysis including patients with an uncertain diagnosis of lymphoma was also performed. Between January 1, 2009, and December 31, 2013, 246 704 individuals were identified with IBD. Among them, 1665 were HIV positive, 1210 were organ transplant recipients, 20 356 had a history of cancer, and 34 184 had an uncertain IBD diagnosis (Figure). A total of 189 289 individuals were therefore included in the cohort, among whom 127 948 (68%) had a prevalent IBD. The annual number of IBD incident cases ranged between 11 411 and 13 212 from 2009 to 2013. The median follow-up time was 6.7 years (interquartile range, 4.5-6.9 years). Fifty-four percent of patients were women, and the median age at cohort entry was 43 years (Table 1). During follow-up, 123 069 patients (65%) remained persistently unexposed, while 40 803 (22%) experienced both periods of nonexposure and exposure to IBD treatments. Among exposed patients, 50 405 (27%) were exposed at least once to thiopurine monotherapy (mean exposure time, 17 months), 30 294 (16%) to anti-TNF monotherapy (19 months), and 14 229 (7.5%) to combination therapy (8 months). At the time of treatment initiation, 6% of patients initiating thiopurine monotherapy had been previously exposed to anti-TNF monotherapy (during 8 months on average) and 35% of patients initiating anti-TNF monotherapy had been previously exposed to thiopurine monotherapy (12 months). At the time of initiation of combination therapy, 50% of patients had been previously exposed to thiopurine monotherapy (7.5 months) and 43% to anti-TNF monotherapy (7 months). Overall, 336 cases of lymphoma occurred: 220 in unexposed patients (incidence rate [IR] per 1000 person-years, 0.26; 95% CI, 0.23-0.29), 70 in patients exposed to thiopurine monotherapy (IR, 0.54; 95% CI, 0.41-0.67), 32 in patients exposed to anti-TNF monotherapy (IR, 0.41; 95% CI, 0.27-0.55), and 14 in patients exposed to combination therapy (IR, 0.95; 95% CI, 0.45-1.45) (Table 2). Lymphoma occurred predominantly in men (57%) at a median age of 60 years. Fifty four percent of patients with lymphoma had Crohn disease, and the median IBD duration at the time of lymphoma diagnosis was 8.5 years. The most common lymphoma subtypes were nonfollicular lymphoma (n = 130, 39%), including 83 patients with diffuse large B-cell lymphoma; Hodgkin lymphoma (n = 55, 16%); and follicular lymphoma (n = 41, 12%) (eTable 2 in the Supplement). Hodgkin lymphoma accounted for 14% of all lymphomas among unexposed patients, 19% among patients exposed to thiopurines, 19% among patients exposed to anti-TNF agents, and 43% among patients exposed to combination therapy (eTable 2 in the Supplement). Quiz Ref IDAs compared with unexposed patients, the incidence of lymphoma was higher among patients exposed to thiopurine monotherapy, anti-TNF monotherapy, or combination therapy, with absolute risk differences of 0.28, 0.15, and 0.69 per 1000 person-years, respectively (eFigure in the Supplement). In multivariable analysis, compared with unexposed patients, the risk of lymphoma was higher among those exposed to thiopurine monotherapy (aHR, 2.60; 95% CI, 1.96-3.44; P < .001), anti-TNF monotherapy (aHR, 2.41; 95% CI, 1.60-3.64; P < .001), or combination therapy (aHR, 6.11; 95% CI, 3.46-10.8; P < .001) (Table 3); the interaction across treatment groups was not statistically significant (P = .94). These increased risks associated with thiopurine monotherapy, anti-TNF monotherapy, and combination therapy were observed both for non-Hodgkin lymphoma and Hodgkin lymphoma (Table 3), and corresponding HRs were of similar magnitude when the analysis was restricted to the 61 341 patients with incident IBD (Table 3). Results were consistent regardless of age, sex, and IBD type (eTable 3 in the Supplement). Increasing age, male sex, Crohn disease diagnosis, and history of smoking-related conditions were also independently associated with a higher risk of lymphoma (eTable 4 in the Supplement). The risk of lymphoma associated with anti-TNF agents remains unclear.24 Monotherapy with anti-TNF agents was not associated with an increased risk of malignancy, including lymphoma, in meta-analyses of randomized clinical trials25,26 nor in a nationwide Danish cohort study.10 In contrast, a study based on the Kaiser Permanente database reported that the use of anti-TNF agents was associated with an increased risk of lymphoma.12 However, most patients in the study had been previously exposed to thiopurines, therefore, no clear conclusion could be drawn. In the present study, restricting the analysis to the first line of IBD treatment, and thus considering only sequences of anti-TNF agents occurring in the absence of previous exposure to thiopurines, did not modify the HR of lymphoma associated with anti-TNF agents, thus providing further evidence for an association of anti-TNF agents with an increased risk of lymphoma. In addition, results were consistent for adalimumab and infliximab. Because anti-TNF agents impair the function of NK cells and negatively affect antilymphoma activity,27 there is a biological plausibility for the association of anti-TNF agents with an increased risk of lymphoma. The higher risk of lymphoma found in patients exposed to combination therapy compared with either thiopurine or anti-TNF monotherapy suggests that the increased risks associated with each of these treatments used alone may accumulate when they are combined. Additional evidence for such a cumulative phenomenon was provided in the analysis considering nonexposed patients as the comparison group: the value of the HR estimate for combination therapy (ie, 6.11), which was very close to the product of the corresponding HR estimates for thiopurine monotherapy and anti-TNF monotherapy (2.60 × 2.41, ie, 6.27), and the absence of interaction between thiopurines and anti-TNF agents, which suggests that the association of each of these treatments with the risk of lymphoma remains of the same magnitude whether they are used alone or in combination. Of note, combination therapy was generally initiated in patients previously exposed to thiopurine or anti-TNF monotherapy, suggesting that the increased risk found in patients with combination therapy may result from past exposure rather than from the combination therapy itself. However, results of the various sensitivity analyses intended to distinguish between current and past exposures were largely similar to those of the main analysis. There was one exception in the sensitivity analysis restricted to the first line of IBD treatment, which did not show an increased risk of lymphoma in patients exposed to combination therapy, although this analysis was limited because it was based on only 2524 person-years of follow-up and a single case of lymphoma. This study had several strengths. First, the database is comprehensive in that it includes all medical prescriptions and hospital stays for IBD in France. Second, by examining a national database of patients with IBD over a 5.5-year period, this study included a large number of patients treated with anti-TNF therapy, either alone or in combination with thiopurines. This allowed study of the association of each drug individually and the combination of the drugs with lymphoma incidence. A total of 336 patients with lymphoma was identified, including 70 in patients exposed to thiopurines. By comparison, a meta-analysis included 93 patients with lymphoma, including 24 exposed to thiopurines.23. This study had several limitations. First, including patients with prevalent IBD at cohort entry, who had possibly been previously exposed to IBD treatment but had no history of cancer, may have selected the most resistant individuals and thus resulted in an underestimation of the association between exposure to IBD treatments and the risk of lymphoma. However, analysis restricted to patients with incident IBD provided similar results, suggesting that such a selection bias, if any, was limited. Second, there was no clinical validation of the SNIIRAM and PMSI data for the diagnosis of IBD and lymphoma. However, IBD incidence rates in this cohort were similar to those reported in previous studies.14 In addition, the identification of lymphoma cases included a combination of criteria based on diagnoses and indicators of therapeutic management recorded in the SNIIRAM and PMSI databases, and results were consistent when using an alternative definition of lymphoma. Misclassification of lymphoma subtype between Hodgkin and non-Hodgkin lymphoma could not be ruled out, particularly for Hodgkin-like disease wrongly encoded as Hodgkin lymphoma.28. In a phase 2, single-group trial including 104 patients with relapsed follicular B-cell non-Hodgkin lymphoma, 59% had complete or partial tumor shrinkage for a median of 12.2 months following treatment with copanlisib. Common adverse events include hyperglycemia, hypertension, diarrhea, decreased strength and energy, nausea, lower respiratory tract infections, and thrombocytopenia. Serious adverse effects include infections, noninfectious pneumonitis, neutropenia, and severe skin reactions. Women who are pregnant or breastfeeding shouldn’t take the drug because it could harm a fetus or newborn. Further trials that the FDA requires to confirm clinical benefit are under way. There are also a variety of efforts going on outside of Congress that contain many similar elements. I have been involved with 2 such efforts. One is the Health Reform Roundtable, a project of the Convergence Center for Policy Resolution, which released a statement in early August on general next steps. These were supported by 9 policy analysts who have worked in both Republican and Democratic administrations and with members of Congress from both parties. With growing knowledge of cancer genetics and mutated proteins expressed by tumor cells, various immunotherapies have been approved to treat individuals with malignancies of the skin, lung, bladder, kidney, and other sites. However, significant and lasting treatment responses tend to occur in only a subset of patients. To fully harness the potential of the body’s immune system to fight cancer, scientists must overcome major hurdles involving tumor escape. New research may bring them closer to this goal, with the help of a “2 cell type” gene-editing approach that examines how genetic mutations in one cell can affect its interaction with a different type of cell. The researchers also verified the clinical relevance of their results by confirming that these genes are altered in patient tumors and affect T-cell responses clinically. Restifo explained that they quantified the T-cell response against patient tumors and looked for correlations between this measure of cytolytic T-cell activity and the activity of the genes on their list in more than 11 000 patient tumors, using data from the Cancer Genome Atlas. “Our analysis revealed 19 genes whose activity correlated with cytolytic activity in most of the 36 cancer types represented in the atlas,” he said. They also found that mutations in many of these genes occurred in multiple types of cancers. Geriatrics was not spontaneously recognized as a special branch of medicine; like pediatrics, it met with slow response. When Abraham Jacobi gave the first course in pediatrics in 1859, it was coolly received; the medical profession had to wait thirty-seven years for a textbook on diseases of infancy and childhood. The word geriatrics was coined by Nascher in 1909, and five years later he completed a textbook on diseases of old age, which went through a second edition in 1916. It was then that Nascher decided to enter the Public Welfare Department of New York City and suggested that I carry on; with his help and advice, this seemed possible. In 1940 there were about thirteen million persons past 60 in the United States, over nine million of whom were over 65. Few among them are self supporting, and many are dependent on small pensions. Most have surrendered and believe that they can no longer pull their own weight. Now some hope that Congress will see fit to grant monthly pensions of $30 to each person past 60, but such an allowance would hardly allow for private medical care if needed. The economic plight of the aged is likely to grow worse.... Geriatrics may loom large, since social and economic factors may force the medical profession to devote as much time to the aged as to the very young. Undoubtedly there will soon be courses in geriatrics in medical schools, and medical societies will be founded to further the study of diseases of old age. There will be more hospitals and clinics devoted to the care of the aged. Physicians will focus on preventive geriatrics (gerontology) dealing with senescence (normal old age) and on senility (abnormal old age). A physician genuinely interested in geriatrics does not necessarily work in hospitals and clinics. Some physicians in rural areas are doing scientific work in their own offices which they use as clinics and laboratories. Their necropsies are often painstakingly done and as accurate as those made in hospitals. Necropsies are important contributions to the advancement of geriatrics. Much is learned at the bedside. For every seriously ill old person who consults a physician there are a hundred with simple ailments causing discomfort which may precede actual disease.... A physician interested in geriatrics might have a clinic in his office and give the underprivileged the benefit of low fees, thus keeping them off the charity list. Old people often feel that they get more attention in a physician's private office than in an institution. If they are ill at home, the geriatrician could visit them for a moderate fee. If physicians adjust their fees to the circumstances of the patient, they will be amply repaid in knowledge gained. Carefully taken case histories and cross indexes are constructive. No specialty in medicine can be approached as easily as geriatrics. But, to be successful, a geriatrician must be genuinely interested in the elderly and willing to study his patients thoroughly. He must not overlook pathologic changes in the aged and must resist the temptation to restore the old to a state normal to maturity. They must function at their own level. An understanding of the senescent organism is necessary to treat elderly patients with some degree of success. Overenthusiastic treatment must be avoided. The usual result of overtreatment is a high mortality rate.... Periodically discuss your medication regimens with your doctor. Ask if there are any medications you can do without. Some diseases or symptoms might worsen your health and quality of life, so treating them with appropriate medications is a priority. Other diseases or symptoms might have a limited effect on your health and quality of life; therefore, some medications used in their treatment might be avoided. Getting family and caregivers involved is useful, especially if you have memory problems. Ask your doctor if there are any medicines you no longer need and if lifestyle changes (for example, diet, physical activity, and weight loss) may help you discontinue some of your medications. Sources: Steinman MA. Polypharmacy—time to get beyond numbers. JAMA Intern Med. 2016;176(4):482-483. Kok RM, Reynolds CF III. Management of depression in older adults: a review. JAMA. 2017;317(20):2114-2122. Lipska KJ, Krumholz H, Soones T, et al. Polypharmacy in the aging patient. JAMA. 2016;315(10):1034-1045. In Reply Dr Desai and colleagues raise concerns about our study in which we investigated the association between use of alendronate and hip fracture risk in older patients taking prednisolone. They question whether early adverse effects with alendronate would not have been captured. It is correct that only patients with at least 3 months of treatment with alendronate were included. This cutoff was chosen because a shorter treatment time would mean that patients had retrieved only 1 prescription, thus making it unclear whether they had actually started treatment, and because alendronate would be very unlikely to affect fracture risk with less than 3 months of treatment.1 As a result, adverse effects leading to immediate treatment discontinuation would not be captured using this study design. To study such adverse effects, patients starting and discontinuing alendronate after 1 prescription would have to be selected and matched to nontreated patients with similar comorbidity and other important risk factors and characteristics, which we believe could serve as the basis of another important but completely different study. Desai and coauthors are also concerned that our Cox proportional hazards model started at the time of entry in the Senior Alert register and not at the time of alendronate initiation, which precludes the possibility of investigating the association between alendronate treatment time and risk of hip fracture during the course of alendronate treatment. However, that was not the main objective. We investigated the association between prevalent alendronate treatment and incident risk of hip fracture (after inclusion in the Senior Alert register). We also investigated if there was an association between prevalent alendronate treatment time and hip fracture risk as one of several sensitivity analyses, including analyses using all unmatched controls, of medication possession ratio of acetylsalicylic acid, by prednisolone dose, and by sex. Longer duration of prevalent alendronate treatment was associated with lower risk of incident hip fracture, indicating a dose-response relationship for the observed association. All sensitivity analyses were consistent and supported our main finding of an association between alendronate use and risk of hip fracture in older patients taking prednisolone. Desai and colleagues argue that residual confounding by unmeasured frailty could have influenced the selection of patients who received treatment and therefore affect the observed associations. We agree that such factors could exist and are always a concern with observational studies. However, extensive measures were taken to minimize the risk of bias. For example, we used propensity score matching2 to identify similar control patients, with matching based on more than 40 variables (including age, sex, anthropometrics, comorbidity, and several risk factors for falls and fracture), and models were adjusted for possible confounders. There was no healthy adherer effect when investigating adherence to acetylsalicylic acid, and there was no association found between alendronate treatment and fall injury without fracture, which would be expected if patients who were using alendronate were healthier than those who were not. First, optimizing the management of chronic disease will be difficult when patient and clinical goals are unaligned. An impoverished patient with uncontrolled diabetes may prioritize rent over dietary changes. A patient who is alcoholic may be disinterested in stopping drinking because he or she values social interactions. Patients’ social needs can undermine self-management of chronic disease. Personal care planning is a tool to understand what really matters to a patient because these social determinants of health drive self-management. Clinicians will need to share power with patients by incorporating each patient’s knowledge of their social circumstances with the health care professional’s clinical expertise. This will yield not only a therapeutic alliance but can improve adherence and clinical outcomes, thereby optimizing disease management.3. Second, patient goals are often secondary to clinical and organizational objectives. Devoting time in an encounter to know the patient as a person, with unique social circumstances, is central to developing a personalized care plan. Yet clinicians are incentivized to complete specific tasks in a clinical encounter (eg, hypertension control). They are rarely rewarded for spending time talking about a patient’s social needs. Furthermore, clinicians who do identify social risks are often left in a siloed system with ad hoc resources, disconnected from the community.4. Network geometry for 54 randomized clinical trials (41 596 patients). Each treatment node indicates an intervention and is weighted according to the number of patients who received the particular intervention. Each edge (line connecting the nodes) is weighted according to the number of studies and directly compares the treatments it connects. See Table 1 for expansions of treatment abbreviations. The coding guide, which provides a description of each intervention component, can be found in eTable 1 of the Supplement. Network geometry for 158 randomized clinical trials (107 300 patients). Each treatment node indicates an intervention and is weighted according to the number of patients who received the particular intervention. Each edge (line connecting the nodes) is weighted according to the number of studies and directly compares the treatments it connects. See Table 1 for expansions of treatment abbreviations. The coding guide, which provides a description of each intervention component, can be found in eTable 1 of the Supplement. A, Network geometry for  68 randomized clinical trials (86 491 patients). B, Network geometry for 39 randomized clinical trials (52 281 patients). Each treatment node indicates an intervention and is weighted according to the number of patients who received the particular intervention. Each edge (line connecting the nodes) is weighted according to the number of studies and directly compares the treatments it connects. See Table 1 for expansions of treatment abbreviations. The coding guide, which provides a description of each intervention component, can be found in eTable 1 of the Supplement. Previous randomized clinical trials (RCTs) and systematic reviews have selectively examined fall-prevention programs.10-13 However, directly comparing more than 2 interventions using conventional meta-analysis has major limitations. The key elements of an effective fall-prevention program remain unclear, which has hampered implementation of effective interventions. Furthermore, a network meta-analysis ranking all available fall-prevention interventions and their combinations has not been conducted. Quiz Ref IDTherefore, a systematic review and network meta-analysis on all available fall-prevention interventions for older people were conducted. MEDLINE, Embase, Cochrane Central Register of Controlled Trials, and Ageline databases were searched from inception until December 1, 2015 (see protocol for search strategy15). Reference lists of included RCTs and relevant reviews were scanned for additional RCTs. Study authors were contacted for unpublished studies or additional data. An updated search was conducted on April 19, 2017, which involved screening, abstraction, and risk-of-bias appraisal by 2 reviewers, working independently without additional reference scanning or author contact regarding conference abstracts, trial protocols, or non-English articles for any studies identified in the update. Data abstraction was completed by independent pairs of reviewers after pilot-testing of the data abstraction form. Conflicts were resolved by a third reviewer. Interventions were coded independently by a clinician (S.E.S.) and a methodologist (A.C.T.) using a preestablished coding guide (eTable 1 in the Supplement). Included interventions were classified into the following broad categories: basic falls risk assessment, calcium supplementation, cognitive behavioral therapy, devices, diet modification, electromagnetic field therapy and whole-body vibration, environmental assessment and modification, exercise, floor modifications, multifactorial assessment and treatment, osteoporosis medications, podiatry assessment and treatment, quality improvement strategies, social engagement, surgery, vision assessment and treatment, and vitamin D supplementation. Quality improvement strategies were focused on increasing use of research in practice and were classified at the health system, clinic and clinician, and patient levels (Box 1 and Table 1).20. Across all outcomes, pairwise random-effects meta-analysis was conducted. Effect estimates are reported as odds ratios (ORs) for dichotomous outcomes and mean differences for continuous outcomes. Studies reporting dichotomous outcomes with zero events across all groups were included in the systematic review, but excluded from analysis. Studies reporting continuous outcomes with the average effect estimate but not reporting the associated measure of variance (eg, standard deviation) were included in the analysis, with standard errors imputed when feasible.23,24. Random-effects network meta-analyses were conducted for connected networks of included RCTs when more than 10 RCTs were available and the number of RCTs was greater than the number of intervention nodes. To surmount small study effects,25 smaller trials (n <100 participants) were excluded from analysis. Across all outcomes for which network meta-analysis was possible, the transitivity and consistency assumptions were assessed a priori.26-28 In both network meta-analysis and inconsistency models, common within-network, between-study variance across intervention comparisons was assumed because the treatments included in each network of trials were mostly nonpharmacological. Subgroup and sensitivity network meta-analyses were conducted for the primary outcome with consideration of potential treatment effect modifiers. Interventions were ranked using P scores29 and presented in a rank-heat plot.30 For each network meta-analysis, the overall risk for the control group (considering usual care as the control) of the included studies was calculated via the variance-stabilizing Freeman-Tukey double arcsine approach.31 A random-effects pairwise meta-analysis was applied using inverse variance weights, and to facilitate interpretation, summary group risks were back-transformed to the initial scale. All network meta-analyses and consistency assessments were conducted with R software (version 3.3.3)32 using the netmeta package.33 Results were summarized using effect estimates (ORs or mean differences) and their associated 95% CIs. Overall ORs, derived from each network meta-analysis, were transformed to risk differences to allow judgment of the clinical importance of statistically significant results.34 Analysis details are provided in eMethods in the Supplement. For 155 of the RCTs (54.8%), the mean age of participants was between 74 and 84 years (Table 2; eTable 2 in the Supplement). In 248 of the RCTs (87.6%), at least 50% of the participants were women. One hundred sixty-nine RCTs (59.7%) included a mixture of individuals with and without a history of falls. The number of medications taken was not reported in 172 RCTs (60.8%). The design-by-treatment interaction model showed no evidence of significant inconsistency across the network meta-analysis (eTable 5 in the Supplement). Because of the large number of results from the analysis, the overall results for each outcome are presented and focus on statistically significant intervention effects relative to usual care for network meta-analyses (summarized in Box 2). The results from all statistically significant treatment comparisons are available in eTable 5 in the Supplement, and the results from all analyses are posted on the Open Science Framework.50 P scores were used to summarize results for the primary outcome, and all results are presented in eTable 6, eTable 7, and eTable 8 in the Supplement. The rank-heat plot, presented in eFigure 3 in the Supplement, indicates that exercise is likely the most effective intervention in terms of numbers of fallers, injurious falls, fractures, and hip fractures. Network meta-analysis for the primary outcome of injurious falls included 54 RCTs (41 596 participants) with 39 interventions plus usual care (Figure 2). The event rate for injurious falls in the usual care group was 0.34 (95% CI, 0.24 to 0.44). Across all 780 network meta-analysis comparisons, 101 (12.9%) were statistically significant (eTable 5 in the Supplement). The following 4 interventions were associated with a reduced risk of injurious falls relative to usual care:. The results of subgroup analyses are summarized in Table 4. For the 37 RCTs that had less than 75% women (20 354 participants), which examined 27 treatments plus usual care, the network meta-analysis results were consistent with the main analysis. For the 44 RCTs with duration of 12 months or less (32 890 participants; examined 28 interventions plus usual care), the network meta-analysis results were consistent with the main analysis for the interventions of exercise vs usual care and combined clinic-level quality improvement strategies, multifactorial assessment and treatment, calcium supplementation, and vitamin D supplementation vs usual care; the remaining 2 significant comparisons from the main analysis (combined exercise and vision assessment and treatment vs usual care; combined exercise, vision assessment and treatment, and environmental assessment and modification vs usual care) were no longer included in the network. Network meta-analysis was conducted for the 32 RCTs with participants who were younger than 80 years old (24 869 participants), which examined 26 interventions plus usual care. Compared with the main analysis, the same interventions were associated with a reduced risk of injurious falls except for the combination of clinic-level quality improvement strategies, multifactorial assessment and treatment, calcium supplementation, and vitamin D supplementation, which was no longer in the network. This finding was consistent in another network meta-analysis involving the 40 RCTs (37 010 participants) for people with a mixed fall history (ie, some had fallen previously and some had not), which examined 34 interventions plus usual care. A network meta-analysis restricted to 11 RCTs involving 3830 patients who had fallen previously and examining 9 interventions plus usual care found that the combination of clinic-level quality improvement strategies, multifactorial assessment and treatment, calcium supplementation, and vitamin D supplementation was associated with fewer injurious falls than usual care (OR, 0.12 [95% CI, 0.04 to 0.44]; ARD, −2.08 [95% CI, −3.34 to −0.83]), while the remaining 3 comparisons (exercise vs usual care; combined exercise and vision assessment and treatment vs usual care; combined exercise, vision assessment and treatment, and environmental assessment and modification vs usual care) were no longer in the network. Sensitivity analysis was conducted restricting the network meta-analysis to 24 RCTs with a low risk of contamination bias (26 969 participants; 19 interventions plus usual care); no intervention was associated with a lower risk of injurious falls compared with usual care (eg, exercise vs usual care, OR, 0.59 [95% CI, 0.29 to 1.18]; ARD, −0.53 [95% CI, −1.23 to 0.17]), and 3 of the significant comparisons from the main analysis were no longer in the network (combined exercise and vision assessment and treatment vs usual care; combined exercise, vision assessment and treatment, and environmental assessment and modification vs usual care; combined clinic-level quality improvement strategies, multifactorial assessment and treatment, calcium supplementation, and vitamin D supplementation vs usual care). Network meta-analysis for the outcome of number of fallers included 158 RCTs, 107 300 participants, and 77 interventions plus usual care (Figure 3). One RCT (0.6%) with 31 participants (0.03%) was excluded from the network meta-analysis because it had zero events across all groups. The event rate for falls in the usual care group was 0.38 (95% CI, 0.33 to 0.43). Across all 3003 network meta-analysis comparisons, 200 (6.7%) were statistically significant (eTable 5 in the Supplement). Of these, 5 interventions were associated with a lower risk of patients experiencing a fall relative to usual care:. Network meta-analysis for the outcome of fractures included 68 RCTs, 86 491 participants, and 43 interventions plus usual care (Figure 4). The event rate for fractures in the usual care group was 0.07 (95% CI, 0.05 to 0.10). Across all 946 network meta-analysis comparisons, 45 (4.8%) were statistically significant (eTable 5 in the Supplement). Of these, 1 intervention (combined osteoporosis treatment (eg, bisphosphonates), calcium supplementation, and vitamin D supplementation) was associated with a lower risk of fractures relative to usual care (OR, 0.22 [95% CI, 0.09 to 0.54]; ARD, −1.51 [95% CI, −2.41 to −0.62]). The remaining 42 interventions were not significantly associated with a lower risk of fractures than usual care. Network meta-analysis for the outcome of hip fractures included 39 RCTs, 52 281 participants, and 23 interventions plus usual care (Figure 4). Four RCTs (9.1%) with 1877 participants (3.5%) were excluded from the network meta-analysis because they had zero events across all groups. The event rate for hip fractures in the usual care group was 0.03 (95% CI, 0.02 to 0.04). Across all 276 network meta-analysis comparisons, 9 (3.3%) were statistically significant (eTable 5 in the Supplement). Of these, 1 intervention (combined osteoporosis treatment, calcium supplementation, and vitamin D supplementation) was associated with a lower risk of hip fracture relative to usual care (OR, 0.18 [95% CI, 0.05 to 0.62]; ARD, −1.70 [95% CI, −2.92 to −0.48]). An additional 22 interventions were not significantly associated with a lower risk of hip fractures than usual care. Fifty-seven of the RCTs (24 558 participants) reported no intervention-related harmful events in any study group, and another 62 RCTs (39 596 participants) reported 1 or more harms (eTable 11 in the Supplement). Only 2 pairwise meta-analyses were possible (eTable 9 in the Supplement). Exercise was not significantly associated with an increased risk of muscle soreness compared with usual care (OR, 4.97 [95% CI, 0.35 to 70.38]; ARD, 0.13 [95% CI, −0.02 to 0.70]; 2 RCTs; 1021 participants). Supplementation with calcium and vitamin D was not significantly associated with an increased risk of gastrointestinal harm compared with usual care (OR, 1.05 [95% CI, 0.52 to 2.09]; ARD, 0.001 [95% CI, −0.01 to 0.03]; 2 RCTs; 3853 participants). Quiz Ref IDExercise alone and various combinations of interventions were associated with lower risk of injurious falls compared with usual care. Choice of intervention may depend on patient and caregiver values and preferences. Combinations of interventions, including exercise, vision assessment and treatment, environmental assessment and modification, multifactorial assessment and treatment, and vitamin D supplementation were associated with preventing injurious falls compared with usual care. The combination of exercise and vision assessment and treatment was probably the intervention most strongly associated with reduction in injurious falls. These results suggest that encouraging patients to exercise, undergo a vision assessment, and consider osteoporosis therapy (for those at risk), given the potential impact of these interventions in preventing injurious falls. Other combinations of interventions to consider include exercise, patient-level and clinic-level quality improvement strategies, multifactorial assessment and treatment; exercise, patient-level quality improvement strategies, hip protectors, and environmental assessment and modification; and orthotics and exercise in patients at risk for falls. The results suggest focusing on implementing patient-level quality improvement strategies (eg, patient education and patient reminders) and clinic-level quality improvement interventions (eg, audit and feedback) to increase uptake of this evidence. The results also suggest that calcium and vitamin D supplementation may decrease fractures, as may osteoporosis therapy plus calcium and vitamin D supplementation. However, the results indicate the need for a tailored approach; subgroup analyses showed that the combination of exercise, environmental assessment and modification, and multifactorial assessment and treatment was associated with an increased risk of injurious falls among patients who had fallen previously. In addition, the combination of exercise, patient-level quality improvement strategies, and social engagement was associated with increased fall risk. Quiz Ref IDExercise may increase fall risk in some individuals because these people become more mobile as their strength increases; patients can be made aware of this situation, but any cautionary advice must be balanced with the need to improve mobility and avoid deconditioning. This finding also raises the issue of the type of exercise to recommend, such as exercise focused on enhancing balance.52 Health care managers might consider the use of clinic-level quality improvement strategies, such as clinician reminders and audit and feedback, to increase uptake of multifactorial assessment and treatment and of vitamin D supplementation to reduce injurious falls. Quiz Ref IDAs well, for patients in long-term care settings, hip protectors, environmental assessment and modification, exercise, and patient-level quality improvement strategies are potential options to reduce falls. This study had several limitations. The published protocol included both the falls rate and costs as secondary outcomes. However, because few studies reported these outcomes consistently, these outcomes could not be analyzed directly; rather, data for falls rates were converted to number of falls. As well, another outcome, quality of life, as measured by SF-12 or SF-36 summary component measures or EuroQol-5D, was added.17-19 Some of the planned subgroup analyses and sensitivity analyses were not conducted because of insufficient data. Although the point estimate was similar to the overall OR, the results were no longer statistically significant for the injurious falls network meta-analysis when only studies with a low risk of contamination bias were included. However, because most of the studies (67%) were assessed as having an unclear risk of contamination bias, the power of this sensitivity analysis was limited by the lower number of studies that could be included. This limitation suggests that improvements in reporting are required. Most network meta-analyses included numerous interventions, with sparse data for the treatment comparisons; additional analyses, using the models suggested by Welton et al56 and Caldwell and Welton57 to account for sparseness, could be conducted in the future. Scanning the reference lists of 32 additional studies from the updated search and inclusion of an unpublished conference abstract58 and a non-English paper59 were not possible. Because of the large number of comparisons in the network meta-analyses, multiplicity may have elevated the rate of false positives in the statistically significant results (type I error).60,61 Although P scores are based on the treatment effect estimates and their associated CIs, it is recommended that the P score values be interpreted along with the network meta-analysis point estimates and their precision.29. In Reply Dr Fix and colleagues highlight the important cultural challenges faced in the implementation of personalized care planning. Primary care is most commonly defined as first-contact care that is comprehensive, long-term, person-focused, and coordinated, all within the context of family and community.1 As such, contextualizing decisions about health care treatments to individual patient’s needs and circumstances is one of the core roles of a primary care clinician. However, we agree that this person-centered ideal is often not achieved in practice. Current measurement and reward systems in primary care often focus on disease-specific processes that may not be important to patients,2 and electronic health records in their current form do not accommodate patient perspectives.3 As such, tools and strategies to help primary care become more person-centered are desperately needed. We believe that personalized care planning and care plans, as mandated by the Centers for Medicare & Medicaid Services for Chronic Care Management (CCM) payment, is a potential step toward integrating more person-centered approaches into the wide range of primary care practices that serve traditional Medicare beneficiaries. The current regulation builds on the language and infrastructure of care management, a multidisciplinary strategy to improve coordination of care for people with multiple chronic conditions that has grown in recent years in primary care.4 The CCM fee also provides additional resources for practices to invest in incorporating approaches like personalized care planning. Uptake of CCM, however, has been low,5 and care plans as currently implemented remain variably defined and poorly understood and frequently lack patient input.6 Hence, as Fix and colleagues note, a more fundamental transformation may be necessary for personalized care planning to reach its full potential. We agree that cultural change is a major factor in achieving the promise of both personalized care planning and person-centered primary care. To the Editor The JAMA Insights article on penicillin allergy1 stated that many children and adults are erroneously labeled as penicillin allergic. However, the title could falsely suggest that true IgE-mediated penicillin hypersensitivity in some patients represents a temporary phenomenon. To our knowledge, there are no robust data that endorse this statement, and the referenced study by Solensky et al2 was incorrectly discussed. In the study by Solensky et al, there is no irrefutable evidence for skin test conversion from positive to negative, as patients were enrolled solely based on clinical history. Patients with positive skin test responses were excluded from further investigation. The authors discussed “resensitization,” but this would require unequivocal proof of previous sensitization (ie, a positive skin test, specific IgE response, or both). Moreover, this limitation was acknowledged by the authors who correctly stated that patients with IgE-mediated allergy to penicillin should be prospectively monitored and the effect of repeated penicillin exposure on their immunological and clinical reactivity evaluated once the skin test, IgE level, or both become negative. A study including patients in whom skin tests and IgE levels were evaluated at different time points3 showed that after a positive challenge, skin tests can convert from positive to negative or from negative to positive within only 4 weeks. In the absence of robust longitudinal data indicating the transient character of IgE-mediated penicillin allergy, caution should be exercised before stating that IgE-mediated penicillin allergy is not necessarily forever. We recommend thorough evaluation of individual patients to avert diagnostic error with potentially serious consequences. Dr Sabato and colleagues argue for “thorough evaluation of individual patients.” Penicillin skin testing followed by oral challenge in the history-positive, skin test–negative patient is a safe and effective mechanism to remove the diagnosis of IgE-mediated penicillin allergy in more than 90% of those tested. This approach is the current guideline-based standard of care. Particularly if partnered with antibiotic stewardship programs, this approach could improve antibiotic appropriateness and reduce the risk of adverse public health outcomes such as antibiotic resistance and Clostridium difficile infection. However, patients with a history consistent with an IgE-mediated reaction and a reaction within the last 12 months define a higher-risk population for a positive skin test, positive oral challenge reaction, or both.2,3 Evidence exists for both penicillins and cephalosporins showing that among patients with initial positive skin tests, skin test reactivity is lost over time. These studies, however, are limited by the lack of oral challenge data both at at baseline—at the time of positive skin testing and at follow-up when the skin test has become negative.4,5 Overall, less than 1% of the population will be advised to avoid penicillin based on both a history consistent with an IgE-mediated reaction and positive penicillin skin testing. We agree that in this select population, long-term studies are warranted to define the safest clinical approaches, as well as the mechanisms and host and ecologic factors that drive sensitization and future loss of reactivity. Internet searches (keywords: CBD, cannabidiol, oil, tincture, vape) were performed between September 12, 2016, and October 15, 2016, to identify CBD products available for online retail purchase that included CBD content on packaging. Products with identical formulation as another product under the same brand were excluded. All unique CBD extracts that met these criteria were purchased. Products were stored according to packaging instructions, or if none were provided, in a cool, dry space. Within 2 weeks of receipt, product labels were replaced with blinded study identifiers and sent to the laboratories at Botanacor Services for analysis of cannabinoid content (cannabidiol, cannabidiolic acid, cannabigerol, cannabinol, Δ-9-tetrahydrocannabinol, Δ-9-tetrahydrocannabibolic acid [THC]) using high-performance liquid chromatography (in triplicate; lower limit of quantification, ≤0.3170% wt/wt). A 10-point method validation procedure was used to determine the appropriate sample preparation and analytical method. Triplicate test results were averaged and reported by product weight. Data were analyzed using SPSS Statistics (IBM), version 23, with descriptive analyses and a 2-tailed χ2 (α <.05). Consistent with other herbal products in the US Pharmacopeia and emerging standards from medicinal cannabis industry leaders, a ±10% allowable variance was used for product labeling (ie, accurately labeled = 90%-110% labeled value, underlabeled >110% labeled value, and overlabeled <90% labeled value). Eighty-four products were purchased and analyzed (from 31 companies). Observed CBD concentration ranged between 0.10 mg/mL and 655.27 mg/mL (median, 9.45 mg/mL). Median labeled concentration was 15.00 mg/mL (range, 1.33-800.00). With respect to CBD, 42.85% (95% CI, 32.82%-53.53%) of products were underlabeled (n = 36), 26.19% (95% CI, 17.98%-36.48%) were overlabeled (n = 22), and 30.95% (95% CI, 22.08%-41.49%) were accurately labeled (n = 26) (Table 1). Accuracy of labeling depended on product type [χ2(1) = 16.75; P = .002], with vaporization liquid most frequently mislabeled (21 mislabeled products; 87.50% [95% CI, 69.00%-95.66%]) and oil most frequently labeled accurately (18 accurately labeled products; 45.00% [95% CI, 30.71%-60.17%]). Concentration of unlabeled cannabinoids was generally low (Table 2); however, THC was detected (up to 6.43 mg/mL) in 18 of the 84 samples tested (21.43% [95% CI, 14.01%-31.35%]), cannabidiolic acid (up to 55.73 mg/mL) in 13 of the 84 samples tested (15.48% [95% CI, 9.28%-24.70%]), and cannabigerol (up to 4.67 mg/mL) in 2 of the 84 samples tested (2.38% [95% CI, 0.65%-8.27%]). Among CBD products purchased online, a wide range of CBD concentrations was found, consistent with the lack of an accepted dose. Of tested products, 26% contained less CBD than labeled, which could negate any potential clinical response. The overlabeling of CBD products in this study is similar in magnitude to levels that triggered warning letters to 14 businesses in 2015-2016 from the US Food and Drug Administration3 (eg, actual CBD content was negligible or less than 1% of the labeled content), suggesting that there is a continued need for federal and state regulatory agencies to take steps to ensure label accuracy of these consumer products. Underlabeling is less concerning as CBD appears to neither have abuse liability nor serious adverse consequences at high doses4,5; however, the THC content observed may be sufficient to produce intoxication or impairment, especially among children.6 Although the exclusive procurement of products online is a study limitation given the frequently changing online marketplace, these products represent the most readily available to US consumers. Additional monitoring should be conducted to determine changes in this marketplace over time and to compare internet products with those sold in dispensaries. These findings highlight the need for manufacturing and testing standards, and oversight of medicinal cannabis products. A primary outcome was not defined in 23 protocols (20%). Discrepancies were found in at least 1 primary outcome defined in the registry for 16 of 69 prospectively registered trials (23%) when compared with the protocol, whereas 9 of 58 published trials (16%) with defined primary outcomes had discrepancies between the publication and the protocol (Table 2). Discrepancies between the protocol and publication were more common in unregistered trials (6 of 11 trials [55%]) than registered trials (3 of 47 [6%]) (P < .001). Only 1 published article acknowledged the changes to primary outcomes. The availability of fracking technology has allowed the US petroleum industry to markedly increase domestic production of natural gas and crude oil over the past decade.1 More than half of all domestic oil and natural gas now comes from wells that have been fracked. At least 10 states overlie shales that are currently being leveraged to produce oil and natural gas through fracking. Pennsylvania, New York, Ohio, Maryland, Virginia, West Virginia, and Kentucky overlie the Marcellus shale along the western edge of the Allegheny Mountains and the Appalachian Mountains. Along the eastern edge of the Rocky Mountains, North Dakota and Montana overlie the Bakken shale. Texas overlies the Barnett shale in the north and the Eagle Ford formation in the south. In the Williston Basin of North Dakota, the Three Forks formation, located beneath the Bakken shale, extends the potential reach of this huge resource to Wyoming and South Dakota as well (Figure). More than 100 000 domestic wells have been fracked in the last decade. By 2014, nearly 30 000 new domestic wells were being drilled and fracked each year, and many existing wells were being fracked to optimize production.1 Even with a decline in the price of oil and gas in recent years, the United States currently drills and fracks approximately 20 000 new wells annually.1 Before a new well can begin production, each site undergoes months of preparation. Steps include preparation of the drilling pad, vertical drilling operations (often >1 mile deep), horizontal extension of the well (often >1 mile laterally), and fracking. Preparation of each well can therefore take months, and care must be taken to monitor local air quality and ground water quality. Each step has the potential to influence the health of local residents. Fracking fluid contains water, sand (silicates), and a proprietary mixture of chemicals that vary by company and site. Silicates are added as a proppant to keep fractures in the shale open. Acids are added to solubilize some of the common minerals in the shale, and surfactants are added to aid in fracture penetration. Millions of gallons of this fluid are injected into each well at high pressures sufficient to fracture rock, and millions of gallons of wastewater (ie, flowback) return to the surface and contain heavy metals (eg, barium, manganese, and iron), radioactive materials (eg, radium), and organic compounds (eg, benzene, toluene, xylenes, oil, and grease).1 This flowback is then either reused (recycled for additional fracking), evaporated in surface pools, or transported and injected into deeper disposal wells. At present, as much as 95% of the wastewater generated by fracking is injected into disposal wells.5 Due to the potential toxicity of this wastewater, monitoring the depth and geological location of these disposal wells is important. Acids are routinely added to the fracking mix, and low pH mobilizes heavy metals from the rock into which the fracking mix is injected.6 Environmental heavy metals are nephrotoxic, and changes in renal function associated with human exposure to fracking flowback will need to be studied longitudinally. A recent series of intentional ingestions of fracking fluid7 associated with acute methanol intoxication highlights the need for studying the health effects of unintentional ingestion, such as may occur through the inadvertent contamination of drinking water. Heavy metals are also potentially neurotoxic. In the central nervous system, manganese has a high affinity for the basal ganglia where it increases risk of parkinsonism. In the peripheral nervous system, mercury has a high affinity for the dorsal root ganglia, and elevated blood mercury levels have been documented in patients with idiopathic neuropathy.8 Because methyl mercury is a more potent neurotoxin than inorganic mercury, the effect of fracking activity on biodiversity and mercury organification (conversion to methyl mercury by aquatic microorganisms) is being quantified in watersheds overlying some large shale formations.9. Drug development in oncology has traditionally focused on therapies that directly target cancer cells—surgery, radiation therapy, chemotherapy, and kinase inhibitors. However, for more than a century it has also been recognized that the immune system interacts with cancer. The immune surveillance hypothesis, proposed in the 1950s, posited that the immune system could recognize and reject cancer cells as being foreign, in the same way that it reacts against microbial agents and transplanted organs. Initial attempts to harness the potential of cancer immunotherapy by directly activating antitumor immunity with cancer vaccines or recombinant cytokines, or by infusing tumor-specific immune cells prepared in the laboratory, had limited success. It was not until scientific research provided a greater understanding of interactions between cancer and the immune system that a quantum leap in immuno-oncology occurred. The immune system is continuously in a state of delicate balance between tolerating normal tissues (“self”) and attacking foreign substances, cells, and organisms. If this balance is perturbed, autoimmune reactions occur. Immunological tolerance is rooted in regulatory immune cell subsets, suppressive cytokines secreted by immune or other tissue-resident cells, and immune checkpoint pathways. Immune checkpoints are paired receptor-ligand molecules with interactions that dampen immune responses. These checkpoints become involved following immune activation, as a natural inhibitory feedback loop to reduce inflammation, thus avoiding collateral involvement of normal tissues. In the past 2 decades, scientific research has revealed that tumors can co-opt immune checkpoints to become shielded from immune elimination by aberrantly expressing the ligands that normally interact with inhibitory immune receptors to protect “self.”. The first immune checkpoint receptor that was discovered and characterized, cytotoxic T lymphocyte antigen 4 (CTLA-4), was in turn the first shown to be relevant to cancer therapy. In animal models, antibodies blocking CTLA-4 could mediate the regression of established tumors.1 This observation led to clinical testing of monoclonal antibodies blocking CTLA-4 in patients with cancer. The first approval from the US Food and Drug Administration (FDA) for an immune checkpoint blocker occurred in 2011, when ipilimumab was approved for advanced melanoma based on long-term overall survival in approximately 20% of patients. A major limitation of CTLA-4 blockade was the high rate of immune-based toxic effects, predicted by the lethal autoimmune phenotype of mice in which CTLA-4 was genetically deleted. Many of the adverse effects experienced by patients receiving anti–CTLA-4 therapy—rash, colitis, thyroiditis, and hepatitis among others—mimic autoimmune disorders and are consistent with the mechanism of action of checkpoint blockade.2. Promising results from a small first-in-human trial of the anti–PD-1 drug nivolumab led to a larger trial in 5 different cancer types. Durable tumor regressions in a substantial proportion of otherwise refractory melanomas, kidney cancers, and lung cancers were observed.4 Lung cancer, previously thought to be nonimmunogenic (incapable of provoking immune rejection), proved responsive to anti–PD-1. This finding immediately transformed the vision for potential applicability, and investigators launched clinical trials of multiple anti–PD-1 and anti–PD-L1 drugs in many cancer types. Six different drugs have entered clinical practice for various indications since the initial approval of an anti–PD-1 monoclonal antibody for advanced melanoma in late 2014. These indications include non–small cell lung carcinoma, Hodgkin lymphoma, and Merkel cell carcinoma; cancers of the kidney, bladder, and head and neck; and tumors with a genetic marker of high mutational burden termed microsatellite instability (MSI) (eTable in the Supplement). In contrast to traditional oncology drug approvals based on large randomized clinical trials measuring patient survival over years, several anti–PD-1 and anti–PD-L1 approvals used early surrogate end points from smaller trials, such as response rate or progression-free survival. Furthermore, the broad effects of blocking a single biological pathway in multiple cancer types changed the traditional approach of disease-based clinical trial design, in favor of “basket” trials incorporating several different cancers, built around biological principles. Thus, anti–PD-1 is a common denominator that has generated a modern approach to oncology drug development.5 The proportions of patients responding to therapy are variable depending on cancer type and treatment setting (first-line vs later-line therapy), ranging from approximately 15% to 65%. In addition, the extent of the response, as measured in progression-free survival and life expectancy, is also variable. The ease of administration of these agents in the outpatient clinic and the generally favorable safety profile have supported adoption of anti–PD-1 therapies in community oncology practices, while underscoring the need for enhanced physician education in this new method of treating cancer. Biomarkers are an important clinical tool for refining the risk-benefit profile of any drug.6 Reasoning that tumors devoid of PD-L1 ligand expression could not suppress antitumor immunity via the PD-1 pathway, and hence that PD-1 blockade would be irrelevant in PD-L1–negative tumors, an immunohistochemistry (IHC) assay was developed to query PD-L1 protein expression in pretreatment tumor biopsies. Initial results revealed a highly significant correlation between pretreatment tumor PD-L1 expression and the likelihood of complete or partial response to anti–PD-1 therapy as measured by standard radiologic evaluation.4 This spurred further studies in thousands of patients with different cancers receiving anti–PD-1 and anti–PD-L1 drugs in clinical trials. Although PD-L1 expression may be hardwired into cancer cells when driven by mutated signaling pathways or chromosomal instability, it more often reflects the tumor’s dynamic adaptive response to sensing a threat from antitumor T lymphocytes. This sensing, termed adaptive resistance, reflects the induction of PD-L1 on tumor cells by certain cytokines produced by activated T lymphocytes. Therefore, tumor biopsies offer a snapshot in time for PD-L1 expression, and clinical correlations are not absolute. However, in some cancer types—non–small cell lung carcinoma, bladder cancer, and melanoma—PD-L1 IHC has identified patients with a higher likelihood of treatment response, supporting evidence-based FDA approvals for 4 commercial tests linked to specific drugs and cancer types (eTable in the Supplement). Six different anti–PD-1 or anti–PD-L1 drugs are currently in standard use, in some cases with multiple drugs approved for the same disease indication, and more than 20 drugs are in clinical testing. This, along with the availability of 4 different commercial PD-L1 IHC assays, creates ambiguities in oncology decision making that are likely to become even more complex with the development of combination treatment regimens. Several hundred clinical trials are concurrently exploring combinations of anti–PD-1 and anti–PD-L1 with experimental immune modulators, such as other checkpoint monoclonal antibodies, cancer vaccines, epigenetic drugs, and metabolic drugs, and with the standard pillars of surgery, radiotherapy, and chemical therapies (chemotherapy and kinase inhibitors). Laboratory models predict that synergistic treatment combinations will be more effective than monotherapies. Two regimens meeting this benchmark have recently been approved by the FDA: ipilimumab plus nivolumab in advanced melanoma, and pembrolizumab plus chemotherapy in advanced non–small cell lung carcinoma. Additionally, anti–PD-1 drugs are in testing in earlier stages of cancer as adjuvant therapy, neoadjuvant (presurgical) therapy, or both, with tantalizing preliminary reports of activity. Building on the ability of anti–PD-1 to reactivate an immune response that can control or eliminate advanced malignant neoplasms in some patients, the ultimate goal of treating cancer effectively and durably is on the horizon. Population-based data will remain important for informing current understanding of health and disease, but the nature of genetic variation means that it can no longer be seen as sufficient. Most actionable genetic variation in individuals derives from extremely rare or even unique variants. The average individual will have about 50 genomic mutations not present in either of his or her parents, most of these variants will not change protein function, and about 200 protein-coding family-specific variants inherited from relatively recent ancestors that are not present in variant databases.3 Such variants are generally designated as a “variant of uncertain significance” (VUS). In a medical model that prioritizes and relies on knowledge derived from population-based studies, these variants might be considered variants of unknowable significance. High-throughput and computational in silico analyses can provide predictions regarding the likely significance of a novel variant. Some computational analyses use inferences from evolutionary conservation because protein domains that are conserved over time are less likely to tolerate variation. Other computational analyses evaluate structural features that are predicted to change RNA splicing or protein folding comparing new variants to data on variants with known outcomes. Some analyses combine multiple approaches. Functional studies of protein variants can provide some understanding of pathogenicity. Normal protein function strongly suggests a benign variation, whereas substantially altered protein function suggests pathogenicity. Family-specific variants affecting multiple, related individuals can be the subjects of family cosegregation studies, an epidemiologic approach to calculate the likelihood of a specific mutation being causative given family relationships. Even correlation with individual clinical presentation, phenotype, or both, may aid in variant classification, particularly in oncology. However, in almost all situations, correlation of information from multiple sources is considered necessary for classification that will alter medical care. While the importance of mechanistic and Bayesian reasoning are emphasized in guidelines for interpretation of sequence variants, the underlying framework for integrating knowledge derived from various quantitative and qualitative methodologies remains controversial and underexamined. As methodologies that rely on mechanistic inference become more reliable, clinical decision making will need to incorporate knowledge derived from a variety of sources to arrive at the best course of action for a particular patient. Clinicians will need to become more comfortable with the “untidy methodological pluralism”7 developing in medicine, in which relevant knowledge comes from a variety of sources and in which the value of that medical knowledge varies from patient to patient. Knowledge derived from populations remains informative for clinical decision-making but is no longer unconditionally preferred over knowledge derived from mechanistic understanding or methodologies. As long as EBM insists on a hierarchy of evidence emphasizing population-based knowledge, EBM and PM are not reconcilable.8 Precision medicine demands case-based reasoning, in which the relevant particulars of the individual patient must be elucidated and incorporated into clinical assessments and decisions.9. The cost of the YF vaccine varies widely around the world and is a complex issue involving supply, manufacturer’s market strategies, intermediate distributors, subsidies, insurance coverage, and clinical site policies. In countries where YF vaccine is subsidized by health authorities (either national, the United Nations, or Global Alliance for Vaccines and Immunization), and where emergency response is required to contain outbreaks, cost is significantly lower (or free for emergency public health response). In the United States, YF-VAX cost fluctuated depending on vaccine availability and whether it was supplied as a single dose or as a multidose (5-dose) vial. The cost to consumers also depends on each institution or facility’s administrative and personnel policies and administration fees. In high-income countries, the estimated cost of YF vaccine ranges from $40 to $250. In the United States, Stamaril is only supplied in single-dose vials at the previous cost of YF-VAX, and it is hoped that insurance companies cover Stamaril as they cover YF-VAX. The estimated cost of Stamaril is in the range of $150 to $250. In Canada, using fractional dosing of YF-VAX may be associated with lower cost for administration prior to upcoming travel (range, Can $45-$200) for the 1-year protection. However, recipients of fractional dosing will need revaccination for future travels. Yellow fever outbreaks will continue to emerge in the future and there is a substantial risk for further urban outbreaks given the widespread distribution of Ae aegypti. Depletion of the YF vaccine stockpile presented serious challenges for outbreak control, but it also created an opportunity for successful use of fractional doses of the vaccine. Separately, US manufacturing problems affecting the sole manufacturer of YF vaccine for North America has necessitated the implementation of an Expanded Access Program, using Stamaril under an IND protocol. The substantial decrease in the number of sites that can administer YF vaccine (YF-VAX or Stamaril) will pose inconveniences for travelers and travel clinics. The IND protocol will require more planning for travelers to identify accessible YF vaccination sites, longer time for clinic visits, and more stringent documentation and reporting of adverse events. Consequently, travelers who are contemplating travel to YF-endemic areas must schedule travel clinic visits early (>2 months before departure). Primary care clinicians need to be prepared to advise patients when travel plans become known, and refer their patients to YF vaccination centers early, given potential delays. The tibia is the most commonly broken major bone in the leg. Injuries require hospital admission, usually require surgery, and result in prolonged periods away from work and social activities. The best treatment for displaced, extra-articular fractures of the distal tibia (specifically the lower third of the tibia) remains controversial. These injuries are particularly difficult to manage due to the limited soft tissue coverage, poor vascularity of the area, and proximity of the fracture to the ankle joint. Infections, nonunion, and malunion are well-recognized complications of fractures of the distal tibia. Quiz Ref IDSurgical treatment options include intramedullary nail fixation, plate-and-screw fixation, and external fixation. External fixators may be beneficial in selected cases (such as intra-articular or segmental fractures) but the nail and plate options are most commonly used for extra-articular fractures. Mid-shaft fractures of the tibia are generally treated with locked intramedullary nails. However, in the more distal metaphyseal region of the tibia, the nail fixation may be less stable.1 The bolts or screws that are inserted into the nail may break,2 malalignment may occur,3 and there is a risk that the nail will penetrate into the ankle joint.4,5 The development of locking plates, for which a thread on the head of the screws locks into the holes in the plate to create a fixed-angle construct, has led to a recent increase in the use of plate fixation. However, plates are not without risks, they require greater soft tissue dissection, which carries a risk of infection, wound breakdown, and damage to the surrounding structures.6. The National Research Ethics Service approved the study, the approved protocol and statistical analysis plan are available in Supplement 1 and Supplement 2, respectively. The trial was overseen by independent steering and data and safety monitoring committees. This was a multicenter randomized clinical trial in 28 UK trauma hospitals. Young adults and adults 16 years or older (hereafter referred to as adults) with an acute, displaced, extra-articular fracture of the distal tibia were eligible to enter the trial if the treating surgeon believed the patient would benefit from surgical fixation of the fracture (Figure 2). A distal tibial fracture was defined as a fracture extending within 2 “Müller squares” of the ankle joint; Müller defined a square based upon the width of the patient’s own distal tibia, such that this measurement accounts for the different height of different patients.7 Patients were excluded if the fracture was open (the broken bone was exposed through a breach in the skin), the fracture extended into the ankle joint, there was a contraindication to intramedullary nail fixation (eg, a total knee replacement or previous deformity of the tibia), there was a contraindication to anesthesia, or the patient was unable to complete questionnaires. All patients gave written informed consent. Surgery was performed under general or regional anesthesia. All patients had thromboprophylaxis and antibiotic prophylaxis. All of the hospitals in the trial used both intramedullary nail and locking plate fixation as part of their routine clinical practice and all surgeons were familiar with both surgical interventions. Specialist trainees were allowed to operate under consultant supervision. For locking plate fixation, a locking plate is inserted at the distal end of the tibia and passed under the skin onto the surface of the bone (Figure 1B). Again, the details of the reduction technique, the surgical approach, the type and position of the plate, the number and configuration of fixed-angle screws, and any supplementary device or technique was left to the discretion of the surgeon. The only stipulation was that fixed-angle screws must be used in at least some of the distal screw holes—this is standard practice with all distal tibia locking plates. All patients received the same standardized, written physiotherapy advice detailing the exercises they needed to perform for rehabilitation following their injury; patients in both groups were advised to move their toes and ankle and knee joints fully within the limits of their comfort. Details of weight-bearing status were advised by the treating surgeon, but patients were encouraged to bear weight as soon as possible. Any other rehabilitation input beyond the written physiotherapy advice was left to the discretion of the treating clinicians. The primary outcome measure for this study was the Disability Rating Index (DRI)8 at 6 months, assuming a minimal clinically important difference of 8 points. The DRI is a validated questionnaire that is completed by the patient. It consists of 12 items specifically related to function of the lower limb, which are combined to provide an overall score from 0 (no disability) to 100 (complete disability). The secondary outcome measures were the EuroQol Health-Related Quality-of-Life 3-Level score (EQ-5D-3L9,10; health-state score range, −0.59 [worse possible health] to 1 [perfect health], death = 0), with a minimal clinically important difference of 0.07,11 and the Olerud-Molander Ankle Score (OMAS12) rating of pain and function at the ankle joint (score range, 0 [worst pain and function] to 100 [no pain and best function]). All serious adverse events occurring in the 12 months after surgery were recorded on a standardized complications form. These were categorized as “local complications related to the injury or its treatment,” “systemic complications,” and “unrelated adverse events.” Local complications included infection, vascular and neurological injury, venous thromboembolism, and malunion. Malunion was determined from standard anteroposterior and lateral radiographs of the tibia and fibula taken at 6 weeks and 12 months. Threshold values for malunion were determined by an independent trauma surgeon using OsiriX software (Pixmeo), version 7.5. These were coronal angulation of the tibia greater than 5°, sagittal angulation greater than 10°, and shortening greater than 10 mm. Any other surgery required in relation to the index fracture was recorded, including revision fixation, interventions for nonunion, and removal of symptomatic metalwork. A cost-effectiveness analysis is not reported in this article. The statistical analysis plan was approved by the independent data and safety monitoring committee. The primary outcome analysis investigated differences in the DRI at 6 months after randomization on an intention-to-treat basis, with early outcome analysis at 3 months and late outcome analysis at 12 months; per-protocol analyses were also reported. The primary (adjusted) analysis used a mixed-effects multivariable linear regression, with a random-effect term included to model potential associations within recruiting centers, and fixed effects for preinjury DRI score, age group (<50 years and ≥50 years), and sex (male and female). Unadjusted differences between treatment groups were assessed using a t test, based on a normal approximation for the DRI. Treatment effect estimates were presented with appropriate 95% CIs and all tests were 2-sided with a P value of less than .05 considered significant. No attempt was made to adjust for multiple testing; therefore, secondary analyses should be interpreted as exploratory. Analyses of secondary outcomes followed the same modeling approach as the primary outcome. Pairwise interaction terms between treatment group and both sex (male and female) and age group (<50 years and ≥50 years) were added to the mixed-effects models to test for important subgroup effects. Postoperative complications were compared between intervention groups using Fisher exact test. Where data were not available due to withdrawal of patients, lack of completion of individual data items, or loss to follow-up, the reasons for the missing data were ascertained and patterns assessed to judge plausibility of an assumption that data were missing at random. Sensitivity analyses were undertaken using multiple imputation techniques (imputation by chained equations15) to assess the effect of missing data on the primary analysis. To complement the preplanned analysis, a post hoc exploratory analysis using each of the DRI, OMAS, and EQ-5D-3L index scores at all 4 time points was conducted. This analysis combined observed longitudinal data collected at the 4 time points into a single value, the area under the curve (AUC), and facilitated comparisons of the AUCs between treatment groups. Mixed models were used to provide unbiased estimates of the AUCs, and CIs determined from standard errors using asymptotic normality, under the assumption of data being missing at random.16AUCs were estimated by fitting a repeated-measures mixed model, with the same fixed-effects structure as used in the primary analyses (adjusted for preinjury score, sex, and age group), but with a 3-level random-effects structure in which observations (time points) were nested within patients and patients were nested within center. All analyses were undertaken in Stata (StataCorp), version 14. Of 537 screened patients between April 2013 and April 2016, 321 were randomized (161 allocated to receive intramedullary nail fixation [nail group] and 160 allocated to receive locking plate fixation [plate group]). The mean age and sex of consenting patients was similar to those who did not take part in the trial; a t test comparing means of both groups showed no evidence of a difference in age (mean difference, −0.9 [95% CI, −4.1 to 2.3]) and a 2-sample test of proportions showed no evidence of a difference in sex distribution (P = .82). The 2 treatment groups of patients were well matched in both demographics and preinjury disability and ankle and quality-of-life scores (Table 1). The mean age of patients was 45 years, 62% (n = 197) were men, 91% (n = 293) of patients received their allocated treatment and a completion rate of at least 80% was achieved at each follow-up time point. Follow-up was completed in February 2017. In both groups of patients, the DRI score improved over the 12 months following randomization, but did not return to preinjury levels (Figure 3). Quiz Ref IDAt the primary outcome of 6 months, on an intention-to-treat basis, the mean DRI score was 29.8 in the nail group and 33.8 in the plate group. The adjusted difference was not statistically significant (4.0 [95% CI, −1.0 to 9.0; P = .11]). For the secondary analysis at 3 months, the mean DRI score was 44.2 in the nail group vs 52.6 in the plate group, with an adjusted difference of 8.8 (95% CI, 4.3 to 13.2; P < .001). For the analysis at 12 months, the mean DRI score was 23.1 in the nail group vs 24.0 in the plate group, with an adjusted difference of 1.9 (95% CI, −3.2 to 6.9; P = .47). The secondary outcome measures showed the same pattern at each time point. Figure 3 shows the change in outcome scores over time graphically and Table 2 shows the actual numbers. Quiz Ref IDThe adjusted estimate of the treatment effect for the OMAS at 6 months was −6.0 (95% CI, −11.2 to −0.7; P = .03) in favor of the nail group (mean score: nail group, 62.4 [SD, 23.1] vs plate group, 57.6 [SD, 24.9]). The adjusted difference of the EQ-5D-3L score at 6 months was −0.06 (95% CI, −0.12 to −0.01; P = .03) in favor of the nail group (mean score: nail group, 0.67 [SD, 0.27] vs plate group, 0.62 [SD, 0.28]). So, the secondary outcome measures were statistically significantly different at 3 and 6 months, but there was no significant difference at 12 months. The secondary per-protocol analysis, analyzing patients in groups according to the treatment received, showed the same pattern of results as the primary intention-to-treat analysis. At the primary end point of 6 months the mean DRI score was 30.2 in the nail group and 34.0 in the plate group. The adjusted difference was not statistically significant (3.4 [95% CI, −1.1 to 7.8]; P = .15). At 3 months, the mean DRI score was 44.6 in the nail group vs 51.9 in the plate group, with an adjusted difference of 7.5 (95% CI, 3.6 to 11.3; P < .001). For the analysis at 12 months, the mean DRI score was 22.9 in the nail group vs 24.1 in the plate group, with an adjusted difference of 0.9 (95% CI, −3.3 to 5.0; P = .68). At 6 months there was no significant interaction between age and treatment group. In the subgroup of patients younger than 50 years, the mean DRI score was 25.6 in the nail group vs 31.1 in the plate group (adjusted difference, 5.4 [95% CI, −1.0 to 11.8]). In the subgroup of patients 50 years and older, the mean DRI score was 35.9 in the nail group vs 38.2 in the plate group (adjusted difference, 2.0 [95% CI, −6.0 to 10.0]); P for interaction = .52. There was also no significant interaction between sex and treatment group (P = .38). There was a very low level of missing data in all patient-reported outcome measures at all time points; hence, the results of the imputation analysis presented in eTable 1 in Supplement 3 result in very similar estimates of treatment effects. Table 3 shows the postoperative complications at the 6-week assessment and further surgical interventions associated with the fracture in each group. Quiz Ref IDThere were no statistically significant differences in the rate of complications. Surgical debridement for deep infection was rare (1 of 161 patients in the nail group vs 5 of 160 patients in the plate group; P value for Fisher exact test = .12), but treatment with antibiotics for superficial wound healing complications was more common (14 of 157 patients in the nail group 9% [95% CI, 5% to 15%] vs 21 of 157 patients in the plate group 13% [95% CI, 8% to 20%]; P for Fisher exact test = .21). However, there were more secondary operations in the plate group vs the nail group. Of those patients allocated to the nail group, 34% (53/157) reported being fully weight bearing at the 6-week follow-up appointment compared with only 15% (23/157) of those allocated to the plate group (P < .001 for Fisher exact test; absolute rate difference, 19% [95% CI, 10% to 28%]). There were similar numbers of systemic complications in both groups of the trial, but there were numerically more reported unrelated serious adverse events over the duration of the trial among those allocated to the plate group (n = 80 events) than the nail group (n = 46 events). Repeated-measures, mixed-effects post hoc regression analysis was used to calculate marginal means, which provide estimates and 95% CIs of AUCs for each treatment group (Table 2). For those patients allocated to the nail group, the AUC was 351 (95% CI, 316 to 380) and, for those allocated to the plate group, the AUC was 407 (95% CI, 374 to 438). The between-group difference was 57 DRI units (95% CI, 12 to 103). A t test comparing the values between groups yielded a P value of .01, indicating a statistically significant difference in the AUC between treatment groups. This multicenter trial of patients with a fracture of the distal tibia found no significant difference between intramedullary nail fixation and locking plate fixation in disability rating at 6 months or 12 months. However, patients in the nail group reported less disability earlier in their rehabilitation at 3 months. Secondary outcomes showed a similar pattern with better ankle function and improved health-related quality of life at 3 months and 6 months in the patients with a nail fixation, but no evidence of a difference at 12 months. However, these secondary analyses should be interpreted as exploratory and require further research. A systematic review of the literature showed 6 other randomized clinical trials comparing nail with plate fixation for this injury. Two trials, involving 64 with nail fixation and 104 patients with plate fixation, found no difference in functional scores but showed a difference in wound complications, with more infections in the plate group.16-18 However, both these trials used traditional “nonlocking” plates rather than the newer fixed-angle locking devices. Modern locking plates are designed to be inserted using minimally invasive techniques, which may reduce the risk of wound complications. The third trial randomized 111 patients either to intermedullary nail fixation or locking plate fixation.19 The investigators found no difference in the time to union on radiographs but this was a single-center investigation and more than 20% of the patients in the trial were lost to follow-up. A fourth trial also compared intramedullary nail fixation with minimally invasive plate fixation.20 This trial did not find any difference in the Foot Function Index, but only 25 patients were included in total. The remaining 2 trials compared the results of external fixation (a different technique in which pins or wires are inserted into the bone and attached to a frame outside the skin) with both plate fixation and intramedullary nail fixation. The first of these trials concluded that “all achieved similar good functional results” but the different techniques may have different rates of complications.21 However, there were only 28 patients in each group in this trial. The final trial also concluded that all 3 methods were “efficient methods for treating distal tibia fractures” but that their complication profiles were different.22 This study was too small to draw firm conclusions with only 137 patients distributed across all 3 groups. In summary, previous trials have indicated that different forms of fixation for patients with a fracture of the distal tibia may have different complication profiles but failed to show a difference in functional outcomes. However, each of these trials was considerably smaller than the FixDT trial and so may not have been large enough to detect a clinically important difference between intramedullary nail fixation and locking plate fixation. This study has several limitations. Two hundred sixteen potentially eligible patients were not randomized, which could pose a risk to the external validity (generalizability) of the trial. However, most of these patients were excluded for logistical reasons (eg, no research staff available at the weekend), good clinical reasons (eg, declined any surgery), or the patient declined the trial because they “did not want to be part of a research study” or “fill out questionnaires.” Of more potential concern were patients who were not included in the trial because of a preference for one treatment or the other. This did not seem to be common among the patients. However, there was a stronger treatment preference among the surgeons. This does suggest that there was a preconceived preference for nail fixation, at least among surgeons in some centers. However, if patients deemed “suitable only for a nail” were excluded, despite being eligible according to the trial criteria, this is only likely to have reduced the relative benefit of nail fixation compared with locking plate fixation. Other possible limitations included postrandomization crossover of patients from one group to the other. However, these were less than anticipated. Ninety one percent of patients received the treatment to which they had been allocated. Similarly, some loss to follow-up was expected. However, the sample size was inflated to account for loss to follow-up of 20%, and more than 80% of patients provided primary outcome scores at every time point. Information on all singleton hospital births at 20 to 45 weeks’ gestation in Washington State between January 1, 2004, and December 31, 2013, was obtained from 2 linked data sources: the live birth and fetal death certificates database and the hospitalization database (Comprehensive Hospital Abstract Reporting System [CHARS]). Information obtained from the birth and fetal death certificates was abstracted by trained abstractors using standardized forms (Supplement 1) and included infants’ sex and maternal characteristics such as prepregnancy BMI, age, race, education, marital status, parity, smoking during pregnancy, assisted conception, and year of childbirth. Information on gestational age at delivery, obstetric history (previous infant death, preterm birth, or small-for-gestational-age birth in parous women), labor characteristics (eg, prolonged labor), mode of delivery, gestational hypertension, gestational diabetes, and congenital anomalies was also obtained from birth certificates. CHARS data included up to 9 diagnostic and 9 procedure codes (International Classification of Diseases, Ninth Revision, Clinical Modification [ICD-9-CM]) related to maternal hospitalization, information on death during hospitalization, the type of health insurance coverage, and intensive care unit admission. Prepregnancy hypertension and prepregnancy diabetes mellitus were identified from both data sources; the condition was deemed present if indicated in at least 1 data set (see eTable 1 in Supplement 2 for details). BMI was calculated as weight in kilograms divided by height in meters squared, using the mother’s self-reported height and prepregnancy weight (US standard certificate of birth, 2003 revision). BMI data were checked for consistency, and maternal BMI values outside the expected range were flagged as potentially erroneous and rectified.3,16 Maternal prepregnancy BMI was classified in the following categories: normal BMI (18.5-24.9), underweight (<18.5), overweight (25.0-29.9), and obesity class 1 (30.0-34.9), class 2 (35.0-39.9), and class 3 (≥40). Gestational age at delivery was based on ultrasound dating; date of last menstrual period was used for women with missing ultrasound data. Multiple births were excluded, because members within a twin or triplet set could not be identified in the data source. There were 952 212 live births and stillbirths in Washington State between January 1, 2004, and December 31, 2013. Births that occurred out of state, multiple births, births before 20 weeks’ gestation, and births to women younger than 15 years or older than 60 years were excluded (35 598 mothers [3.7%]), as were births that occurred out of hospital (24 716 mothers [2.6%]) and births that could not be matched with hospital records (64 609 mothers [6.8%]). Women with missing information on BMI (83 659 mothers [8.8%]) were excluded from the primary analyses. The study population included 743 630 women. Overall, 49.3% of women were overweight or obese (25.8% were overweight, 13.1% obese class 1, 6.2% obese class 2, and 4.2% obese class 3), while 47.5% were of normal BMI and 3.2% were underweight. Underweight women and those with normal BMI were younger and included a higher proportion of women of Hispanic and other race/ethnicity and a higher proportion of nulliparous women. Obese and underweight women had a higher rate of smoking during pregnancy, and obese women had higher rates of preexisting diabetes and chronic hypertension (Table 2). Obese women also had higher rates of cesarean delivery, labor induction, previous cesarean delivery, prior infant death, preterm birth or small-for-gestational-age birth, hypertension in pregnancy, and gestational diabetes (Table 3). Women with class III obesity had higher rates of breech presentation and lower rates of chorioamnionitis and precipitous labor than women with normal BMI. Compared with women with normal BMI, underweight women had significantly higher rates of antepartum hemorrhage and acute renal failure and were more likely to receive potentially lifesaving interventions, while overweight women had higher rates of acute renal failure (Table 5). Women with class 1 obesity had statistically significantly higher rates of thromboembolism, cerebrovascular morbidity, sepsis, acute renal failure, and complications of obstetric interventions, while women with class 2 obesity had significantly higher rates of respiratory morbidity, eclampsia, sepsis, acute renal failure, and complications of obstetric interventions (Table 5). Women with class 3 obesity had higher rates of respiratory morbidity (including pulmonary embolism [Table 1]), thromboembolism, cerebrovascular morbidity, cardiac morbidity, eclampsia, sepsis, acute renal failure, complications of obstetric interventions, and ICU admission (Table 5). On the other hand, women with class 3 obesity had significantly lower rates of severe antepartum and postpartum hemorrhage requiring transfusion (Table 5). Quiz Ref IDIn this study of pregnant women in Washington State, low and high prepregnancy BMI, compared with normal BMI, were associated with a statistically significant but small absolute increase in severe maternal morbidity or mortality. Rates of severe maternal morbidity or mortality showed a dose-response pattern among women with above-normal BMI, with adjusted odds ratios increasing with BMI from normal to class 3 obesity compared with women with normal prepregnancy BMI.Quiz Ref ID Underweight women had an increased risk of antepartum and postpartum hemorrhage with blood transfusion and renal failure and were more likely to require a potentially lifesaving intervention. Women with BMI in the overweight category and obese women had significantly higher rates of several different subtypes of severe maternal morbidity. Although relative measures showed higher rates of severe maternal morbidity or mortality among underweight, overweight, and obese women compared with women who had a normal BMI, the absolute risk of severe maternal morbidity or mortality was small. Two studies have examined the association between BMI and severe maternal morbidity.11,29 Lindquist et al29 carried out a case-control study (with severe maternal morbidity defined as including eclampsia, amniotic fluid embolism, acute fatty liver, peripartum hysterectomy, postpartum hemorrhage with therapy, and uterine rupture) and reported no association between BMI and severe maternal morbidity. A study by Schummers et al11 reported no association between BMI and a composite outcome of severe maternal morbidity or mortality and a significant inverse association between BMI and postpartum hemorrhage requiring interventionQuiz Ref ID. It is possible that anemia, which is more common among underweight women,7 exacerbates the effects of hemorrhage and leads to higher rates of blood transfusion. The association between high BMI and eclampsia is consistent with a previously reported 2- to 4-fold increased incidence of preeclampsia among obese women.4,12,14,15,30,31. This study has limitations. First, the observational design precludes causal inferences. Second, despite the large study size, there was insufficient statistical power to assess associations between BMI and rare severe morbidity subtypes and maternal death. This latter concern necessitated the creation of composite morbidity or mortality outcomes. Third, the number of maternal deaths in this study was low compared with reports on US maternal mortality (including deaths during pregnancy and 42 days postpartum).32 The smaller number of deaths was because this study focused on maternal deaths during the delivery hospitalization only. Fourth, reliance on ICD-9-CM codes meant that not all severely morbid conditions could be captured (eg, use of ICD-9-CM codes precluded the identification of eclampsia superimposed on chronic hypertension). As a result, the reported rates and odds ratios for eclampsia and for the composite severe morbidity or mortality may be underestimated among overweight and obese women. Fifth, errors and omissions in diagnostic coding that are inevitable in large databases may have led to underreporting and nondifferential misclassification and may have biased results toward the null. Sixth, data on income were not available, and results were therefore adjusted for characteristics of socioeconomic status such as marital status, education, race, and type of medical insurance. The shooting in Las Vegas, Nevada, that left 59 people dead, 10 times that number wounded, and thousands of people with the psychological distress from being present at the scene during and after the massacre has once again raised the issue of what we as a nation can and should do about guns. The solution lies in not just focusing on Las Vegas and the hundreds of other mass shootings that have occurred in the United States in the last 14 months, but rather to underscore that on average almost 100 people die each day in the United States from gun violence. The 36 252 deaths from firearms in the United States in 20151 exceeded the number of deaths from motor vehicle traffic crashes that year (36 161).1 That same year, the US Centers for Disease Control and Prevention reported that 5 people died from terrorism. Since 1968, more individuals in the United States have died from gun violence than in battle during all the wars the country has fought since its inception.2. Physicians and other health care professionals can do more. The US Court of Appeals for the 11th Circuit has ruled that attempts to prevent physicians from asking and counseling patients about guns violate the First Amendment.12 As with any epidemic, prevention is important. Physicians and others should ask about guns in the home, especially for high-risk patients,13 and advise about removal and safe storage. Good evidence has shown that safe storage of firearms is effective in reducing misuse.14 Physicians can conduct appropriate screening and early intervention for suicide,15,16 the most common cause of gun deaths. There is an obesity pandemic in the United States. In 1991, approximately 12% of the US population was obese,1 and no single state had an obesity rate greater than 15%. In 2014, the obesity rate was 38%,2 and no single state had an obesity rate less than 20%. Assuming that the prevalence of obesity was nearly zero for much of human history, it took thousands of years to reach an obesity prevalence of about 15%, but then just 25 years to more than double that rate. This pandemic is not only in the United States. Obesity rates have increased across most developed countries. Canada, for example, has had a similar increase in obesity, with prevalence reaching 24% among reproductive-aged women in 2012.3 Even in low-and middle-income countries, obesity rates are increasing.4 Health care costs associated with obesity are substantial. According to a 2013 report, obesity was estimated to add $600 per year in health care costs for a 20-year-old and $3800 per year in health care costs for a 70-year-old.5 According to these estimates, it is conceivable that health care costs associated with obesity could be more than $300 billion per year in the United States. In this issue of JAMA, Lisonkova and colleagues9 examine the association between prepregnancy body mass index (BMI) and severe maternal morbidity. The authors designed a population-based retrospective cohort study over a 10-year period (2004-2013) in Washington State using linked vital statistics and hospital discharge data. They examined BMI categories of underweight, normal weight, overweight, and obese (BMI >30), and further subcategorized obesity into class 1 (30-34.9), class 2 (35-39.9), and class 3 (≥40), allowing a more detailed examination of a dose-response relationship. Severe maternal morbidity was identified using International Classification of Diseases, Ninth Revision and Current Procedural Terminology codes as defined by the Canadian Perinatal Surveillance System and the Centers for Disease Control and Prevention. Conditions constituting severe maternal morbidity included sepsis, shock, cardiovascular events, cerebrovascular events, acute renal failure, and other similarly severe medical and surgical complications. The study population included 743 630 women, with prepregnancy BMI categorized as follows: underweight, 3.2%; normal weight, 47.5%; overweight, 25.8%; obesity class 1, 13.1%; obesity class 2, 6.2%; and obesity class 3, 4.2%. For the composite outcome of severe maternal morbidity, there were statistically significant increases in risk of severe maternal morbidity among women with class 1 obesity (absolute rate, 167.9 per 10 000; adjusted odds ratio [OR], 1.1 [95% CI, 1.1-1.2]), women with class 2 obesity (absolute rate, 178.3 per 10 000; adjusted OR, 1.2 [95% CI, 1.1-1.3]), and women with class 3 obesity (absolute rate, 202.9 per 10 000; adjusted OR, 1.4 [95% CI, 1.3-1.5]) compared with women who had prepregnancy BMIs in the normal range (absolute rate, 143.2 per 10 000). When specific outcomes were examined, transfusion for postpartum hemorrhage was lower among the obese women compared with those with normal BMI. Most of the other outcomes, including cardiovascular morbidity, cerebrovascular events, sepsis, and acute renal failure, were increased, with the largest risks among pregnant women with class 3 obesity. These findings support the hypothesis that the association of class 3 obesity with increased severe maternal morbidity in pregnancy may contribute to an increased risk of maternal mortality in those women. Additionally, identifying women at increased risk for complications may enable clinicians to reduce their risk of morbidity. For example, in one study of a Maternal Early Warning Trigger tool that examined maternal characteristics to identify women at increased risk of severe complications in labor and delivery, the authors reported a reduction in severe maternal morbidity.10 Perhaps if class 3 obesity were added to these screening characteristics, the predictive utility of such tools could be improved. But perhaps more important than how obese women are cared for during their pregnancies is whether the prevalence of obesity can be reduced before pregnancy. Public health preventive interventions (such as educational and environmental modifications) may be better suited to reduce obesity and related morbidity. However, the behavioral modifications necessary to reduce obesity (ie, increased physical activity and reduced caloric consumption) are particularly challenging to achieve in the 21st century, given that less physical activity is required to function in the modern world and because of the ready availability of calorie-rich foods. A multifaceted approach that combines the use of technology (eg, cell phone messaging, electronic exercise communities), environmental design, and societal or governmental efforts outside of traditional public health may be needed. For example, a more deliberate approach to the environmental design of communities to encourage physical activity might help reduce obesity.14 Financial incentives to encourage proper nutrition and exercise also may be useful, such as employers paying for their employees’ gym memberships and providing incentives to those who exercise.15. AMS-C indicates Acute Mountain Sickness-Cerebral score; LLQS, Lake Louise Questionnaire Score. The pooled analysis was stratified for each test: AMS-C (AMS-C of ≥0.7), the Hackett clinical score (≥3 points), the Chinese AMS score, and the LLQS at multiple cutoffs (LLQS ≥3, LLQS ≥4, LLQS ≥5). The data markers represent the size of each study. For indicative purpose, the few data points available for the instrument-in-comparison studies (visual analog scale score45,46 and clinical functional score22,47), not included in the meta-regression, are also shown. Detailed results of pooled analyses are shown in Table 2. A 52-year-old healthy man living at sea level arrives at a hotel at 4000 m (13 100 ft) after traveling all day. This is his first exposure to an altitude above 2500 m (8200 ft). When walking to dinner that evening he feels unusually exhausted, has shortness of breath and dizziness, and experiences palpitations and nausea. He is unable to eat and must return to his room. Which diagnostic instruments can be used to determine if these symptoms are indicative of severe acute mountain sickness (AMS)?. Identified risk factors for AMS can be grouped in the following ways: (1) an individual’s health, physiology, and genetics; and (2) specific behaviors and activities performed at high altitude. Although a recent meta-analysis challenged this concept,8 the most widely recognized risk factor is an individual person’s susceptibility to AMS. After a first episode of AMS, the risk of recurrence following reascent in similar conditions (rapidity of ascent, absolute altitude, no medical prophylaxis) can be as high as 60% with an odds ratio (OR) of as much as 12. Estimates of this risk vary by the type of diagnostic instrument used to establish a diagnosis of AMS.9-15 Although not yet demonstrated in humans, animal studies suggest that individual susceptibility to AMS can be explained by genetic differences in the respiratory drive.16-18 The risk for AMS is as much as 2.06-fold (95% CI, 1.15-3.72) lower for people older than 50 years.12,14,19-22 Women may be more likely affected than men,19,22,23 but this finding is not consistent.11,13,14,24 Medical conditions such as migraine,10,11,25 obesity,22,26 and mood states (anxiety) might also play a role in the development of AMS,27 whereas smoking and alcohol consumption do not appear to increase the risk for AMS.3,20,28. There are no biomedical tests that can establish a diagnosis of AMS; consequently, the diagnosis is made from clinical features. AMS is characterized by subjective symptoms (headache, anorexia, nausea, sometimes vomiting, dizziness, fatigue, and sleep disturbances) and, less frequently, few objective clinical signs (ataxia, palpitations, pulmonary rales, cyanosis) reported by the affected individual or through observations made by travel companions of persons with AMS.16 The presence and intensity of these altitude-related symptoms, their associated functional impairment, or both are assessed using a variety of diagnostic instruments. The Acute Mountain Sickness-Cerebral score (AMS-C), the Hackett clinical score, and the Lake Louise Questionnaire Score (LLQS) are the instruments used most frequently to establish a diagnosis of AMS. Each of these instruments was derived from a previous nonaltitude-specific Environmental Symptoms Questionnaire III score and are calculated as the sum of values given to different symptoms and signs weighted by their severity. Different cutoff values have been used to establish a diagnosis of AMS using the LLQS. In general, values larger than 5 points have been considered diagnostic of moderate to severe AMS. The Chinese AMS score, also based on the presence of several symptoms, is almost exclusively used in China. A visual analog scale (VAS) score, quantifying the subjective feeling of overall severity of sickness at altitude (VAS[O]), is the most recent instrument to be used for diagnosing AMS and has no commonly accepted cutoff value. Quiz Ref IDThe clinical functional score (CFS) is the simplest instrument to use because it relies on a single question: “Overall if you had any symptoms, how did they affect your daily activity?” scored on an ordinal scale of 0 to 3 (Table 1 and eAppendix 1 in the Supplement). Despite several of these instruments having been extensively used in clinical and research settings, how they perform relative to one another has not been studied in detail. The PRISMA Statement was followed to systematically review published literature on AMS (eFigure in the Supplement). MEDLINE and EMBASE were searched from inception to May 22, 2017, without language restriction to identify AMS in unselected visitors to high altitude. Keywords from the Rational Clinical Examination search strategy38 were combined with the MeSH keywords acute mountain sickness and altitude sickness (eAppendix 2 in the Supplement). Additional relevant articles were identified from searching the bibliographies of retrieved articles. Original studies that reported epidemiological data, described diagnostic procedures, or included comparison of different diagnostic instruments (including both observational and intervention study designs) were included. Review articles, studies that lacked clinical data, those in which the diagnostic procedure was not clearly defined, and those dealing exclusively with children or adolescents were excluded. High-altitude pulmonary edema (a separate entity from AMS that has different pathophysiological mechanisms) was not reviewed.39 Each abstract was reviewed independently by 2 authors (D.M. and T.-H.C.) to ensure that relevant publications met inclusion criteria. Subsequently, these same investigators independently reviewed each full-text article to confirm that inclusion and exclusion criteria were met and also abstracted data from the included studies. Disagreements were resolved by discussion, and, when necessary, consensus was reached with a third author (C.S.). For eligible studies, the risk of bias and applicability concerns were evaluated using the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) criteria40 by 2 coauthors (D.M. and T.-H.C.). The items or domains in QUADAS-2 were labeled as unknown if the corresponding study characteristics were not reported. Disagreements in quality assessment were resolved by consensus among coauthors. To compare different instruments, the LLQS was selected as the reference instrument based on expert opinion and because the LLQS has become the most frequently studied comparator scale. Expert opinion is that a threshold score of 3 or greater enhances the opportunity to detect mild AMS but may result in overdiagnosis. Most studies evaluated the LLQS at various cutoffs. For the present review, the highest cutoff (a score of ≥5) was used as the reference standard.24. To determine the prevalence of AMS for establishing the pretest probability of AMS, a random-effects meta-regression was performed based on the reported prevalence of AMS as a function of altitude. The midpoint of the range of altitudes reported by study participants was used to assign an altitude for each study included in this analysis. A pooled analysis was performed that included all the data, then score-specific meta-regressions were performed to compare the relationship between the prevalence of AMS and altitude for the LLQS using various thresholds for establishing the presence of AMS (LLQS ≥3, LLQS ≥4, and LLQS ≥5): the AMS-C of 0.7 or greater (derived from the Environmental Symptoms Questionnaire III, see eAppendix 1 in the Supplement); the Hackett clinical score of 3 or greater; and the Chinese AMS score. In studies using the VAS(O) for AMS, differing thresholds (as defined in each article) were used to establish a diagnosis of AMS. Between-study variance was estimated using the I2 statistic.41 The proportion of between-study variance explained by altitude was estimated using the R2 statistic. Random-effects meta-regression showed that studies conducted at higher altitudes reported a higher prevalence of AMS. Above 2500 m (8200 ft), for every 1000-m increase (3300-ft increase) in altitude, there was a 13% increase (95% CI, 9.5%-17%) in the prevalence of AMS (Figure; Table 2). The majority of data was obtained from studies using the LLQS with a cutoff score of at least 3 to diagnose AMS. Despite the narrow CI, there was significant heterogeneity (I2 = 98%) among these studies. The heterogeneity was partly explained (28%) by different altitudes examined in the studies. The contributions from other known determinants of AMS (such as speed of ascent, preacclimatization, and prophylaxis) could not be established because of insufficient detailed data on these factors. Seven different instruments (LLQS, AMS-C, VAS[O], VAS[C], Hackett clinical score, Chinese AMS score, and CFS) were found in the literature in which the diagnosis of AMS was described. For 5 of these instruments (LLQS, AMS-C, Hackett clinical score, Chinese AMS score, and VAS[C]), AMS is determined by the presence of several neurological, gastrointestinal, and respiratory symptoms that develop at high altitude because there is no reliable biomedical test for diagnosis of AMS. The number of symptoms evaluated by these instruments varies as does the weighting for the severity of the symptoms. The remaining 2 instruments (VAS[O] and CFS) explore other aspects of AMS. The VAS(O) measures a patient’s perception of being unwell from AMS. The CFS, which has the simplest scoring system, diagnoses AMS based on the extent of functional impairment of daily activities that might occur at high altitude. The LLQS can be considered the de facto reference standard for diagnosing AMS for both clinical and research purposes. Our study showed that even though the various instruments emphasize different aspects of AMS, the VAS(O), AMS-C, and CFS scores performed similarly for diagnosing AMS. The performance of the Hackett and Chinese AMS scores could not be assessed because of insufficient published data regarding their diagnostic accuracy. The relationship between altitude and AMS was examined using random-effects meta-regression. Beginning at the altitude of 2500 m (8200 ft), the commonly accepted definition of high altitude for every 1000 m (3300 ft) of ascent, the prevalence of AMS increased by approximately 13%. Less than one-third (R2 = 28%) of the relation between altitude and AMS prevalence was explained by altitude alone. This is likely because many well-recognized AMS risk factors such as previous episodes, speed of ascent, preacclimatization, and use of medical prophylaxis were not controlled for or were incompletely reported in most studies of AMS. The AMS-C, the VAS(O), and the CFS had similar diagnostic accuracy for severe AMS when compared with the LLQS when its score threshold was greater than 5. These results were not entirely unexpected for the AMS-C because it was derived from the LLQS. The similar performance of the VAS(O) and the CFS instruments, compared with the AMS-C, was not anticipated. By assessing a nonspecific functional impairment induced by altitude exposure, independent of the presence and nature of the symptoms, the CFS and, in part, the VAS(O) explore different aspects of AMS than the other instruments.35,58 Despite the differences in CFS and VAS(O) assessment, the performance of these simpler instruments was good and comparable to that of AMS-C. This observation is consistent with the new concept that AMS might not be a single entity but may manifest in different ways and present as symptom clusters that vary between patients (fatigue and insomnia vs headache and sleep disturbances vs headache alone).62. Determining which instrument might perform better at diagnosing AMS at different altitudes was challenging because the instruments have not been compared directly with one another at the same altitudes. At higher altitudes, there is higher risk of AMS evolving to life-threatening high-altitude cerebral edema. In this situation, an instrument with greater sensitivity (such as the CFS) is preferred because it is important to identify cases of AMS even at the risk of overdiagnosis. At lower altitudes (for example, <4000 m [13 100 ft]), where risk of severe AMS is less, one might favor a more-specific instrument such as the AMS-C or VAS(O), which will facilitate the decision for the need of medical prophylaxis if a patient plans reascent to similar altitude. In the absence of objective measures to diagnose AMS, the LLQS was used as a reference standard for establishing a diagnosis of AMS. The LLQS is not an ideal standard because it relies on the presence and severity of the patient’s subjective symptoms. No study has used the rapid disappearance of altitude-related symptoms with descent as a reference standard. This approach would be less dependent on clinical judgment for establishing a diagnosis of AMS. Because of insufficient granularity of AMS studies for the examination of the contribution of individual symptoms to AMS, it was not possible to determine the relative importance of each symptom in each scoring system. This analysis would be important for 2 reasons. First, AMS might not be a single entity. Rather, it might consist of symptom clusters (fatigue and insomnia vs headache and sleep disturbances vs headache alone) that affect individuals differently.62 Second, controversy exists about the inclusion of headache as an essential symptom of AMS (required by the LLQS) and the equivalent weight given to disrupted sleep compared with the other 4 symptoms (headache, gastrointestinal upset, fatigue, and dizziness) in the LLQS.66-68. Some of the heterogeneity between studies observed when estimating the prevalence AMS at different altitudes might be explained by differences in the individual characteristics of included participants. The studies of AMS have frequently relied on convenience samples of unselected travelers at different study locations. Compared with studies at lower altitudes (<4000 m [13 100 ft]), observational field studies performed at very high altitude might include more experienced travelers who might be less susceptible to AMS. At these higher altitudes, generalization of our findings to trekkers and occasional climbers is uncertain. The clinical vignette depicts a typical presentation of altitude-related symptoms. Based on our model, predicted prevalence of moderate to severe AMS at 4000 m (13 100 ft) is approximately 39% (Figure). The presence of an important functional impairment (CFS = 2) from multiple symptoms in an otherwise healthy person increases the likelihood of that person having AMS (positive LR, 3.2). Thus, the probability that the patient has AMS is approximately 67%. An estimation of the AMS-C can be calculated from the symptoms listed in the introduction and would be approximately 1.4, which is twice the threshold value of 0.7. There are many harmful manifestations of atherosclerotic cardiovascular disease (ASCVD). Because all of these manifestations are undesirable, combining the most important ones into a single study outcome measure can simplify efforts to measure the overall effect of the disease on health outcomes. For example, ASCVD can result in myocardial infarction (MI), stroke, or death. Each of these is to be avoided, and how well an intervention reduces the risk of any of these occurring can be measured by combining all of these clinical outcomes into a single composite end point. A composite end point is an outcome that is defined as occurring if 1 or more of the components occurs. For ASCVD, one of the most common composites is called major adverse cardiovascular events (MACE). Because a composite outcome occurs more frequently than its individual components, composites can reduce the number of study participants required to achieve the desired power of a study, making it easier and less expensive to conduct a clinical trial. Composite end points may be used in a clinical trial (or in observational studies) if the target disease has several clinically important consequences and the study is intended to examine the effects (or association) of an intervention on (or exposure with) more than 1 consequence or end point. In this case, a composite end point provides a summary measure for the treatment effect. Composite end points, such as MACE, may also be used when a single outcome of interest (eg, CHD death in a low-risk population) is rare, making it impractical to conduct studies that are adequately powered to demonstrate an effect of an intervention on its occurrence.2 For rare outcomes, researchers often combine several types of events (CHD death, MI, and stroke) in a single composite end point. Because the frequency of the composite end point is greater than any of its components, this facilitates the design of studies of reasonable size and duration that have sufficient statistical power. If only 1 infrequent outcome were considered, such as CHD death, studies to determine the effect of an intervention on those outcomes could be unreasonably large or take too long to complete. When multiple outcomes are combined into a composite end point, each is given the same importance because the occurrence of any component counts equally within the composite. However, the relative importance of each component to patients, their families, and clinicians may be very different. For example, it is common to count the occurrence of CHD death, nonfatal MI, or stroke equally as a MACE event. If any of these outcomes should occur, the patient is considered to have the outcome event, resulting in each component being weighed equally. However, CHD death is more important to patients than nonfatal MI, especially if the patient recovers from the MI with little or no long-term effects. If patients perceive the importance of individual components differently (eg, death being a much worse outcome than having an MI), then using a singular composite end point to represent a study result may be misleading. For example, the LIFE trial compared losartan and atenolol for hypertension and showed a statistically significant advantage of losartan for reducing the composite end point of having an ASCVD event (CHD death, MI, or stroke). However, this effect was only observed with stroke (fatal and nonfatal) and not with MI or CHD death.3 Often, a positive effect on the composite end point is driven by the event with the highest occurrence rate (eg, significant reduction on nonfatal MI). If this component has relatively few consequences for a patient but other outcomes, such as stroke or CHD death, that are more consequential are unaffected or even increased by the intervention, then the apparent benefit of the intervention is misleading. To remedy this limitation, additional analyses, with each component considered individually, are recommended as an adjunct to the analysis of the composite outcome. However, because each individual component is less common than the composite, the power of these analyses is often limited and, further, multiple comparisons increase the risk of a false-positive result.4. In addition, if patients can experience the composite outcome more than once, and the number of events is the outcome of interest, then an intervention that increases the incidence of CHD death might incorrectly appear beneficial. This can happen because, when a death occurs, patients are no longer at risk of having a nonfatal MI or stroke. This is known as a competing risk because some of the risks being assessed cannot happen after another, such as death, has occurred. The intervention may not be desirable, even if it shows an overall advantage on the composite of incident ASCVD, because a study participant who would have had 2 mild MIs dies first and has only 1 event that is more serious. Competing risks might therefore result in an underestimate of the occurrence rate of the composite end point that would occur if the mortality rate were not increased by the intervention. The study by Kavousi et al1 avoided possible bias from competing risks by including only first-incident events in the analysis. Composite end points can be more useful when a specific weight is assigned to each component that reflects each component’s importance or “utility” to patients and clinicians. For example, avoiding CHD death would have a greater utility to patients than avoiding a nonfatal MI. A composite end point with no weights to its components assumes the utilities of all outcomes are equal and, in clinical medicine, this is almost never the case. The relative value of the weights or utilities of each outcome of interest should be scientifically elicited from patients or clinicians. For example, in a study by Ho et al,5 patient preferences were elicited using a discrete-choice experiment in which 540 obese respondents evaluated the effectiveness, safety, and other attributes of weight-loss devices. The study generated patient utilities for effectiveness, safety, and other device attributes that were subsequently used to inform regulatory decision-making. A 66-year-old man with newly diagnosed multiple myeloma underwent autologous hematopoietic stem cell transplantation (HSCT). On posttransplant day 10, he developed fever (38.4°C). He was started on empirical treatment with cefepime; however, his fevers persisted despite antibiotic therapy. On posttransplant day 14 he developed new-onset pruritic, confluent, erythematous, blanching morbilliform macules and papules on his trunk and extremities (Figure, left). Review of systems was notable for diarrhea and a 6-pound weight gain since transplantation and negative for headache, cough, shortness of breath, or abdominal pain. On examination, there was no lymphadenopathy or hepatosplenomegaly. The patient’s heart rate was 100/min, blood pressure was 106/68 mm Hg, and he occasionally experienced mild hypoxia to 94% on ambient air. Results of a basic metabolic panel were normal, with a blood urea nitrogen level of 10 mg/dL (3.57 mmol/L) and creatinine level of 0.7 mg/dL (61.88 μmol/L). White blood cell (WBC) count was 7300/μL, and absolute neutrophil count (ANC) was 5480/μL. Neutrophil recovery occurred beginning on posttransplant day 10, with WBC count of 700/μL and ANC of 532/μL on posttransplant day 10, correlating with the onset of the patient’s fevers. Platelet count was 48 × 103/μL, and hemoglobin was 8.2 g/dL. Results of liver function tests were normal. Blood, urine, sputum, and respiratory viral cultures were negative. Tests for cytomegalovirus, Epstein-Barr virus, human herpesvirus-6, adenovirus, parvovirus, cryptococcal antigen, aspergillus galactomannan antigen, β-glucan, and Clostridium difficile were negative. Chest computed tomography demonstrated multifocal ground-glass and consolidative pulmonary opacities (Figure, right). Computed tomography of the abdomen was unremarkable. A skin punch biopsy demonstrated subtle vacuolar change; rare epidermal dyskeratosis, including within a hair follicle; and superficial perivascular mononuclear cell infiltrate with occasional eosinophils. Empirical antiviral and antifungal therapy is unnecessary, given the patient’s clinical stability, the low suspicion for infection, and negative findings on viral and fungal testing. Ferritin and soluble IL2Rα are used in the diagnosis of hemophagocytic lymphohistiocytosis (HLH), an inflammatory syndrome that can result in multiple-organ failure and death. However, the patient did not have hepatosplenomegaly, and his cytopenias were attributable to his recent HSCT, making the diagnosis of HLH unlikely. Although bronchoscopy may be considered in the workup for new pulmonary infiltrates, it is unnecessary for diagnosis of engraftment syndrome. The primary differential diagnosis for the symptoms observed in engraftment syndrome includes morbilliform drug rash, viral exanthem, and acute graft-vs-host disease (GVHD).1 Exanthematous drug rashes are generally asymptomatic and do not present with systemic findings. Symptoms of engraftment syndrome can resemble those of acute GVHD, which manifests with erythematous morbilliform rash, hepatic dysfunction, and diarrhea. However, acute GVHD typically develops after allogeneic transplant, whereas engraftment syndrome is more commonly associated with autologous transplant. Furthermore, pulmonary interstitial infiltrates and respiratory symptoms are not characteristic of acute GVHD. Radiologic features of engraftment syndrome are variable but include ground-glass or consolidative pulmonary opacities, interstitial pulmonary edema, and pleural effusions.7 The histologic features of engraftment syndrome are nonspecific and include interface vacuolar change and superficial perivascular infiltration. These findings can overlap with those seen in acute GVHD and a subset of drug rashes. However, prominent eosinophils, which were not observed in this patient, are more suggestive of drug rash. According to one study, features that favor engraftment syndrome over acute GVHD include a predominance of CD4+ T cells and decreased CD1a+ cells, but this has yet to be replicated in additional studies.8 Thus, although certain histologic clues may point to a diagnosis of engraftment syndrome, engraftment syndrome is a clinical diagnosis that does not require biopsy. Antibiotics were discontinued, and the patient’s rash was managed with topical clobetasol (0.05% ointment) and antihistamines for pruritus. Interval chest computed tomography in 4 days revealed improvement of the pulmonary infiltrates. The fevers resolved within a few days, and the rash resolved within 1 week. The patient was discharged home on posttransplant day 20. In attendance was Robert T. Schooley, MD, chief of the infectious disease division at the University of California, San Diego (UCSD), who joined the second wave  of the bacteriophage movement 2 years ago after a friend and patient contracted a multidrug-resistant Acinetobacter baumannii infection, which has also sickened many US veterans returning from the Middle East. After his friend’s infection took a turn for the worse, traditional antibiotics were no longer effective, and septic shock took hold. Time was running out, so bacteriophage therapy was administered under Schooley’s direction after applying for and receiving Emergency Investigational New Drug status from the FDA in a last-ditch effort. The phages specifically active against A baumannii were obtained from the US Navy and Texas A&M University, both of which have ongoing phage research programs, as well as the San Diego biotech firm AmpliPhi. Within days of treatment, his friend’s condition began to improve, thanks to what is considered the first time in the United States that phage therapy was administered intravenously for a systemic infection. Dr Schooley:I think there are many potential applications for phage. Like other therapeutic approaches, you have to do the kinds of clinical trials that tell you how most effectively to use them. The thing I was unaware of until I got involved in this patient’s case was that phage technology has moved along quite well. There are very good techniques to identify active phage in a short period of time so as to assemble a cocktail against any given organism put in front of you. Dr Schooley:Yes. Phages are everywhere. They're the most dominant organism on earth. So we're never going to run out of phage.  . Dr Schooley:Right now, the selection process is pretty empirical. You toss your library at the organism, and then find the phage that have activity. Each organism you treat has a different range of phage that are active against it. For example, Staphylococcus aureus has a family of phage that will attack it. Select the best, combine them, and you have your cocktail. The diversity of phage you need in your library to cover most bacteria you will run into clinically varies. So for Staph, which is a relatively homogeneous organism from the standpoint of phage targeting, you might come up with a relatively small cocktail that would cover most strains. With other organisms like Acinetobacter, the diversity is such that you couldn't do that with a single cocktail. You'd have to personalize the cocktail from a larger library. It might take several hundred phages in a library to adequately cover what you would need to treat an Acinetobacter infection. So depending on the application, you could envision either a fixed cocktail or having to come up with an ad hoc one. But over time as we learn more about how phage interact with bacteria and understand more about resistance mechanisms, it might be possible to take a bioinformatics approach that would allow this process to be more systematic and less empirical. Dr Schooley:Yes. This patient’s case was in some ways the most complicated situation in which to employ it. You had a rapidly progressive clinical process that made it imperative that if the phage were going to be used, you had to develop a cocktail very quickly. And we were fortunate that Texas A&M and the Navy were so successful in quickly coming up with a phage prep [treatment] that we could give to the patient. If we'd been on the other side of this a week or so, I think we wouldn't have been able to pull him through. Dr Schooley:Yes, you can't count on them all the time in situations where a patient is critically ill and you only have a few hours to get an antimicrobial on board. As you know, in some of the sepsis guidelines the goal is to get people on therapy almost immediately. So that's a challenge. But there are other situations in which the multidrug-resistant organisms are chronic and you have more time to come up with a cocktail that is usable. For example, patients with cystic fibrosis end up with multidrug-resistant gram-negative organisms causing recurrent bouts of pneumonia. Pseudomonas, Serratia, Acinetobacter, Staph—these are all organisms that you could target with phage therapy. And you could develop these cocktails for each individual patient and use them either via an intravenous or aerosol route. Dr Schooley:Yes. For example, if you have an infected knee prosthesis that has a relatively small organism load, you could envision a phage cocktail to augment antibiotic use. There are [also] devices you don't want to take out of patients [like] left ventricular–assist devices that get infected. One problem with sterilizing them is biofilms. Phage have a way of dealing with biofilms, and they make antibiotics that are not always effective be more effective. Urinary tract infections with complex anatomy can often end up with multidrug-resistant organisms. Phage use there might help without pushing the microbiome into a more and more resistant posture. Dr Schooley:Yes, one of their beauties is their specificity. They wipe out the organism that you're after. That organism will develop resistance to the phage you're using, but sometimes to their detriment. In our patient, the organisms that were resistant to phage lost their capsule [the outermost enclosure of a bacterium] and became less invasive in an in vitro model. So they pushed the organism in directions that the organism would not like to go. And they also make it more amenable to attack by host offenses. Dr Schooley:It's like any other therapeutic. You need to go about investigating how to use them in an organized way. Clinical trials are urgently needed, so we can understand the pharmacokinetic and pharmacodynamic interactions with infections in different locations. Dr Schooley:I have been very impressed with the way they’ve looked at it. They understand that multidrug-resistant, gram-negative infections are a real problem. And that the progress with traditional small molecules has been very slow. They see this as something that clearly needs to be evaluated, and they've been very open to rigorous and carefully controlled clinical application of these organisms. Dr Schooley:Well, it's always hard to know what Big Pharma's doing. Most activity is in smaller companies, many of them phage-specific ones. AmpliPhi provided some of the phage for the therapy for our patient. There’s another company called Applied Phage Therapeutics that's [working] with the Navy on some of their technology. Then there's another European company that has been working on phage therapy for burns and other things. So a lot of this is growing up in small biotech companies. Pharma has been relatively risk averse, and they don't really know how to think about phage. We have to figure out how to develop them clinically. And there are some nuances that will be different from small molecules. Not [that] these problems are insurmountable; it just means they're different. The other issue with phage is they've been around for 100 years and there was a time in the 1930s in the US where they were being sold commercially in a huckster-like way. When you lose your reputation as something that's above board, it's sometimes hard to get it back. So I was a phage skeptic until I began to get involved. One of the problems has been the clinical trials have not been that structured and it's been hard to evaluate their success. Just as it is in the patient we treated. I mean, I think the turnaround was because of the phage. But a lot of other things were going on too. And you need more than anecdotal experience to be able to validate a therapy and understand how to use it in the broadest possible context. Dr Schooley:I think that over the next 6 months to a year we’ll be seeing patients not unlike ours who have limited options, but then I think you'll begin to see some trials that may well be company-sponsored and will involve more than one institution. I'd like to see trials that don't just generate clinical end points but get at some of the biology in the patients being treated, so we can understand how more rationally to plan the next range of studies. There's been a lot of work at the basic science level that has pushed phage biology further. But the support for organized clinical trials of the type that have helped us understand how to use antibiotics and antiviral drugs and others is where I think the hole is. Of course, physicians ordered all of this care. When asked why they would do it, knowing it was unnecessary, the most common reason cited (85%) was a fear of being sued for malpractice. Research shows, though, that “defensive medicine,” as this practice known, likely accounts for far less wasted spending than we think. When physicians practice in areas with a lower risk of lawsuits, their overall practice doesn’t change that much. Another study in JAMA Internal Medicine showed that although a lot of care may be ordered in part because of defensive medicine, wasted care ordered only because of fear of lawsuits comprised less than 3% of overall costs. Earlier this year, H. Gilbert Welch, MD, MPH, and Elliott Fisher, MD, MPH, wrote in the New England Journal of Medicine about how overdiagnosis is one of the few areas where having a higher income may be detrimental for patients. For years we’ve known that the incidence of breast cancer, prostate cancer, thyroid cancer, and melanoma have been rising. There hasn’t, however, been a parallel increase in cancer-specific mortality. This implied that many cases are being picked up, likely through screening, that didn’t need to be. They hypothesized that people with more money are likely to be screened more often and by more advanced methods. This leads to more overdiagnosis. It’s not clear whether these patients are demanding more screening or whether the health care system is pushing it on them. It’s likely a combination of both. The elephant in the room—sometimes—is, of course, money. Some physicians have a direct financial conflict of interest in making decisions as to what patients should receive. Physicians who own surgical, laboratory, or radiological centers receive extra compensation from the services they advocate for their patients. The recent survey found that more than 70% of physicians believe that physicians are more likely to perform unnecessary procedures when they profit from them. The problem is that this care isn’t necessarily benign. Drugs have adverse effects. Diagnoses cause worry and also can lead to more and more invasive interventions. Procedures may have adverse outcomes, sometimes serious ones. Drugs, tests, and procedures that aren’t necessary provide no benefit. Therefore, by definition, the cost of these drugs, tests, and procedures—in adverse effects, time, and money—outweigh the benefits. We can try to justify the rationale for this in some defensive language, but at its core, all we are doing is causing harm. There are things we can do to mitigate this problem. We can educate the public better on the issue of low-value care, stressing the harms, both financial and health-related. We can minimize the financial incentives of providing unnecessary treatments, by identifying conflicts of interest and making hospitals and physicians more responsible for some of the decisions they make. We could also push for insurance reform that refuses to pay for care that’s not needed, making everyone think twice before employing it. It will likely take changes on all these fronts to make a real difference. bTo ascertain whether patients had advanced chronic kidney disease, the first estimated glomerular filtration rate (eGFR) of less than 30 mL/min/1.73 m2 was obtained a median of 75 days (interquartile range [IQR], 40-160 days) after discharge and the last a median of 267 days (IQR, 125-321 days) after discharge in the derivation and internal validation cohorts. There were 29 patients (0.2%) in the derivation and internal validation cohorts  with only 1 eGFR less than 30 mL/min/1.73 m2 and did not have a subsequent measurement during follow-up. They were not considered to have developed advanced chronic kidney disease. Patients were followed up through March 2015. We identified all adult residents 18 years of age or older who were hospitalized during the study enrollment period (Figure 1). Eligible participants required at least 1 outpatient estimated glomerular filtration rate (eGFR) value between 7 and 365 days prior to hospitalization to establish baseline kidney function (outpatient serum creatinine tests in this period generally reflect stable values).17 We restricted the cohorts to those hospitalized with acute kidney injury (based on an increase in serum creatinine during hospitalization >0.3 mg/dL or >50% of their prehospitalization baseline),4 a baseline eGFR higher than 45 mL/min/1.73 m2, and at least 2 outpatient serum creatinine measurements separated by a minimum of 3 months with the first between 30 days and 1 year after the hospital discharge date. This required that patients survived more than a month after hospital charge and had sufficient follow-up information to ascertain development of advanced chronic kidney disease, and reflected a pragmatic approach that recognizes that organizing chronic kidney disease care is not needed when patients do not survive long enough to develop advanced chronic kidney disease. We excluded people treated with chronic dialysis or who had had a kidney transplant prior to or during the index hospitalization, identified by a validated approach using physician billing claims for a continuous period of 90 days of dialysis in provincial administrative databases or registration for chronic dialysis or kidney transplant with the Canadian Organ Replacement Registry.18. We identified candidate predictor variables for potential inclusion in our models from a review of the literature and identified potential predictor variables available prior to and during the index hospitalization with acute kidney injury.1-4 Baseline kidney function was determined from the most recent outpatient serum creatinine measurement made 7 to 365 days prior to the index hospitalization.17 Albuminuria was characterized by urine albumin:creatinine ratio (ACR) or dipstick using random spot urine measurements during the index admission or for up to 6 months prior to admission. We used urine ACR measurements preferentially, with urine dipstick measurements added for patients without ACR measurements in the derivation cohort. We defined albuminuria categories as normal (ACR, <30 mg/g or urine-dipstick negative), mild (ACR, 30-300 mg/g or urine dipstick trace or 1+), or heavy (ACR, >300 mg/g or urine dipstick positive ≥2+) and characterized patients with multiple tests using the median of multiple measurements, recognizing that albuminuria can be transient.26 Absence of a result for albuminuria was included as a separate category. We categorized the severity of acute kidney injury according to 3 categories of the Kidney Disease, Improving Global Outcomes (KDIGO) staging system based on the highest serum creatinine value identified during hospitalization or requirement for dialysis during the index hospitalization.4,27 We identified the extent of kidney function recovery at the time of hospital discharge based on the last inpatient serum creatinine value measured before hospital discharge.2,28 Comorbidities and procedures were identified from hospital discharge records and physician claims using validated ICD-9-CM and ICD-10 coding algorithms12-14,18,29 (eTable 1 in Supplement 1). Candidate variables from the derivation cohort were included as potential covariates in multivariable logistic regression models. Stepwise backward variable selection with a significance level of .05 for variable retention was used in 1000 bootstrapped samples of the same size as the derivation sample, to develop a parsimonious predictor model while minimizing overfitting.30,31 Variables selected in at least 70% of samples were included in the full multivariable model. We then fit a series of reduced models by sequentially removing variables and compared the full multivariable model with the simplified models. Reduced models were structured to include variables with greater ease of ascertainment at hospital discharge and to include removing variables requiring prehospitalization laboratory data. To further support clinical use of the model, we created a simple integer risk index using the method described by Sullivan et al.32 The performance of the risk index was determined based on modeling the logit of the total risk index for each participant. The regression coefficients from the logistic regression models in the derivation sample were fixed. The fitted models were applied to both the internal and external validation cohorts. The overall goodness-of-fit of the models was compared based on Akaike Information Criteria. Model discrimination was compared based on the C statistic and integrated discrimination improvement (IDI). Model calibration was assessed by the calibration intercept, by the calibration slope, and graphically by locally weighted scatterplot smoothing (LOESS) plots of observed vs predicted probabilities of the outcome.33,34 The ability of models to reclassify patients into high- or low-risk categories was compared using the net reclassification index (NRI).35,36 Risk thresholds for the outcome were defined as less than 1.0%, 1.0% to less than 5.0%, 5.0% to less than 10.0%, 10.0% to less than 20.0%, and 20.0% or higher. Reclassification was also compared using the continuous NRI, which assesses reclassification across a continuous range of risk thresholds. A P value of .05 was considered statistically significant using 2-sided testing. Statistical analyses were performed using SAS version 9.4 (SAS Institute Inc) and R (https://r-project.org). A total of 14 958 participants were included in the derivation and internal validation cohorts (9973 in the derivation cohort and 4985 in the internal validation cohort) and 2761 in the external validation cohort (Figure 1). The mean age of participants ranged from 66 years through 69 years, the mean baseline serum creatinine value was 1.0 mg/dL, obtained a median of 66 days (interquartile range [IQR], 24-161 days) prior to admission in the derivation and internal validation cohorts and 44 days (IQR, 17-124 days) in the external validation cohort, more than 20% had stage 2 or 3 acute kidney injury and the serum creatinine value was 1.3 mg/dL or higher at the time of hospital discharge in one-third of patients (Table 1; to convert creatinine from mg/dL to μmol/L, multiply by 88.4). Albuminuria status was available for 57% of the derivation and internal validation cohorts and 36% of the external validation cohort. Compared with the derivation and internal validation cohorts, patients in the external validation cohort were slightly older and were more likely to have received cardiovascular procedures or mechanical ventilation. The median number of outpatient serum creatinine measurements per patient during follow-up was 11 (IQR, 6-22) in the derivation and internal validation cohorts and 12 (IQR, 7-21) in the external validation cohort. There were 29 patients (0.2%) in the derivation and internal validation cohorts with only 1 eGFR less than 30 mL/min/1.73 m2 and did not have a subsequent measurement during follow-up. They were not considered to have developed advanced chronic kidney disease. Six variables were associated with a higher risk of progression to advanced chronic kidney disease in bootstrapped samples of the derivation cohort: older age, female sex, higher baseline serum creatinine values, albuminuria, greater acute kidney injury severity, and higher discharge serum creatinine values (Table 2; eTable 2 in Supplement 1). The 6-variable model (model 1) had the highest C statistic (0.86; 95% CI, 0.84-0.89) and lowest Akaike Information Criterion (1967.0). Based on the predicted risk using model 1, 5228 patients (52.4%) in the derivation cohort had less than 1%; 3512 (35.2%), 1% to less than 5%; 626 (6.2%), 5% to less than 10%; 360 (3.6%), 10% to less than 20%; and 247 (2.5%), 20% or more risk of developing chronic kidney disease (eTable 3 in Supplement 1). Discrimination (based on the C statistic and integrated discrimination improvement [IDI]) was lower in reduced models without inclusion of albuminuria, discharge serum creatinine values, and baseline serum creatinine values. The lowest C statistic was observed for model 5, which included age, sex, and acute kidney injury stage only (C statistic, 0.71; 95% CI, 0.67-0.74). In the internal validation cohort, the 6-variable model remained well calibrated (eFigure 1 in Supplement 1) and also had a higher C statistic (0.87; 95% CI, 0.84-0.90) than did the models with fewer variables (Table 3; eTable 4 in Supplement 1). The IDI, continuous NRI, and categorical NRI favored the 6-variable model compared with the reduced-variable models. For example, significantly improved classification into risk categories was observed with the 6-variable model 1 compared with model 4—age, sex, and discharge serum creatinine values (categorical NRI, 16.6%; 95% CI, 7.7%-25.6%; P value <.001) and model 5—age, sex, and acute kidney injury stage (categorical NRI, 99.3%; 95% CI, 87.6%-110.9%; P value <.001). Model calibration and discrimination were similar for patients with baseline serum creatinine measurements taken within 3 months or taken more than 3 months prior to admission and with baseline eGFR greater than vs less than 60 mL/min/1.73 m2 (eTable 5 in Supplement 1), as well as for patients excluded from the derivation and validation cohorts due to absence of outpatient serum creatinine measurements prior to index hospitalization (eTable 6 in Supplement 1). Model validation is an important aspect of developing clinical prediction models. There was no deterioration in model calibration upon evaluation in the internal validation cohort, indicating that the models were likely not overfit. An even more important aspect is external validation in a different population of patients before widespread model adoption and use. These models performed well in the geographically distinct Ontario cohort, despite some expected loss of discriminative performance (model 1 C statistic, 0.81; 95% CI, 0.75-0.86). Models are typically considered useful for clinical decision making when the C statistic is higher than 0.70 and strong when the C statistic exceeds 0.80, suggesting this model could support clinical decision making.37 Furthermore, the net reclassification improvement of the full model compared with simpler models demonstrates that using this model could improve accuracy of decision making for follow-up compared with that based on fewer variables alone. This study has several limitations. First, candidate variables were identified from secondary analysis of data, which does not include all potential risk factors for chronic kidney disease such as blood pressure, urine sediment, cause of acute kidney injury, and details of dialysis treatments. However, similar to this study, another risk prediction model for chronic kidney disease also found that clinical variables provided little improvement in model performance beyond kidney-related laboratory variables.38 Second, if low-risk patients were systematically excluded from these cohorts due to lack of follow-up creatinine testing, then estimates from the resulting models could overestimate risk of advanced chronic kidney disease. However, the predicted risks of excluded patients were similar to those of included patients, suggesting that such bias is unlikely. Third, some participants did not have albuminuria measured, which may influence the performance of the models. However, models without albuminuria also performed well, suggesting that risk prediction for chronic kidney disease following acute kidney injury could still be achieved without albuminuria measurements. Fourth, models were derived and validated in cohorts from Canada, and generalizability to patients from other regions was not examined. The heart is an organ charged with extraordinary symbolic value featured in art across time. Contemporary clinically rooted representations of the heart have appeared recently in UK exhibits by fashion and portrait photographer Kirsty Anderson, who mounted a photograph display of patients with congenital heart disease (CHD) at Kelvingrove Art Gallery and Museum in Glasgow accompanied by first-person stories relating to each portrait; and by artists Tim Wainwright and John Wynne, who employed photography, film, and sound to recount the experience of transplantation and organ donation at the Hunterian Museum in London. In both cases, the voice of the patient was an integral component of the artwork itself. Here we describe how a participatory arts practice led to co-creating original artwork incorporating the voice of patients and families. With Heart Narratives, a 165 × 85-cm wall-mounted panel, British artist Sofie Layton aimed to represent the language and narrative of CHD. Layton was artist in residence in 2015-2016 at Great Ormond Street Hospital for Children in London. On the hospital ward or through creative workshops,2 the artist worked with families and medical personnel treating CHD to explore imagery and to develop a visual representation of their narratives of illness. Participants included children with CHD (adolescents with repaired defects, including transposition of the great arteries, tetralogy of Fallot, and hypoplastic left heart syndrome); parents of newborns or infants on pediatric cardiac wards with diagnoses of hypoplastic left heart syndrome and DiGeorge syndrome; and researchers and practitioners. The panel is composed of equally spaced A6-sized metal plates (either copper or aluminum), screen-printed with a heart image in a range of colors and sizes. As part of the process, participants were invited to choose a plate that most resonated with them in terms of material and color, and they then embossed it with elements directly relating to their narrative. Small images delicately embossed around a central velvet-flocked heart thus become symbols for ambitions, memories, desires, and states of being. Shining suns, whirling spirals, spinach leaves, brick walls, the stripes of a tiger—they all tell stories. Burning flames speak of aspirations and resilience in adolescents that grow up with CHD. A spinach leaf tells a story of acceptance that a favorite food cannot be eaten because of anticoagulant medication. And interposed between these stories, the artist has placed direct quotes from conversations that took place on the ward or during workshops—very few, yet resonating, words: “My new heart is red and it pumps”; “My daughter was born with half a heart”; “My heart is a soldier, it’s been through wars but it’s still fighting, and I am really proud of it.” The result is less confrontational than an approach that may use a medium like photography to directly document or represent the effect of living with CHD. But it is the experiential component that shines (literally)—stories that have been collected and represented by an artist who effectively becomes a conduit for the stories themselves. In a country rich in gold, observant wayfarers may find nuggets on their path; but only systematic mining can provide the currency of nations. In the search for natural knowledge, the experimentalists are the miners.” There is a peculiar appropriateness in these words of Gowland Hopkins of Cambridge University at a time when men’s minds are diverted by the great emergencies of war and are upset by the distorted values of every-day life. Nowadays it would require a foolhardy person to maintain that “there is nothing new under the sun,” or to prophesy what cannot be done. The progressive evolution of science is expressed in the arts exemplified daily before us. We marvel at the newest phase of the conquest of the upper air or the ocean depths, at the irresistible force of the newest explosive or the perfection of the latest motor, all too often oblivious of the great strides that experimentation in medicine has brought about. It is always refreshing and not infrequently stimulating to indulge in retrospect. What currency have the experimentalists in medicine provided in the passing years? The history of bacteriology and the entire structure of the modern conception of infectious diseases is a triumph of the experimental method. The real causes of diseases known from earliest history have been elucidated so that the modern therapeutist can build on the basis of this newer knowledge, and the sanitarian can prepare himself effectively for the struggle against invasion by micro-organisms. More recently even the diseases caused by pathogenic protozoa have yielded their story to the experimentalist. Less startling, perhaps, but equally unique in its disclosures has been the discovery of the functions of internal secretion. The story of the pancreas, the thyroid structures, the suprarenals, the pituitary, the corpus luteum and the sex glands, each in turn presents a new point of view of regulation that takes us out of the domain of pure speculation into the field of biochemistry. It is only a few years at most since we were totally ignorant of these things. How imperfect and hampered medical thought would appear today without them! Added to this is the inroad already made in the experimental study of natural immunity to which clinical medicine already owes a great debt. We cannot expect the public to wax enthusiastic over something that those most concerned fail to appreciate; nor can we hope for their much needed support in any feature of medical progress that the physician himself does not heartily endorse. Experimentation in medicine belongs to a profession rather than to a trade, and therefore is not purely self-supporting or conducted solely for gain. But, as Hopkins has formulated the matter, it is not financial assistance alone that the nation should provide for the investigator. This is not even the most important stimulus that the nation can provide for him. Recognition and proper standing in the body politic are the investigator’s due, and these, at least, should be forthcoming. It is the most common type of high-altitude illness and occurs in more than one-fourth of people traveling to above 3500 m (11 667 ft) and more than one-half of people traveling to above 6000 m (20 000 ft). Symptoms include headache, fatigue, poor appetite, nausea or vomiting, light-headedness, and sleep disturbances. Symptoms usually occur 6 to 12 hours after ascent and can range from mild to severe. Symptoms usually improve after 1 to 2 days if there is no further ascent, but they can sometimes last longer. In less than 1% of cases, symptoms can progress to high-altitude cerebral edema, a life-threatening condition marked by symptoms of wobbly gait, confusion, and decreased consciousness. Acute mountain sickness is a clinical diagnosis based on typical symptoms in the context of ascending to high altitudes. Physical examination is usually normal. It is usually not difficult to diagnose in otherwise healthy adults; however, in children and people with baseline health problems, it can be more difficult to diagnose. No blood work or imaging tests are necessary, except to rule out other diagnoses. Symptoms usually improve after treatment with supplemental oxygen; sometimes this can be used as a diagnostic “test” for acute mountain sickness. People with acute mountain sickness should hold off on ascending to higher altitudes until their symptoms have resolved. Mild cases are usually treated with supportive care including rest, pain medications for headache, and hydration. More severe cases can be treated with oxygen given through a nasal cannula as well as with prescription medications such as acetazolamide, dexamethasone, or both. If symptoms are severe or persistent, descent is recommended. Source: Collet TH. Does this patient have acute mountain sickness? the rational clinical examination systematic review. JAMA. doi:10.1001/jama.2017.16192. To the Editor The SWITCH 1 randomized clinical trial compared the effect of 2 insulin analogues, insulin degludec vs insulin glargine, on the frequency of hypoglycemic events in patients with type 1 diabetes and risk factors for hypoglycemia.1 According to the study protocol, the insulin dose was titrated to reach fasting glucose values between 71 mg/dL and 90 mg/dL (3.9-5.0 mmol/L). Such low target values increase the risk of severe hypoglycemia. The American Diabetes Association2 and National Institute for Health and Care Excellence3 in the United Kingdom recommend fasting glucose values that range from 90 mg/dL to 130 mg/dL. Also, professional diabetes organizations suggest less-stringent hemoglobin A1c (HbA1c) targets and, thus, higher target glucose values for patients at risk of severe hypoglycemia to prevent this complication. In Reply The SWITCH 1 trial was the first trial, to our knowledge, to investigate hypoglycemia as a primary outcome in patients with type 1 diabetes, comparing insulin degludec with insulin glargine. Personalizing fasting blood glucose targets for patients remains important in clinical practice. However, in the trial, identical fasting blood glucose target goals and noninferior HbA1c values were imperative for analysis of the primary outcome of hypoglycemia. The protocol included algorithms for up- or down-titration of the insulin dose to achieve target fasting blood glucose levels of 71 mg/dL to 90 mg/dL; titration was conducted at the investigator’s discretion, so at no time was patient safety compromised. Targeting a higher fasting blood glucose would not facilitate achieving HbA1c values of less than 7%, as recommended by the American Diabetes Association1 or less than 6.5%, as recommended by the American Association of Clinical Endocrinologists.2 Achieving a fasting blood glucose target of less than 110 mg/dL and a HbA1c target less than 7% proved difficult in past treat-to-target trials using older insulins, and earlier landmark studies showed that intensive treatment aimed at reducing microvascular and macrovascular complications increased hypoglycemia risk.3 Aiming for higher blood glucose targets in the management of type 1 diabetes implies a conservative approach to avoid hypoglycemia, in part reflecting a past era of older insulins, which were shorter acting and with more variable and less-predictable time-action profiles than newer basal insulin analogues. When atypical antipsychotic drugs were first approved for use in patients with schizophrenia in the 1990s, their manufacturers initiated campaigns to promote their use for other off-label indications. Although physicians are free to prescribe medications off-label, advertising their use for nonapproved indications is illegal. Nonetheless, the major drug companies that manufactured antipsychotics improperly gave kickbacks to physicians for prescribing these drugs to nursing home residents for treatment of symptoms associated with dementia, or they dispatched special drug representatives or pharmacists to provide disinformation about antipsychotics to nursing home physicians until the Department of Justice put a stop to these practices. Bristol-Myers Squibb paid $515 million in 2007 to settle accusations that it had illegally promoted the drug Abilify (aripiprazole)2; Eli Lilly paid $1.4 billion in 2009 to settle criminal and civil charges that it illegally marketed Zyprexa (olanzapine)3; AstraZeneca paid $520 million in 2010 to settle charges of illegal marketing of Seroquel (quetiapine) in nursing homes4; and Johnson & Johnson paid $2.2 billion in criminal and civil fines in 2013 to settle accusations that it improperly promoted Risperdal (risperidone) to older adults in nursing homes.5. When atypical antipsychotic drugs were first approved for use in patients with schizophrenia in the 1990s, their manufacturers initiated campaigns to promote their use for other off-label indications. Although physicians are free to prescribe medications off-label, advertising their use for nonapproved indications is illegal. Nonetheless, the major drug companies that manufactured antipsychotics improperly gave kickbacks to physicians for prescribing these drugs to nursing home residents for treatment of symptoms associated with dementia, or they dispatched special drug representatives or pharmacists to provide disinformation about antipsychotics to nursing home physicians until the Department of Justice put a stop to these practices. Bristol-Myers Squibb paid $515 million in 2007 to settle accusations that it had illegally promoted the drug Abilify (aripiprazole)2; Eli Lilly paid $1.4 billion in 2009 to settle criminal and civil charges that it illegally marketed Zyprexa (olanzapine)3; AstraZeneca paid $520 million in 2010 to settle charges of illegal marketing of Seroquel (quetiapine) in nursing homes4; and Johnson & Johnson paid $2.2 billion in criminal and civil fines in 2013 to settle accusations that it improperly promoted Risperdal (risperidone) to older adults in nursing homes.5. In Reply We did not claim in our Viewpoint, nor do we believe, that the National Partnership to Improve Dementia Care in Nursing Homes has been the sole cause of the decline in antipsychotic use observed among nursing home residents over the past 5 years.1 What we did claim is that the evidence suggests that it played a significant role, and that, without it, progress would have been slower and less steady. In their letters, Dr Gillick and Dr Westbury raise some important contextual issues relevant to the findings reported in our Viewpoint. Gillick describes the marketing efforts of pharmaceutical manufacturers to promote non–FDA-approved (off-label) use of atypical antipsychotics and the large financial settlements that have been paid to federal and state governments for engaging in off-label promotion. Pharmaceutical marketing activities in nursing homes were quite common prior to the launch of the National Partnership to Improve Dementia Care in Nursing Homes.2 In the 2011 report of the Office of Inspector General, “Medicare Atypical Drug Claims for Elderly Nursing Home Residents,”3 the aggressive marketing efforts of the pharmaceutical industry were highlighted. However, although it is possible that large financial settlements against manufacturers and the cessation of illegal marketing practices contributed to reductions in off-label antipsychotic prescribing, evidence supporting this specific association is not available.4. Westbury comments on the exclusion of nursing home residents who carry a diagnosis of schizophrenia in calculating the prevalence of antipsychotic use, which is a publicly reported nursing home quality measure. Quality measures that are linked to specific diagnoses can create unintended shifts in diagnosis patterns that can offset apparent gains in quality.5 Westbury suggests that intentional diagnosis “shifts” to schizophrenia may be undermining the validity of the quality measure relating to antipsychotic use and place many nursing home residents at risk of indefinite treatment with antipsychotics without justification. In our Viewpoint, we specified a number of unanswered questions relating to the success of the CMS-led initiative to reduce excessive use of antipsychotic agents in nursing homes. The following question should be added to our list: Has the effect of the initiative been undermined by inappropriate use of schizophrenia diagnoses in nursing home residents who are receiving antipsychotics or being considered for treatment?. This was a phase 2b, multicenter, double-blind, placebo-controlled, dose-ranging study of epicutaneous immunotherapy with a peanut patch (Viaskin Peanut) for 1 year followed by a voluntary, 2-year, open-label extension study to evaluate the efficacy of the peanut patch. Patients and investigators were kept blinded to study treatment (phase 2b: July 31, 2012-July 31, 2014; extension completed September 29, 2016). The trial protocol (Supplement 1) and consent forms were approved by each center’s institutional review board. Written informed consent was obtained from all study participants or parents/guardians with assents for children older than 7 years or per local institutional review board guidelines. Standardized food challenges using PRACTALL criteria12 were conducted before initiating therapy and following 12, 24, and 36 months of daily peanut-patch application. Incremental peanut protein doses of 1, 3, 10, 30, 100, and 300 mg every 30 minutes were used for all challenges, with additional doses of 1000 and 2000 mg of peanut protein for challenges at 12, 24, and 36 months. An additional dose of 1600 mg of peanut protein was administered at 24 and 36 months. Peanut protein doses were administered in a standardized chocolate pudding.13 Food challenges were discontinued and eliciting doses were established only when clear-cut objective symptoms were present (eTable 1 in Supplement 2).12 Once qualified, patients were randomly assigned at a ratio of 1:1:1:1 to receive patches containing 50 μg, 100 μg, or 250 μg of peanut protein or placebo patch. Randomization was stratified by site and age group using a dynamic randomization schedule through Interactive Web Response Systems: children aged 6 to 11 years and adolescents/adults aged 12 to 55 years (treatment block size was 4). Patients and their families were reimbursed for travel cost limited to US$40 per visit; there were no additional financial incentives. Peanut and placebo patches, which were indistinguishable in appearance, were applied daily on the backs of children and inner upper arms of adolescents and adults. The first patch was applied under observation at the study site. Subsequent patches were self-administered at home once daily for 3, 6, and 12 hours per day during the first, second, and third weeks, respectively, followed by 24 hours daily thereafter. Patients were seen at 3, 6, and 12 months, at which time skin prick tests and serum immunoglobulin levels were repeated (eAppendix in Supplement 2). Blood was also collected to screen for filaggrin gene mutations, which have been associated with defective skin barrier and atopic dermatitis.14 Patients who had received a placebo patch in the first year were initially rerandomized to 50-μg, 100-μg, or 250-μg doses at entry into the 2-year, open-label extension; at 6 months, all were switched to 250 μg, which was found to be the most efficacious dose. Patients were instructed to refrain from peanut consumption throughout the trials, except during the food challenge. Secondary efficacy end points included the percentage of responders in each of the 2 predetermined age strata (6-11 years [children] and 12-55 years [adolescents/adults]); mean cumulative reactive dose (sum of all food challenge doses received at development of objective clinical symptoms) at month 12 and change from baseline; and changes in severity of symptoms (sum of all symptoms during food challenges), skin prick test wheal size (wheal diameter measured successively for undiluted and 1/10, 1/100, 1/1000, and 1/10 000 dilutions), and serum peanut-specific IgE and IgG4 levels following 12 months of treatment (4 additional secondary end points not presented in this report are listed in the eAppendix in Supplement 2). Safety end points included the type, frequency, and severity of treatment-emergent AEs (TEAEs), serious AEs, and premature discontinuation. Compliance was defined as the total number of patches dispensed minus the number returned, divided by the number of days within the treatment period. Patients graded application site skin reactions (erythema/redness, pruritus/itching, or edema/swelling) or cutaneous symptoms daily for the first 3 months and whenever symptoms occurred thereafter. The grading scale for each symptom ranged from 0 to 3 (0 = absent; 1 = mild; 2 = moderate; and 3 = severe) (eAppendix in Supplement 2). Skin symptoms were also assessed by investigators at each study visit. End points for the 2-year open-label extension were the same as for the randomized trial. The overall type I error rate for the primary analyses was controlled through a prespecified fixed-sequence testing strategy (pairwise comparisons between 250 μg, 100 μg, and 50 μg vs placebo). Accordingly, testing would cease beyond the first observed P value less than or equal to .05. All other outcomes in this study were considered exploratory. However, an interaction between age categories (ie, children and adolescents/adults) and each treatment group (vs placebo) for the primary end point was carried out to identify the need for any subgroup analysis. For the primary end point, if the month 12 eliciting dose was missing, the patient would be defaulted to count as a nonresponder (last-observation-carried-forward). Treatment effects on eliciting dose, cumulative reactive dose, skin prick test, and immunological markers (IgE, IgG4) were compared with placebo. Adverse events reported in patients who had at least 1 patch application were summarized (eAppendix in Supplement 2). For the 2-year, open-label study, variability estimates (confidence intervals or quartile [Q] 1, Q3) were presented for descriptive purposes. No tests of significance were conducted. SAS (version 9.4; SAS Institute) and Stata/IC (version 15; StataCorp) were used for all analyses. A total of 221 patients (median age, 11 years [Q1, Q3: 8, 16]; 37.6% female) were randomized (53, 56, and 56 patients to 50-μg, 100-μg, and 250-μg peanut patches, respectively, and 56 to placebo patch) across 22 study sites (Figure 1). Baseline distributions of age, peanut-specific IgE or IgG4 levels, skin prick test wheal diameter, or peanut protein eliciting doses in the 4 study groups were balanced (Table 1). There were 113 children (aged 6-11 years), 73 adolescents (aged 12-17 years), and 35 adults (aged 18-55 years) randomized; the median eliciting doses for children and adolescents/adults were 30 mg (Q1, Q3: 1, 100) and 100 mg (Q1, Q3: 30, 300), respectively (eFigure 2 in Supplement 2). At 12 months, overall compliance with treatment in the phase 2b trial was 97.6% (Q1, Q3: 93, 100); 6.3% of patients discontinued the study prematurely. Primary efficacy results on the intention-to-treat population are presented in Table 2. The observed month 12 absolute difference in response rates between the 250-μg patch (n = 28; 50.0%) and placebo patch (n = 14; 25.0%) was 25% (95% CI, 7.7%-42.3%; P = .01), thus achieving statistical significance. This corresponded to an estimated number-needed-to-treat of 4. The response rate for the 100-μg patch was 41.1%, for a difference from placebo of 16.1% (95% CI, −1.1% to 33.2%; P = .11), which failed to achieve statistical significance. Hence, in concordance with the prespecified fixed-sequential testing strategy, a formal 50-μg placebo hypothesis test was not conducted. All TEAEs are summarized in Table 3. During the phase 2b period, occurrence of TEAEs and TEAE event rates were balanced across all peanut patch–treated groups. TEAEs related to the investigational product occurred approximately twice as often in the peanut-patch groups when compared with the placebo-patch group: 96.2% for the 50-μg peanut patch, 94.6% for the 100-μg peanut patch, and 96.4% for the 250-μg peanut patch vs 48.2% for the placebo patch, primarily during the first months of treatment. TEAEs leading to study discontinuation were rare. Serious TEAEs occurred infrequently in all treated groups. The rate of patients with more generalized allergic TEAEs was approximately 25%, including mostly cutaneous reactions extending beyond the borders of the patch (approximately 18%). One case of nonserious moderate anaphylaxis was reported as possibly related to therapy (eAppendix in Supplement 2). Overall, 20 serious AEs were recorded in 17 patients, none related to the study drug (14 during food challenges). Three patients experienced serious AEs of moderate severity following accidental peanut ingestion, resulting in visits to an emergency department: a 6-year-old child with the 50-μg peanut patch and 2 adults, 1 with the 50-μg peanut patch and 1 with the 250-μg peanut patch; all were discharged several hours later after receiving an epinephrine injection. In addition, no differences in AEs were identified in patients with atopic dermatitis or with heterozygous (25 patients [15.8%]) or homozygous (2 patients [1.3%]) filaggrin gene mutations. Of the 207 patients completing the 1-year, blinded phase 2b trial, 171 (82.6%) were enrolled in the 2-year, open-label extension: 97 of 113 children (85.8%) and 74 of 108 adolescents/adults (68.5%). Within 6 months of completing the phase 2b trial, all enrolled patients were transitioned to the 250-μg peanut patch for the remainder of the study. Of 171 patients enrolled in the open-label extension study, 3 withdrew from the study before receiving treatment with the 250-μg patch. Of 168 patients who received the 250-μg patch in the open-label extension study, 57 switched to the 250-μg patch at month 6: 22 patients who received the 50-μg patch in the phase 2b study received the 50-μg patch at open-label extension entry before switching to the 250-μg patch at month 6; 20 patients who received the 100-μg patch in the phase 2b study received the 100-μg patch at open-label extension study entry before switching to the 250-μg patch at month 6; 7 patients who received the placebo patch in the phase 2b study received the 50-μg patch at open-label extension entry before switching to the 250-μg patch at month 6; and 8 patients who received the placebo patch in the phase 2b study received the 100-μg patch at open-label extension entry before switching to the 250-μg patch at month 6. All patients underwent a food challenge at months 12 and 24 of the extension. Based on the per-protocol population, the response rates at months 12 and 24 in the overall population were 59.7% (89/149) and 64.5% (80/124), respectively. During the open-label extension, 54 of 171 patients overall (31.6%) discontinued for various reasons, 2 (1.2%) because of AEs (including 1 TEAE). Median treatment compliance during the open-label extension was 95.5% (Q1, Q3: 89, 99). Occurrence of TEAEs was 93% (159/171) and 62% (106/171) in years 1 and 2 of the extension, respectively (Table 3). Local skin reactions decreased over time but continued to be the most common adverse symptoms reported. During the extension, TEAEs, severe TEAEs, and TEAEs related to the investigational product occurred largely during year 1 (93%). The 25% response rate of placebo-patch patients was considerably greater than the projected rate (10%) and higher than reported in other studies. The reason for this finding is not clear but may be owing to the challenge procedure and prespecified response criteria. To our knowledge, this is the largest trial to use the PRACTALL food challenge guidelines,13 which initiate food challenges at very low doses (1 mg) of food protein and then increase the dose by semilog quantities. The variability of response to food challenges at such low doses (doses at which the highest rate of placebo response occurred in this study) has never been evaluated, especially in the adolescent/adult group. Also, most placebo responders fulfilled the criterion of a 10-times increase over the baseline threshold challenge dose, which was adapted from the National Institutes of Health–sponsored Consortium of Food Allergy Research trial of epicutaneous immunotherapy16 and which may not have provided adequate stringency for response. This study has several limitations. First, the primary end point (10-times increase in challenge threshold) may not have been sufficiently stringent for the lowest food challenge doses (1, 3, and 10 mg of peanut protein), which contributed to the higher than expected rate of placebo responders. Second, the sample size of each treatment group was relatively small and therefore, the study was not powered to detect a dose-response gradient. Third, the study was not designed to detect an age effect independent of a treatment effect, and the interaction by age group was not significant for the 2 lower doses. Fourth, the open-label extension had no placebo control. Fifth, exclusion of patients with a history of severe anaphylaxis (as done in all other food immunotherapy trials that include food challenges) may influence the results of the study, especially those related to safety and tolerability end points. In Reply In our study, the type of hearing loss was limited to sensorineural hearing loss, which is commonly associated with the age-related hearing loss that an over-the-counter option (ie, PSAPs) would target. In addition, our study used real-ear measures, considered an aspect of best-practice clinical hearing aid fitting, to address the customization in each individual ear. We did not include loudness discomfort level. Although loudness discomfort levels have been previously reported as a common aspect of hearing aid fitting, the most recent review revealed low and limited scientific evidence of their effectiveness in hearing aid fitting.1. Dr Casale and colleagues raise the point that low hearing aid uptake is driven by cost, awareness of benefit, and inadequate customization. An over-the-counter option for hearing care could feasibly address these concerns. Over-the-counter devices could conceivably drive down prices and increase awareness with marketing and visibility in storefront and online sales. Moreover, some innovative PSAPs are already using at-home hearing testing via a smart phone to conduct in-situ fittings to prescriptive targets, thus addressing change in volume issues from device insertion. Since the publication of our Research Letter, the bipartisan Over-the-Counter Hearing Aid Act of 2017 (HR 1652, §670), which instructs the US Food and Drug Administration to create a separate regulatory classification for over-the-counter hearing aids, has been passed in both houses of Congress and signed by the president. In the Original Investigation article entitled “Effect of Routine Low-Dose Oxygen Supplementation on Death and Disability in Adults With Acute Stroke: The Stroke Oxygen Study Randomized Clinical Trial”1 published in the September 26, 2017, issue of JAMA, incorrect data were reported. In Table 1, the SI conversion factor should have read “To convert glucose to mmol/L, multiply values by 0.0555.” In Table 2, some values, reported as 99% CIs were actually 95% CIs. The correct 99% CIs for the Barthel ADL index should be 70.2 (68.2 to 72.2) in the continuous oxygen group, 71.1 (69.1 to 73.1) in the nocturnal oxygen group, and 70.9 (68.9 to 72.8) in the control group; for the Nottingham Extended ADL, 9.66 (9.29 to 10.02) in the continuous oxygen group, 9.54 (9.17 to 9.90) in the nocturnal oxygen group, and 9.77 (9.40 to 10.14) in the control group; for VAS for quality of life, 55.4 (53.8 to 57.1) in the continuous oxygen group, 55.7 (54.1 to 57.3) in the nocturnal oxygen group, and 55.5 (53.8 to 57.1) in the control group; for highest systolic BP within 72 hours, −1.96 (−3.48 to −0.44) in comparison 1; for highest diastolic BP within 72 hours, −1.10 (−2.06 to −0.15) in comparison 1; for highest temperature within 7 days, 0.01 (−0.03 to 0.04) in comparison 1; and for serious adverse events, 0.94 (0.78 to 1.13) in comparison 1 and 1.19 (0.96 to 1.47) in comparison 2. In Figure 3, there were 4703 patients in the continuous or nocturnal oxygen group who did not have congestive heart failure, and for patients in the continuous or nocturnal oxygen group on whom thrombolysis was performed vs not, the P value test for interaction was .40. Changes to these reported statistical measures do not affect the conclusions of this study. This article was corrected online. We calculated the proportion of survey participants with an HbA1c level less than 7% (good control), less than 8% (moderate control), and greater than 9% (poor control). We also calculated the proportion of participants who reported having a test for HbA1c in the prior year and who were aware of their HbA1c result and their target HbA1c level. We analyzed results by subgroups of age (20-44, 45-65, and >65 years), race (white vs nonwhite), and sex. Survey weights were used to calculate proportions. P values for trend over the 4 survey periods were calculated using a multilevel logistic regression in STATA (StataCorp), version 14. Age, sex, body mass index, race, insurance status, education, and income3 were included as covariates of the first level, and survey period as the dependent variable of the second level. No covariates were included for the subgroup trend analyses. Significance was defined as a 2-tailed P value less than .05. Of 23 482 participants, 2908 (12.40%) reported having diabetes (mean age, 62 years [SD, 12.96]; women, 50.40%; white race, 35.80% [unweighted]). Median HbA1c level was 6.90% (interquartile range [IQR], 6.20%-8.15%) in 2007-2008 and 6.95% (IQR, 6.30%-8.20%) in 2013-2014 (P = .39) (Table 1). Glycemic control did not change between 2007-2008 and 2013-2014 overall or in any subgroup (Table 1). Over time, a higher proportion of patients with diabetes reported having an HbA1c test within the past year; 55.10% (95% CI, 49.69%-60.50%) in 2007-2008 to 77.78% (95% CI, 75.07%-80.48%) in 2013-2014 (P value for trend <.001) (Table 2). All subgroups showed a similar change. In 2007-2008, 52.32% (95% CI, 47.63%-57.00%) of patients reported being aware of their past year HbA1c result. This proportion increased to 74.31% (95% CI, 69.63%-78.99%) in the 2013-2014 survey (P value for trend <.001). The change was statistically significant in all subgroups except participants aged 20 to 44 years (P value for trend = .16). The proportion of participants who were aware of the target HbA1c level set by their clinician increased from 2007-2008 (74.07% [95% CI, 70.48%-77.66%]) to 2013-2014 (89.70% [95% CI, 86.11%-93.29%])  (P value for trend <.001). The change was statistically significant in all subgroups except participants aged 20 to 44 years (P value for trend = .81) (Table 2). The improvement in glycemic control between 1998 and 2010 among patients with diabetes3 appears to have plateaued during 2007-2014. Approximately 1 in 7 patients with diabetes had poor glycemic control (ie, HbA1c level >9%) throughout the study period despite an increase in the frequency of self-reported HbA1c testing and patient awareness of HbA1c result and patient-specific targets. One potential explanation is that in 2012 the American Diabetes Association set a higher target HbA1c level (8%) for people who are older, frail, and have many comorbidities or diabetes complications6; however, no change in glycemic control was found in the elderly subgroup in this study. Individualization of the target HbA1c level may explain the improvement in HbA1c testing and awareness of HbA1c targets over time in all subgroups except for patients younger than 45 years. Therefore, focusing attention on this subgroup may be important especially because they would benefit most from treatment. Limitations of this study included the cross-sectional study design; therefore, inferences should be made cautiously. During the Las Vegas mass shooting, nearly 500 injured persons sought medical attention. University Medical Center, Nevada’s only level I trauma facility, received 104 wounded patients in the first hours. These numbers create a significant increase in the accumulating 2017 tallies of US gun violence injuries and deaths; to date, more than 11 000 firearm-related deaths and nearly 24 000 nonfatal gunshot wounds have occurred. While on an average day 93 US residents are killed by guns, at least twice as many are injured (https://everytownresearch.org/about/). Accumulating evidence suggests that the toll of nonfatal firearm injuries is extraordinary, with the consequences of firearm injuries resulting in higher rates of rehospitalization and chronic disability than seen for those with unintentional injuries such as motor vehicle survivors. Simply put, firearm injuries have devastating, long-term effects on those who are shot. While media narratives after mass shootings appropriately focus on the legislative inaction that leaves the door open for such events to happen, a growing literature provides evidence of approaches to help mitigate the consequences. The outreach strategy that was implemented following the deadliest mass shooting on record, the massacre on Utøya Island, Norway, may be of interest to health professionals and decision makers.6 Three main principles guided the program: proactivity, continuity, and targeted interventions.6 Municipal crisis teams made proactive contact with nonhospitalized survivors and their families, as well as with bereaved families. Hospitalized survivors were contacted after dismissal from the hospital.6-8 To ensure continuity, a designated individual had the role of “contact person.” Mental health needs were monitored via use of a screening tool (based on the UCLA PTSD Reaction Index with additional questions on depressed mood, problems with daily functioning, and problems with social interaction) for mental health problems,  alertness for sleep problems, social isolation, and difficulties coping with daily life activities. The majority of shooting survivors and their families, as well as bereaved families,  were successfully contacted, the contact was assessed to be acceptable and appreciated, and most survivors were connected with specialized mental health services.6-8. The Utøya interventions were embedded into Norway’s national public health system. The United States currently has no organized system to deliver such approaches. In the Utøya intervention, postintervention evaluations showed that the time needed for improvement was longer than anticipated and interventions, for example at schools, were terminated too soon.7 However, given the frequency of repeated mass shootings and the pervasiveness of community gun violence in the United States,1 a discussion regarding the design and funding for initiatives that reach gun violence–affected communities rapidly, and maintain psychological support and referral services long term, is a national imperative. Funding for firearm research has been substantially hampered by a 1996 amendment to an omnibus spending bill that barred firearm research that can be used for policy advocacy. This has cast a pall on potential funding for the field. The public conversation around firearms and mass shooting incidents tends to coalesce around arguments that weigh the rights of gun owners vs the needs to ensure the safety for the collective, often leaning on the Supreme Court decision in District of Columbia v Heller as the definitive word in the direction of the rights of gun owners.9 The Heller decision ruled that the Second Amendment protects an individual’s right to possess a firearm for self-defense and Washington, DC’s ban on handguns was accordingly found unconstitutional. However, that conversation tells only a very selective story. The Heller decision reflects a particular interpretation of the Second Amendment, decided by a 5:4 split decision falling along strict political lines.9 Perhaps as important, that decision, and the sharp turn toward privileging individual gun ownership over collective safety, comes after 3 decades of advocacy by particular corporate interests whose incentive is to manufacture and sell more firearms. A discussion around firearms and how to mitigate their consequences cannot happen absent a reckoning with the undue influence that private interests have played in the national discussion, resulting in a highly tilted public conversation.9 Although scholarship is slowly emerging that sheds light on corporate activities that have led to the current situation, this work represents drops in the bucket compared with 30 years of focused promotion of the interests of a particular industry.10 Any consideration of potential solutions to the public health challenges that firearms present must balance and offset the powerful influences of commercial interests in that discussion. Mass shootings are inescapably horrific and provoke needed attention to the challenge of firearm mortality and injury in the United States and other countries. Yet these rampage shootings remain a very small fraction of all firearm deaths and injuries. The important public and scientific discussions that emerge after each of these shootings should be encouraged as a way toward finding solutions. However, the discussion will be better served if complemented by a focus on the issues highlighted herein that are as important, if not more so, than many aspects of the firearm epidemic that currently dominate public discussion and debate. Although little appreciated by the public, many of whom erroneously believe that nicotine causes tobacco-related cancer, nicotine per se is not especially toxic at the doses at which people self-administer it. The real danger comes through the effects of nicotine in causing addiction to smoking, which disperses more than 7000 chemicals through the lungs into the bloodstream and then to every body organ. Seventy of those chemicals are known human carcinogens; others cause cardiovascular and pulmonary diseases, and numerous other disorders, that affect large proportions of long-term smokers. FDA’s emphasis on the centrality of smoking in tobacco-produced disease echoes the 2014 surgeon general’s report: “The burden of death and disease from tobacco use…is overwhelmingly caused by cigarettes and other combusted tobacco products; rapid elimination of their use will dramatically reduce this burden.”2. The new FDA plan complements these measures, charting a novel and creative course. It includes several components, the most unconventional of which may depend on what might be considered the second most unconventional. The surprising proposal is to examine the potential benefits and adverse consequences of mandating reduction in cigarette nicotine yields to levels incapable of sustaining addiction. FDA has the legal authority to mandate a reduction. Making cigarettes and all combustible tobacco products nonaddictive would markedly reduce the number of smokers in future generations and would assist current smokers in quitting. In surveys, substantial proportions of smokers support such a policy. Still, nicotine reduction could have serious unintended consequences, including an illegal market of nicotine-containing cigarettes. To minimize that risk, and more generally to address the nicotine needs of smokers, FDA is committed to encouraging product innovations that offer smokers alternatives to cigarettes, consisting of consumer-acceptable nicotine-delivery products that present far less health risks than smoking. From a pragmatic perspective, and a moral one, FDA recognizes that its most radical concept of reducing nicotine in combustible tobacco products to nonaddicting levels will necessitate the availability of such alternative products. Toward this end, the agency is extending a near-term deadline for the submission of tobacco product review applications for novel products, including electronic or e-cigarettes. The high cost of preparing such applications might make them prohibitively expensive for all but the major tobacco companies, thereby reducing the potential for product creation from smaller, more innovative companies. With this in mind, FDA will review the applications’ requirements. Reduced-risk products are available today, albeit with little regulation thus far, except in the case of FDA-approved nicotine replacement products. The content and marketing of some of these products, and even their availability, are controversial. The e-cigarette is the controversy’s poster child, with a wide range of issues debated, including whether flavor additives, which may attract children, should be banned or limited. While experts agree that e-cigarette use is less hazardous than smoking, estimates range from an overall health risk of 5% (or less)3 to a third (or more) that of smoking.4 Opponents are concerned that e-cigarettes cause never-smoking young people who use them to try smoking. Several prospective studies support this conclusion,5 although the studies have limitations.6. Yet cigarette smoking by students has declined at an unprecedented rate during the brief period of widespread use of e-cigarettes by adolescents. Based on a survey of more than 80 000 individuals from 2012 through 2015, the 30-day prevalence of e-cigarette use among high school students increased rapidly from 2.8% in 2012 to 16% in 2015.7 During the same years, their 30-day prevalence of cigarette smoking declined from 14% to 9.3%, representing the largest percentage decrease in student smoking in history. In 2016, e-cigarette use declined by nearly 30%, to 11.3%, whereas smoking continued its rapid decline, to 8%.7 Supporters of e-cigarettes consider these products to be the harbinger of an era of “disruption” of the cigarette market. They view novel products as providing smokers with attractive alternatives to existing pharmaceutical cessation aides. Recent studies credit e-cigarettes with increasing smoking cessation in the United States and England.8. To date, tobacco harm reduction—encouraging inveterate smokers to switch to less harmful nicotine-yielding products—has been opposed by many mainstream public health organizations. These include government agencies, such as the US Centers for Disease Control and Prevention and state health departments, and leading nongovernmental organizations. The dominant approach is “just say no” to all tobacco products, reminiscent of the war on drugs. Yet harm reduction has featured prominently in multiple areas of public health, including clean needle distribution to minimize the spread of HIV/AIDS; sex education for adolescents instead of abstinence only to reduce teen pregnancies (now at an all-time low) and sexually transmitted infections; methadone as a heroin substitute; motorcycle helmet laws; and designated driver programs. All of these approaches are or were controversial but have reduced the toll of the problems they address. The FDA plan accepts a role for harm reduction in tobacco control. Given opposition from much of the public health community, this approach will meet with resistance. It is made even more difficult by widespread public ignorance about the core underlying principle of the plan, the nicotine product continuum of risk. Members of the public likely do not appreciate the health hazard differences of combusted and smokeless tobacco products. Regular smokeless tobacco product use, especially low-nitrosamine products, confers no more (and possibly less) than 10% the mortality risk from cigarette smoking.9 Even the risk of oral cancer, prominently associated with smokeless tobacco in the public’s mind, is substantially higher from smoking than from smokeless tobacco products. Yet in a recent government survey of 3733 adults, two-thirds of the respondents answered No to the question “Do you believe that some smokeless tobacco products…are less harmful than cigarettes?”. Complete FDA plan implementation will take years, should it occur. Still, the fact that such a radical plan has been proposed is remarkable. For decades, the nation’s most lethal products—cigarettes—were one of very few consumer products not subject to federal government safety regulation. A wide range of health and safety laws would appear to apply to tobacco, but those laws specifically excluded tobacco products or were amended to do so after challenges. Prominent examples include the Controlled Substances Act of 1970, the Consumer Product Safety Act of 1972, and the Toxic Substances Control Act of 1976. This changed with passage of the 2009 Family Smoking Prevention and Tobacco Control Act (FSPTCA). The act authorized FDA to regulate cigarettes and other tobacco products. The agency’s 2016 “deeming” regulation extended the regulatory authority to conventional tobacco products not previously included and to novel products, including e-cigarettes. FSPTCA has given broad powers to regulate tobacco product manufacturing, distribution, and marketing with the objective of protecting public health. To date, the agency has done little to achieve this purpose, the result in part of profound bureaucratic, legal, and political obstacles. FDA did issue a regulation requiring new graphic cigarette pack health warnings, but tobacco industry legal action blocked implementation. (Graphic warnings now festoon cigarette packs in more than 100 countries.) The agency’s Tobacco Products Scientific Advisory Committee recommended removal of menthol from cigarettes—menthol being the one characterizing flavor not banned by FSPTCA—and the agency’s own study concluded that menthol likely posed a risk to public health. But FDA has not banned menthol in cigarettes. The FDA’s inability to have put in place regulations that will substantially reduce the toll of tobacco use should temper optimism about implementation of its ambitious new plan. Furthermore, while all members of the tobacco control community are likely to favor some plan elements, few are likely to support all. Still, the potentially enormous pay-off to full plan implementation warrants its serious consideration. Could a tobacco control community so long divided unite in supporting a comprehensive plan that might one day consign cigarettes, and their horrific toll of disease and death, to the dustbin of history?. Comprehensive preparedness is a multifaceted endeavor including global surveillance networks, health care infrastructure ranging from primary care centers to referral hospitals, health care workforce capacity, and engagement with affected communities. Governments, the United Nations, and other nongovernmental organizations have made important strides in these areas. For example, the World Health Organization’s International Health Regulations, updated in 2005 after the SARS-CoV epidemic, helped improve global disease surveillance.1 Individual nations worked together to build on this foundation, creating the multilateral Global Health Security Agenda (GHSA) to “prevent, detect and respond” to new threats. Nearly 60 nations including the United States have joined the GHSA, collaborating in multisectoral preparedness including enhanced capacity for surveillance and laboratory diagnostics.2. A critical component of effective pandemic preparedness is biomedical research, including domestic and international research capacity. The research enterprise complements other elements of preparedness by improving understanding of the pathogenesis of infectious diseases and by developing interventions in the form of diagnostics, treatments, and vaccines. The foundation of this work is a portfolio of basic research applicable to multiple pathogens of public health significance. Through these investigations, the research community develops an understanding of the microbiology and pathogenesis of known infectious diseases. Even for pathogens not yet identified as major human health threats, research on related organisms can bolster efforts in the event of an outbreak. When Zika virus emerged in the western hemisphere, investigators working on the closely related dengue flavivirus were quickly marshaled against Zika. The presence of active researchers with relevant expertise facilitated the rapid launch of the Zika research response. For example, applying knowledge gained from work with dengue and other flaviviruses, researchers rapidly developed mouse models that recapitulate critical aspects of Zika infection, including replication and disease in the fetus; these were subsequently used as surrogates to study congenital Zika syndrome. These tools have been used to evaluate treatments, including monoclonal antibodies capable of neutralizing Zika and protecting mouse pups.3 Evaluation in human trials is under consideration. In shaping the research agenda for pandemic preparedness, prediction of microbes likely to cause outbreaks is often more art than science; as HIV, SARS, and Zika have demonstrated, no single algorithm will “get it right” all the time. For this reason, several research approaches are pursued in pandemic preparedness, including (1) pathogen-specific work; (2) platform-based technology; and (3) prototype-pathogen efforts. Each approach has strengths and weaknesses. Vaccine-related efforts serve as examples for each approach. In platform-based technology, developers are agnostic about specific pathogens. Research instead focuses on the platform used to present a relevant immunogen to the host. Vaccine platforms such as viral vectors can be used with genetic material coding for the relevant immunogen against which an immune response would be directed. In theory, such a platform could be used to present the genes from a range of pathogens. In this area, preparedness efforts typically involve the development of the platforms themselves, including manufacturing capacity. Another approach using prototype pathogens can hasten the platform-based approach by prospectively filling research gaps necessary to advance successful candidates as efficiently as possible. In this approach, investigators would conduct countermeasure research for prototype pathogens, understanding that the prototype may not emerge as a threat but assuming that techniques would be applicable to closely related microorganisms (oral communication, Barney S. Graham, MD, PhD, and Nancy J. Sullivan, PhD, June 2017). One example is the flavivirus prototype. Zika virus was not on priority pathogen lists before 2015, and essentially no Zika-specific research had been undertaken at NIAID. However, because of an extensive research portfolio on related flaviviruses such as dengue and West Nile viruses, researchers were able to leverage approaches such as animal models, immunogenicity assays, and vaccine design elements to develop Zika vaccine candidates. In this regard, DNA vaccine development for Zika took 13 weeks to move from sequence selection to first-in-human trial, largely because of the “road map” that West Nile research provided. The vaccine candidate is currently in a phase 2/2b trial; however, further development and distribution will require a commercial partner. Adapting this model for the future, countermeasure development approaches could be mapped out for multiple prototype pathogens, and if that pathogen (or a related one) emerges, the community would be poised for rapid countermeasure development, evaluation, and implementation. While priority-pathogen lists might not reflect the next emerging threat, platform and prototype-pathogen approaches run the risk of taking too long. The most prudent path is to invest in research on all 3, bolstering the current ability to predict emerging infections, developing platforms that can be more rapidly adapted to new threats, and pursuing prototype-pathogen efforts to accelerate candidate development. However, broad availability of vaccines requires partnerships with industry, affected countries, and local communities. Moreover, even though considerable attention has been given to improved vaccine preparedness, solutions for treatments and diagnostics require further consideration, as both may play critical roles in any effective response. Investing in outpatient and community-based services can improve health and lower cost in the United States. Opportunities include enhancing primary care, expanding evidence-based behavioral health services, improving the coordination of care for patients with complex health needs, and linking patients to critical social resources. Yet because there is limited direct reimbursement for such efforts, major expansions depend on capturing savings. As a committee of the National Academy of Medicine recently noted: “In terms of sustainability, interventions that improve health and quality of care or reduce utilization and cost are only feasible to maintain if the provider is paid in such a way that profits (revenues minus costs) are higher with the intervention than without.”1. One of the primary goals of alternative payment models, including patient-centered medical homes and accountable care organizations, is to allow clinical systems to reap the rewards of investments in prevention. Most of the anticipated savings from new models of care are expected to accrue in the form of reduced inpatient utilization. However, if hospitals remain paid on a fee-for-service basis—either because they are not part of the alternative payment models or because the new approaches still reimburse by admission—then their balance sheets stand to lose revenue as preventable illnesses and their related complications decline. Such hospitals might compensate with additional admissions to keep their beds filled, undermining outpatient and community-based initiatives. An important priority is to ensure that hospitals are full partners in transformation efforts in US medicine. The Commonwealth Fund recently released a report that focused on the potential utility of all-payer global hospital budgeting for large, urban safety-net health systems.5 These critical community institutions often struggle to find sustainable funding for key investments in prevention. Their funding depends, directly and indirectly, on fee-for-service hospital revenue. For example, public hospital systems, such as NYC Health + Hospitals, receive most of their revenue through funding mechanisms tied to the volume of services provided. Much of this revenue comes from the Medicaid program in the form of indirect payments, which are generally determined by longstanding formulas largely based upon levels of inpatient utilization. A reduction in inpatient utilization, for various reasons, reduces revenue. It is often difficult for safety-net hospitals to compete with surrounding institutions and increase market share, which would help offset declining inpatient utilization and revenue. There are 3 steps to creating a system of all-payer hospital global budgeting. First, hospital leaders, in coordination with their communities and the public sector, develop a vision for transformation. The vision should set out critical health challenges, detail a plan for shifting resources to outpatient and community-based preventive services and partnerships, and explain the limits of current financial reimbursement systems to foster change. The vision should align with efforts by local physicians to obtain greater reimbursement under the Medicare Access and Chip Reauthorization Act of 2015 (MACRA). In addition, the vision should explain clearly that participation from all payers can best support the redesign of clinical services. Second, public officials design an all-payer global budgeting system. This requires designating a credible administering agency to set and update budgets, assign payments to payers, provide technical support, monitor data, and make needed changes. This system also requires identifying a reference population—based in geography or attribution or some combination—for each participating hospital to guide adjustments to the global budgets over time. Should a hospital attempt to game the system by diverting patients (eg, such as high-cost patients) elsewhere, the administering agency must be able to recognize the market shift quickly and apply substantial penalties, including making long-term adjustments to the hospital’s budget. A favorable environment also requires support from key state and federal policy makers. With a waiver granted by the Center for Medicare & Medicaid Innovation, the Medicare program can switch the basis of hospital payment from the current system of paying for each admission by Diagnosis Related Groups to a new approach of paying for an annual share of a global budget. Through a section 1115 demonstration waiver sought by the state, the Medicaid program can do the same. Moreover, Medicaid can provide indirect payments to safety-net hospitals through their global budgets, rather than base payments on inpatient utilization, creating a common incentive structure for insured and uninsured patients alike.6. There are alternatives to all-payer hospital global budgeting for aligning hospital incentives with outpatient and community-based prevention efforts. One is for hospitals to assume financial risk for all health care expenditures for a population of patients. Many hospitals are developing their own insurance products, joining or leading advanced accountable care organizations, and establishing the management infrastructure to take a percentage of premium from managed care organizations in the Medicaid program or private insurers. A key question, however, is whether these innovations cover enough patients to realign the hospital’s overall incentives. If not, a majority of the patients admitted to a hospital might not be covered by the new incentives, and there is little likelihood of true transformation and long-term cost savings without the addition of a global hospital budgeting approach. The package was in plain brown paper wrapping. The note enclosed read, “Thanks for everything you did for my father. It meant more than you can ever imagine.” I teared up, remembering the day I helped the father of a colleague die peacefully. When I opened the wrapping and saw the picture of two saplings planted side by side and a certificate saying that the family planted one in my honor next to the one in their father’s, the single tear turned to a river. Although I never thought that a patient’s death would be one of the best clinical experiences of my then young career, that day increased my love for practicing medicine significantly. Early in my second year, I admitted a patient with an emphysema exacerbation. I was supposed to bring him with me to morning report and present his care to the chief of medicine the next day. But the patient’s tobacco addiction was not tempered by his need to be attached to an oxygen tank, so the night before my presentation, he lit up an unfiltered Camel with his oxygen on. The next morning, since my patient was in the burn unit, I presented a case of “spontaneous human combustion,” bringing in a gurney with a pile of ashes on the pristine white sheet. In my postcall delirium, I thought this was ironic and hilarious. The chief resident and the chief of medicine deemed it unprofessional, and I was duly chastised. This did not stop me, however, from writing an order for “Adolph’s Meat Tenderizer to bedside” when I admitted a man who had aspirated a piece of meat, requiring a bronchoscopy for removal, or an order for “standing and squatting stool velocities” when a nurse called me one too many times to report a patient’s diarrhea that I had already begun to treat. She said she did the standing velocity but couldn’t get him to squat. I still wonder how she performed the test. In my estimation, it would have required a stop-watch and a radar gun, which I’m pretty sure we didn’t have on the unit. I may still snicker about that, but I knew all along that my behavior was disrespectful and impeded good care for my patients. I just didn’t really care in those moments. By the time I made it through the third year of residency, I was completely ossified by these survival tactics. There was not much left for reflecting on the nature of healing or on the mysteries of life and death. My colleague’s father was an older gentleman, originally from Ireland, but had been in the states for decades before his family finally convinced him to see a doctor. More specifically, to see me. He was a true raconteur for the old country, with a thick brogue, and a heart battered by years of neglect. He’d had a taste for whiskey, salt, and fat and never a thought about exercise. One evening, after another day of unsuccessfully trying to wean down the drugs that not only kept him tethered to life but also tied to the hospital, I stopped by his room. I had planned a brief visit, but when I walked in, I immediately saw his distress. He sat fully upright, working to breathe. His wife sat stoically by his side. I had barely entered when he said, “I’ve had enough.”. I really didn’t know what to say. We’d had furtive conversations before about what he would want when…, but when had become now. How to help a patient at this juncture had not been part of my training. There was no guidebook for this. Palliative care and hospice care were not yet tangible forces. But I did know how to listen. Through tears, the patient told me he’d had a great life with a wonderful wife, kids, and grandkids. This, though, this not-quite-living was not what he had signed up for. He and his wife had agreed that it was time for the suffering and interminable hospital admissions to end. It was time for him to say goodbye. I gathered the team: my colleague, the patient’s cardiologist, the head nurse of the CCU. Together with the patient and his wife, we made a plan. The next day, we were all jammed into his hospital room, along with his extended family and a priest. As I began to increase the morphine running into his IV, his dyspnea decreased, and he seemed more comfortable and peaceful than he had in months. I guess it was a little like a wake before the actual wake. I had never been to a wake before, let alone one in the hospital, but the stories and laughter and spirituals on that day seemed to transcend any particular religion. Each family member sat by his side in turn, talking to him and holding his hand, until finally it was his wife’s turn. He had a look on his face that I can only call serenity as she held him, and then it was over. We all hugged and thanked each other, and then I left the room. I think it was the blessing and the tragedy of this wonderful man’s life and death that finally reminded me why I went into medicine in the first place. I felt something long-frozen inside me melt. I could finally leave the gallows humor and bad-boy pranks behind me. Participants completed at least 1 assessment after randomization and were included in the primary analysis. Error bars indicate SDs, which were calculated separately for each time point. Each of the 16 QIDS-C16 items can yield a score of 0 to 3 on a Likert scale. The score range is 0 to 27; higher scores indicate more severe depression; a score of 0 to 5 corresponds to a normal affect; 6 to 10 to a mild affect; 11 to 15 to a moderate affect; 16 to 20 to a severe affect; and 21 or greater to very severe depression. After randomization, participants attended clinic visits every 2 weeks for 6 weeks, then every 3 weeks for the remaining 6 weeks during the double-blind phase of the study (eFigure in Supplement 3). Dose was escalated by 50 mg/d at each successive visit to a potential maximum dose of 200 mg/d based on tolerability and response using a previously described measurement-based care protocol.12,13,15,19 The dose was kept constant for the last 6 weeks of the study. Adherence was ascertained by using pill counts at each visit, with nonadherence defined as having taken less than 80% of the drug. After 12 weeks, the drug was tapered at a rate of 50 mg/wk until completely discontinued. Participants were reassessed for depression 2 weeks after discontinuation and offered open-label sertraline or other therapy, which was managed by primary care or mental health physicians. Quiz Ref IDAdults with non–dialysis-dependent CKD stage 3, 4, or 5 (estimated glomerular filtration rate <60 mL/min/1.73 m2) were invited to participate. The inclusion criteria were changed to an estimated glomerular filtration rate of less than 45 mL/min/1.73 m2 early during the study in response to the National Institutes of Diabetes and Digestive and Kidney Diseases recommendation to reduce sample heterogeneity. There was no lower eligibility threshold for estimated glomerular filtration rate. Patients were excluded for the following reasons: inability to provide consent; having a functioning kidney transplant or requiring maintenance dialysis; having a transaminase elevation 3 or greater times the upper limit of normal; receiving current treatment with an antidepressant or other serotonergic drugs, or previous sertraline treatment failure; receiving psychotherapy for depression within 3 months; having psychosis or bipolar disorder, dementia, or suicidal intent; or being pregnant. Race and ethnicity were recorded by self-report based on fixed categories to describe the diversity of participants. Patients were randomized in a 1:1 ratio using block randomization (block size range, 4-8) based on a computerized random-number generator, stratified by hospital site ([1] the University of Texas Southwestern Medical Center and Parkland Hospital or [2] the Veterans Affairs North Texas Health Care System) and CKD stage (3, 4, or 5). Treatment assignments were conducted by research pharmacists at each site using the prespecified random allocation sequence. Participants, researchers, and clinicians were blind to treatment assignment. The QIDS-C16 outcome measure was administered by trained personnel who were blind to treatment assignments and measurement-based care algorithms. Using the modified intention-to-treat principle, participants were analyzed in the groups to which they were randomized if at least 1 QIDS-C16 assessment after randomization was available. The QIDS-C16 score was compared between groups using a repeated-measures mixed-effects model, with treatment group as the between-participants factor and random intercept and slopes as within-participant factors. Baseline QIDS-C16 was included as a covariate. The interactions between hospital site and CKD stage (3 vs 4 or 5) by treatment group and time were tested but not retained unless significant. The proportion experiencing an adverse event; the maximum SAFTEE global assessment of adverse effects; and the maximal frequency, intensity, and burden of adverse effects at any visit were compared between groups using χ2 tests. A binary outcome (any adverse effect at any visit vs none) was created for each participant and compared between groups using the χ2 test. The mean number of worsening SAFTEE symptoms was compared between groups using a t test. Statistical tests were 2-sided and P < .05 was considered significant. The statistical analyses were performed using SAS version 9.4 (SAS Institute Inc). There were 201 patients (mean [SD] age, 58.2 [13.2] years; 27% female) randomized. The first participant was randomized in March 2010 and the last clinic visit occurred in November 2016. Of the 997 patients who qualified based on having a QIDS-SR16 score of 11 or greater and meeting other eligibility criteria, 697 did not provide consent. Of the 300 who provided consent, 261 met the Mini International Neuropsychiatric Interview MDD criteria and entered the placebo run-in. Sixty were excluded, leaving 201 to be randomized (Figure 1). Eight patients exited the study prior to the first QIDS-C16 assessment at week 2 and, therefore, had no outcome assessments after baseline and were excluded from the primary analysis. Therefore, 193 patients (97 in the sertraline group and 96 in the placebo group) constituted the modified intention-to-treat sample. The median treatment duration was 84.0 days (interquartile range, 83.0 to 87.0 days; between-group difference, 0 days [95% CI, 0 to 2.0 days], P = .19; Table 2). Altogether, 92% completed at least 6 weeks and 84% completed all 12 weeks of the study. The number of postrandomization visits and the proportion that completed 4, 6, 9, or 12 weeks of the study were not significantly different between the groups (Table 2). The median achieved sertraline dose was 150 mg/d (interquartile range, 100 to 150 mg/d; between-group difference, 0 mg/d [95% CI, −50.0 to 0 mg/d], P = .10). The mean percentage of drug taken ascertained by pill count was 94% in the sertraline group and 96% in the placebo group (between-group difference, −1.6% [95% CI, −4.7% to 1.4%], P = .29). Three participants in the sertraline group and 2 in the placebo group were nonadherent (P = .66). The binary mixed-effects model for remission showed no treatment group main effect (P = .57) or interaction with time (P = .58). The proportion with remission was 15.5% in the sertraline group and 14.6% in the placebo group (between-group difference, 0.9% [95% CI, −9.2% to 11.0%], P = .86; Table 3). The mixed-effects model for response showed no treatment group main effect (P = .54) or interaction with time (P = .97). The proportion of participants with a treatment response was 32.0% in the sertraline group and 25.0% in the placebo group (between-group difference, 7.0% [95% CI, −5.7% to 19.6%], P = .28; Table 3). There was no significant difference in change in patient-reported overall health on the KDQOL-SF (median, 0 vs 0; between-group median difference, 0 [95% CI, −10.0 to 0]; P = .61). The mixed-effects model for the physical component of the KDQOL-SF also found no significant treatment group main effect (P = .55) or interaction with time (P = .98). No differences were observed in quality-of-life components from baseline to study exit, except for improvement in sleep, which was significantly greater in those receiving sertraline (between-group median difference, 5.0 [95% CI, 0 to 12.5], P = .03; Table 3). The 12-week or study exit efficacy and quality-of-life measures for each treatment group are included in the eTable in Supplement 3. The groups did not significantly differ in the proportion that experienced 1 or more prespecified serious adverse events (Table 4). One participant receiving placebo developed acute suicidal intent; however, there were no suicides. Another participant receiving 200 mg/d of sertraline was hospitalized with elevated liver enzyme levels. Liver biopsy indicated drug-induced acute liver injury, and sertraline was discontinued. Between randomization and the first assessment of outcomes, 2 participants assigned to sertraline withdrew due to adverse effects (Figure 1). A high placebo response has been postulated to be associated with negative trials of antidepressant medications.35 However, the 4-point decrease in the QIDS-C16 score in the placebo group is lower than what is considered a “high” placebo response rate observed in depression treatment trials. A meta-analysis of 169 randomized placebo-controlled trials of antidepressant monotherapy concluded that a placebo response rate of less than 30% provided the best chance of finding efficacy in drug-placebo comparisons.36 Thus, a “high” placebo response rate is an unlikely explanation for a lack of sertraline efficacy in this study. If sertraline did have any effect, it was significantly smaller than postulated in the sample size calculations and clinically irrelevant. In addition, use of a measurement-based care design to bring each participant to the highest individually tolerable dose resulted in adequate dosing (median, 150 mg/d) compared with lower achieved SSRI doses in other large trials among patients with chronic diseases that did not show SSRI treatment efficacy compared with placebo. The mean (SD) sertraline doses were 65 (29) mg/d and 68.8 (40.1) mg/d in trials of patients with congestive heart failure36 and ischemic heart disease,12 respectively, and the mean (SD) escitalopram dose was 15.8 (6.4) mg/d in a more recent large congestive heart failure trial.14 Therefore, inadequate dosing would not explain the lack of effect, and it is unlikely that sertraline is effective for depression treatment in this patient population. This study has several limitations. Although the duration was longer than previous studies, it did not assess the long-term effects of antidepressant treatment in CKD. The duration was designed to minimize potential adverse events while allowing adequate drug exposure for a meaningful response. The largest trial of MDD treatment in the general population also assessed efficacy over 12 weeks and showed that a substantial portion of patients who achieved response or remission did so after 8 weeks of treatment with an SSRI.13 In addition, trials with longer durations (6 and 18 months) among patients with other chronic diseases did not reveal efficacy of SSRI vs placebo.14,36 The potential benefit of SSRIs for the treatment of more severe depression may also deserve exploration. Although patients with dialysis-dependent CKD were not included, there is a substantially larger population of patients with non–dialysis-dependent CKD, and the majority of patients with CKD die of cardiovascular disease before reaching end-stage kidney disease. Identifying interventions that could affect outcomes earlier during the disease duration is imperative. The efficacy and safety of interventions for the treatment of depression among dialysis-dependent patients, a population in which the burden of symptoms, demands of treatment, and propensity for adverse drug reactions are substantially higher than among patients with non–dialysis-dependent CKD, deserve further investigation. Although the current study involved a single geographic area, the demographic breakdown was racially and ethnically diverse, regionally representative, and consistent with published data from other large nationally representative CKD cohorts.40 The current study did include a larger proportion of black participants than previously reported in some CKD cohorts. However, given that the ratio of adjusted incidence rates for CKD in 2014 for blacks vs whites was 3.1, inclusion of a large proportion of blacks is clinically relevant.1 Twenty-two percent of study participants had a history of drug abuse; however, it is uncertain whether this factor is generalizable. The inclusion of non-Veteran patients from the University of Texas Southwestern Medical Center and Parkland Hospital increases the generalizability to females with CKD.1 Future studies need to explore whether non-SSRI medications or nonpharmacological therapies can result in improvement in depressive symptoms among patients with non–dialysis-dependent CKD, who are at a disproportionately higher risk for depression and its complications than the general population. A, The quartiles of time to diabetes were 18.4, 31.9, and 55.1 months for oral insulin and 18.9, 32.7, and 54.4 months for placebo. Of those in the oral insulin group, 140 participants did not develop type 1 diabetes and 58 did. Of those in the placebo group, 122 participants did not develop type 1 diabetes and 62 did. The median months of follow-up was 32.0 (1.87-114). B,  The quartiles of time to diabetes were 12.9, 19.2, and 52.8 months for oral insulin and 10.5, 18.5, and 39.8 months for placebo. Of those in the oral insulin group, 14 did not develop type 1 diabetes and 13 did. Of those in the placebo group, 8 did not develop type 1 diabetes and 19 did. The median months of follow-up was 18.9 (2.33-77.5). C,  The quartiles of time to diabetes were 25.3, 49.2, and 65.8 months for oral insulin and 18.1, 30.2, and 63.3 months for placebo. Of those in the oral insulin group, 40 participants did not develop type 1 diabetes and 11 did. Of those in the placebo group, 53 participants did not develop type 1 diabetes and 10 did. The median months of follow-up was 39.4 (6.21-115). D,  The quartiles of time to diabetes were 18.4, 34.9, and 59.7 months for oral insulin and 17.8, 31.2, and 54.0 months for placebo. Of those in the oral insulin group, 194 participants did not develop type 1 diabetes and 82 did. Of those in the placebo group, 183 participants did not develop type 1 diabetes and 91 did. The median months of follow-up was 32.4 (1.87-115). Potential participants were identified through participation in the TrialNet Natural History Study (subsequently renamed the TrialNet Pathway to Prevention Study).5 Nondiabetic relatives of probands with type 1 diabetes were screened at institutional review board–approved sites after informed consent was obtained. These included first-degree relatives (sibling, parent, or child) who were aged 3 through 45 years, as well as second- or third-degree relatives (niece, nephew, aunt, uncle, cousin) who were aged 3 through 20 years. Initial screening was for diabetes autoantibodies—antibodies to microinsulin, glutamic acid decarboxylase, and insulinoma-associated antigen-2. Islet cell autoantibodies were measured if at least 1 other antibody tested positive. Race/ethnicity was included as part of federal reporting requirements, based on participant self-report of fixed categories determined by the National Institutes of Health. Eligible participants had to have either islet cell autoantibodies (≥10 juvenile diabetes foundation units) confirmed positive on 2 sample collections, or if not confirmed for islet cell autoantibodies, both glutamic acid decarboxylase and insulinoma-associated antigen-2 autoantibody tested positive on the same sample with confirmation of at least 1 of these autoantibodies required on a separate sample. Participants had to have first-phase insulin release higher than the threshold determined from the sum of the 1- and 3-minute insulin values from an intravenous glucose tolerance test. For participants aged 3 through 7 years or parents of probands with type 1 diabetes, the threshold was 60 μU/mL or higher. For siblings or offspring aged 8 through 45 years or other relatives aged 8 through 20 years, the threshold was 100 μU/mL or higher. These were the same thresholds used in the DPT-1 study.1,2. Secondary Stratum 3: Identical to secondary stratum 2 except participants had first-phase insulin release lower than the thresholds defined in the primary stratum above. Other entry criteria included normal glucose tolerance by oral glucose tolerance test, or if they had a previous abnormal glucose tolerance, 2 consecutive oral glucose tolerance tests with normal glucose tolerance. Abnormal glucose tolerance was defined as it was in DPT-11,2 using the standard criteria in effect at the initiation of DPT-1: fasting plasma glucose of more than 110 mg/dL; and/or 2-hour plasma glucose of more than 140 mg/dL; and/or 30-, 60-, or 90-minute plasma glucose of more than 200 mg/dL (to convert glucose from mg/dL to mmol/L, multiply by 0.0555). After participants signed the consent form, completed screening visits, met all of the inclusion criteria and none of the exclusion criteria, and completed the baseline procedures, they were randomized in equal allocations to each treatment group via a computerized random-number generator. Randomization was stratified by study site, and block size was a variation of size 2 and 4. Randomization was not stratified by stratum. Treatment assignment was double masked. Outcome assessments were conducted without knowledge of treatment assignment. Participants were assigned to receive capsules of either oral insulin, 7.5 mg of recombinant human insulin crystals (Eli Lilly), or matched placebo. This was the same dose used in the DPT-1 study.2 Capsules were prepared with methylcellulose filler at a compounding pharmacy (Eminent Services Corp) and masked bottles were shipped to the clinical sites. All participants were requested to take 1 capsule of study medication daily for the duration of the study. Study medication was dispensed at each 6-month visit. Participants consumed the capsule as a single daily dose, either by taking the capsule or, if the participant could not swallow capsules, sprinkling its contents in juice or on food. Participants were seen every 6 months. At those visits, an oral glucose tolerance test was performed to assess whether diabetes had developed. Criteria for diabetes onset were, as defined by the American Diabetes Association (ADA), based on glucose testing or the presence of unequivocal hyperglycemia with acute metabolic decompensation.6 Specific criteria for diabetes onset is defined as the presence of symptoms of diabetes plus casual (random) plasma glucose of 200 mg/dL or higher or fasting plasma glucose of 126 mg/dL or higher, or 2-hour plasma glucose of 200 mg/dL or higher. The criteria must have been met on 2 occasions as soon as possible but no less than 1 day apart for diabetes to be diagnosed. It was preferred that at least 1 of the 2 testing occasions involved an oral glucose tolerance test. Tolerance tests were performed after an overnight fast. Blood samples were drawn through a temporary indwelling intravenous catheter. For the oral glucose tolerance test, the oral glucose dose was 1.75 g/kg (maximum, 75 g). Quiz Ref IDThe primary outcome was the elapsed time from random treatment assignment to the development of diabetes among those enrolled in the primary analysis stratum, consisting of participants with insulin autoimmunity and first-phase insulin release that was higher than the threshold. Secondary outcomes included the effects of oral insulin treatment vs placebo in each stratum and in all strata combined, the consistency of oral insulin vs placebo treatment effect among strata, various subgroup analyses, and longitudinal analyses to assess the effects of oral insulin vs placebo over time. Other secondary outcomes such as the association of demographic, genetic, immunologic, metabolic, and lifestyle factors are not reported herein. The study was designed as a maximum information trial, which did not include a fixed sample size. Instead, participants were recruited and followed up until the required amount of statistical information was achieved.9 At any point during the study, the information in the data for a log-rank test is provided by I = (DOI×DC)/(DOI+DC), for which DOI and DC refer to the number of participants who developed diabetes in the oral insulin and control groups, respectively. The information required to provide 85% power to detect a 40% risk reduction (identical to the DPT-1 Oral Insulin Trial)2 with a 1-sided log-rank test at the .05 significance level is I = 27.551. All but 7 participants contributed to the analysis in the primary stratum. No attempt was made to impute missing data and no adjustment has been made for multiple comparisons, except the interim monitoring and multivariate analyses. Consequently, all but the primary analysis should be considered exploratory. Except for the post hoc hazard rate comparison with the DPT-1 study, the primary and secondary analyses were prespecified, and the exploratory analyses were preplanned. Participants in this study were relatives of patients with type 1 diabetes who tested positive for multiple autoantibodies and had normal glucose tolerance at the time of randomization.  Screening of 138 385 individuals for potential enrollment began in March 2004. The first participant was randomized March 2, 2007, and the last participant, December 21, 2015. For the entire cohort, the randomization rate was a mean of 5 participants per month, and for the primary stratum, 3.5 participants per month. The database for this report was locked  January 20, 2017. Of those screened, 3583 (2.56%) tested positive for microinsulin autoantibodies so were potentially eligible for this study. Of these, 2068 tested positive for microinsulin autoantibodies (Figure 1). A total of 560 participants were randomized, including 389 in the primary stratum. In the secondary strata, 55 participants were randomized in secondary stratum 1, 114 participants in secondary stratum 2, and 2 participants in secondary stratum 3. In the entire cohort, the median age at enrollment was 8.2 years (IQR, 5.7-12.1 years), 60% were boys, more than 90.7% were non-Hispanic white, and 57.6% had a sibling diagnosed with type 1 diabetes. The Table shows the baseline participant characteristics by treatment group for the entire cohort, for the primary stratum, and for the secondary strata. There were no substantial imbalances between treatment groups. In an exploratory analysis of the primary stratum that included participants with 85% or more adherence with treatment (n = 215), the annualized rate of diabetes was 6.9% with oral insulin and 9.7% with placebo (HR, 0.69; 95% CI, 0-1.04; P = .06). In examining Kaplan-Meier curves for the primary stratum that included participants with varying degrees of adherence among more adherent participants, there was separation between the oral insulin and placebo groups during the first 24 months after randomization. Therefore, in an analysis of the first 24 months’ follow-up of adherent participants in the primary stratum, a protective association was observed among those taking oral insulin with fewer participants progressing to diabetes than among those taking placebo—the annualized rate of diabetes was 2.4% with oral insulin and 6.9% with placebo (HR, 0.348; 95% CI, 0-0.855; P = .02). This exploratory analysis is included in the eAppendix in Supplement 2. Because this study was undertaken to replicate the DPT-1 subgroup finding, the same 7.5-mg/d of oral insulin was used. However, immunomodulatory effects were reported with higher doses of oral insulin in preclinical studies and in a small-dose escalation study of children early in the disease process. The Pre-Point study11 found potentially protective immunomodulatory effects only in the 6 children who received the highest dose of oral insulin studied (67.5 mg/d). Therefore, as a companion study to the clinical trial reported herein, TrialNet has recently completed enrollment of more than 90 antibody-positive relatives in a study aimed at determining whether higher doses of oral insulin administered daily or every other week have measurable immunomodulatory effects (NCT02580877). Thus, trials of disease-modifying therapies among individuals with multiple antibodies could be considered treatment trials, in which the therapy aims to treat early type 1 diabetes or islet autoimmunity as distinct from prevention trials, which implies testing of therapies in healthy individuals to prevent disease. Most randomized clinical trials investigating type 1 diabetes–modifying therapy have been conducted among individuals with clinically overt disease. Disease-modifying therapies including the anti-CD3 monoclonal antibody, 13-19 rituximab,18-21 abatacept,22 and alefacept23 significantly preserved insulin production (as measured by C-peptide levels) as well having immunological effects in the first 1 to 2 years after diagnosis of clinical disease. As demonstrated in the Diabetes Control and Complications Trial (DCCT),24,25 improved short-term diabetes control associated with sustained C-peptide production decreased the risk of complications. However, an effect at earlier stages of the disease in delaying onset of clinical diagnosis of diabetes would have a much more profound effect in relieving the burden of disease. Despite marked advances in insulin delivery and glucose monitoring, there are still significant unmet needs for disease modifying therapy in type 1 diabetes. This study has several limitations. First, there was a change in insulin autoantibody assay methods from the DPT-1 Oral Insulin study. The change in assay may have contributed to an inability to replicate previous results from the study using a 7.5-mg fixed dose of oral insulin because the selected study population for this trial may differ slightly from the population in the previous study that showed apparent benefit. Second, as in other type 1 diabetes prevention trials, there is the limited knowledge about and the ability to incorporate heterogeneity in the pathogenesis of disease. This trial enrolled participants based on evidence of autoimmunity but did not take into account genetic background, age at onset, and type of first-appearing diabetes-related autoantibody. The emerging literature now suggests that future trials need to consider these factors, and this trial adds to that, suggesting that first-phase insulin release status and treatment adherence should also be incorporated into study designs. Third, due to lack of adjustment for multiple comparisons, all secondary analyses need to be interpreted as exploratory. To illustrate, according to the latest report from the Boards of Trustees of the Federal Hospital Insurance and Federal Supplementary Medical Insurance Trust Funds,1 spending on the federal Medicare program currently is 3.6% of a GDP per capita of about $58 000. That leaves $55 912 of non-Medicare GDP for all other spending. The trustees project that by 2050, Medicare will account for 6% of GDP. Assuming a low future rate of growth of only 1% per year for real GDP per capita, spending on Medicare will be 6% of a projected inflation-adjusted GDP per capita of $80 544 (with the 6% spending leaving the contemporaries living in 2050 a non-Medicare GDP per capita of $75 500). That amount is 35% higher than non-Medicare GDP per capita today. So will a spending level of 6% of GDP per capita in 2050 be affordable? Is Medicare sustainable?. The proposed Affordability Index also is closely related to the Milliman Medical Index (MMI),4 which is based on a large data bank of ESI-covered US families. The data bank is maintained by the employer-benefit consulting firm Milliman. The MMI shows, in dollars, the total cost of health care for a typical US family of 4 covered by a Preferred Provider Health Insurance Plan under ESI. Total costs in the index include the employee’s direct contribution toward his or her family’s insurance coverage, plus the family’s out-of-pocket health spending. In 2017, the estimated total cost is $26 944, of which employers paid $15 259 and employees, $11 685. The desired reports or interactive information systems could be produced by the Agency for Healthcare Research and Quality or the Kaiser Family Foundation. But because the MEPS database is an open-access file, it could be produced by any organization capable of producing such analyses. All that is needed would be the fashioning from the data regular user-friendly reports or interactive information systems that would make them prominent for the media and the general public. Most importantly, such a data system could help constrain the folklore on which health policy so often is conducted and provide to legislators robust data about the fragile US health insurance system over which they preside. Estimating the average family’s ability to pay for health care is not a straightforward calculation. Complications arise because of the complex way health insurance is financed. Employer contributions to employer-sponsored insurance and a host of subsidies for health coverage through Medicare, Medicaid, insurance exchanges, and the tax system all provide resources that must be accounted for in any calculation. However, the amount of that support varies widely across individuals. That complexity does not lend itself to a clear understanding of affordability using a single simplified measure. Emanuel and colleagues1 correctly ignore the split between what employers and workers pay and instead focus on the total premium. Economists agree that, in a competitive labor market, even when a firm pays a large share of the premium for its workers, the cost of the premium is generally passed on to the workers in some form. The firm may reduce cash compensation by the amount of the firm’s premium contribution. Other adjustments would also reduce the firm’s labor costs, including reducing the generosity of the health plan, modifying other benefits, or hiring fewer workers.4 To simplify the following discussion, assume that the full cost of the employer’s contribution comes out of wages. While the affordability index correctly identifies the total premium, rather than just the workers’ share, it is not clear what the corresponding measure of ability to pay should be. Median household income captures what workers are paid in salaries and wages, netting out employer contributions for insurance premiums and other benefits. The median household income of workers excludes billions of dollars in payments implicitly made by workers for health insurance. Those payments are made by the firms that employ the workers, but it is the workers who ultimately pay these premiums in the form of reductions in the other forms of compensation they receive, including wages. In concept, that money associated with employer-paid insurance premiums should be included in the measure of ability to pay. Viewed this way, the Affordability Index double-counts a large portion of premium costs. Household income, measured in cash, reflects the implicit payment by workers that was already made for insurance premiums. However, the index implies that workers are being forced to shoulder the total premium cost of an average employer plan out of their cash compensation. That exaggerates the burden of rising health costs imposed on workers. Total compensation would be a better measure of worker ability to pay. With rising health costs, premiums and employer contributions for insurance have increased rapidly. That has contributed to slow wage growth. Between 1999 and 2006, the total compensation of workers with incomes around the 50th percentile of annual income increased by a total of 34%, while their cash compensation increased by only 27%.5 The rising cost of health care took up a greater share of total compensation in 2006 than it did in 1999, thus depressing the growth in cash income. If total compensation were used in the Affordability Index, it would show slower growth in the percentage of income going to health insurance, particularly for middle-class families. The Affordability Index is intended to provide an easily understood measure that is accessible to experts and the general public. Although such an approach can be useful for some purposes, it does not capture the wide variation in health care costs and financial resources to cover those costs across the diverse US population. In addition to a national index, regional or state-level indexes would partially address geographic variations. But the index would be misleading if it did not account for the sizeable government subsidies for health insurance. The largest single subsidy is the favorable tax treatment for employer-sponsored insurance. Employer contributions to premiums are not treated as taxable income for workers, and nearly all of their own contributions are also free from federal income and payroll taxes. According to the Joint Committee on Taxation,6 households with adjusted gross incomes between $50 000 and $75 000 in 2007 received an average tax break of $3106 for employer-paid premiums. In effect, this tax subsidy reduces the cost of their health insurance premium. The Affordability Index ignores this important complication. Federal insurance subsidies do not lower overall health costs. Instead, the subsidies move some of the cost burden from some households to others. However, ignoring the subsidies creates a distorted impression that moderate-income households must shoulder the entire cost burden. In fact, much of that cost is borne by federal taxpayers, particularly upper-income households in higher tax brackets.8. In this issue of JAMA, Hedayati et al11 report the findings of a clinical trial evaluating the effect of sertraline on depressive symptoms in patients with MDD and CKD who were not receiving dialysis. This randomized, placebo-controlled, parallel-group, investigator- and clinician-masked trial was performed at 3 centers affiliated with the University of Texas Southwestern Medical Center. Persons with CKD stage 3, 4, or 5 (eGFR <60 mL/min/1.73 m2) not receiving dialysis were initially eligible; however, during the study the eGFR threshold was reduced to less than 45 mL/min/1.73 m2. Depressive symptom screening was performed in primary care and CKD clinics with the 16-item Quick Inventory of Depressive Symptomatology–Self-Reported (QIDS-SR16), and patients were eligible if they scored 11 or greater (score range, 0-27). Those who screened positive were then given the Mini International Neuropsychiatric Interview to confirm MDD and exclude other psychiatric conditions. Persons who were taking antidepressants or recently received psychotherapy were excluded. Of the 261 patients who started the placebo run-in phase, 201 successfully completed this phase and were randomized. At least 1 follow-up assessment was completed by 97 of 102 persons in the sertraline group and by 96 of 99 persons in the placebo group; overall, 84% completed the full 12 weeks. Median eGFR was 27.5 mL/min/1.73 m2 in both study groups, reflecting quite advanced CKD in the majority of patients, and the 2 groups were reasonably balanced on most baseline characteristics. The median final sertraline dose was 150 mg/d. The primary outcome was change in depressive symptom severity as measured by the QIDS-C16 (has a minimal clinically important difference of 2 points), which was administered over 12 weeks by trained clinicians masked to treatment assignment. The mean baseline QIDS-C16 score was similar in the 2 groups (14.0 in the sertraline group vs 14.1 in the placebo group). This well-conceived, well-conducted, methodologically sound trial, the largest and best to this point addressing one of the most important challenges facing the CKD population, found that therapy with one of the most promising antidepressants for this population is largely no more effective than placebo and has adverse effects. Although most of the adverse effects were not severe, gastrointestinal symptoms are already common and bothersome among patients with CKD and cannot be disregarded. The study was adequately powered so that the effect estimate for depressive symptom change excludes the clinically important benefit. The findings are likely generalizable, considering that the diverse, multimorbid, medicated patient sample seems to reflect the CKD population at large. As multimorbidity becomes the rule rather than the exception in much of clinical practice, trials targeting specific subpopulations, pragmatic trials, and personalized trials (including n-of-1 trials) are necessary to help determine whether prescribing a certain medication is more likely to help or to harm. Large, highly-selected and standardized efficacy trials, excluding those with CKD or other comorbidities, may have little relevance to the care of patients with complex multimorbidities. Two studies of antidepressants in patients with heart failure, namely the Effects of Selective Serotonin Re-Uptake Inhibition on Morbidity, Mortality, and Mood in Depressed Heart Failure Patients (MOOD-HF; using escitalopram)14 and the Sertraline Against Depression and Heart Disease in Chronic Heart Failure (SADHART-CHF; using sertraline),15 failed to show significant improvements in depression despite using clinician-rated depression scales (Montgomery-Åsberg Depression Rating Scale and Hamilton Depression Rating Scale, respectively). Furthermore, essentially all studies of SSRIs have shown that their efficacy to treat moderate depression is usually close to 40%, and the placebo effect is close to 30%, with more robust antidepressant response seen in patients with severe depression. The 2 studies in patients with heart failure14,15 and the study by Hedayati et al11 selected patients with moderate depression. Patients with CKD constitute an especially important subpopulation, given its size and pathophysiological differences, and accumulating evidence that interventions that had been shown to be efficacious in populations mostly free from advanced CKD may not be effective in patients with chronically reduced kidney function. The illustrative experience of statin therapy, repeatedly shown ineffective at improving cardiovascular outcomes in patients with chronic kidney failure requiring dialysis, is an object lesson that even the most important and effective therapies in more general populations must be tested in CKD before clinicians can be confident of benefit. Thus, Hedayati et al11 have made an important contribution to the care of patients with CKD. The nephrology and psychiatry communities now have the responsibility to conduct further trials that use rigorous criteria for identifying depression, include more severely depressed patients, and evaluate other SSRIs to further probe for effective and safe treatments for depression in patients with CKD. What would happen if on one day more than 50 people died and over 10 times that many were harmed by an infectious disease in the United States? Likely, our nation’s esteemed and highly capable public health infrastructure would gear up to care for those harmed and study the problem. There would be a rush to identify the cause, develop interventions, and refine them continually until the threat is eliminated or at least contained. In light of the risks to public health (after all, over 500 people have been harmed already!), health care professionals would sound the alarm. We would demand funding. We would go to conferences to learn what is known and what we should do. We would form committees at our institutions to plan local responses to protect our communities. The United States would spend millions or more in short order to ensure public safety, and no elected officials would conceive of getting in the way. Rather, they would compete to be calling the loudest for the funds and focus required to protect our people. Americans should be proud of our prowess at and commitment to addressing public health crises. Yet, here we are again with another editorial about the public health crisis of firearm-related injury and death following what used to be unthinkable, this time a mass murder and casualties at a concert in Las Vegas. We’ve written it all before. The staggering numbers killed annually. The numbers left permanently disabled. The families left to cope with the loss of loved ones or to care for those broken but not killed by a bullet. As health care professionals, we seem powerless. This public health crisis seems beyond the reach of our tools. Is there really nothing health care professionals can do? We think there is a great deal. We need to each ask ourselves what we have done to apply our knowledge and skills to help address the problem since the moment of silence that followed the last mass shooting. More silence is not the answer. Have we demanded funding to adequately study the problem and test solutions? Have we participated in such studies? Have we mobilized forces at our institutions to plan strategies to lower the risks in our communities? Have we talked to our patients about gun safety and effectively challenged policies that would enforce our silence on this matter? Some of our colleagues have. We should be proud of them, but they need all of our help. And so do our patients. Educate yourself. Read the background materials and proposals for sensible firearm legislation from health care professional organizations. Make a phone call and write a letter to your local, state, and federal legislators to tell them how you feel about gun control. Now. Don’t wait. And do it again at regular intervals. Attend public meetings with these officials and speak up loudly as a health care professional. Demand answers, commitments, and follow-up. Go to rallies. Join, volunteer for, or donate to organizations fighting for sensible firearm legislation. Ask candidates for public office where they stand and vote for those with stances that mitigate firearm-related injury. Meet with the leaders at your own institutions to discuss how to leverage your organization’s influence with local, state, and federal governments. Don’t let concerns for perceived political consequences get in the way of advocating for the well-being of your patients and the public. Let your community know where your institution stands and what you are doing. Tell the press. Educate yourself about gun safety. Ask your patients if there are guns at home. How are they stored? Are there children or others at risk for harming themselves or others? Direct them to resources to decrease the risk for firearm injury, just as you already do for other health risks. Ask if your patients believe having guns at home makes them safer, despite evidence that they increase the risk for homicide, suicide, and accidents. Don’t be silent. We don’t need more moments of silence to honor the memory of those who have been killed. We need to honor their memory by preventing a need for such moments. As health care professionals, we don’t throw up our hands in defeat because a disease seems to be incurable. We work to incrementally and continuously reduce its burden. That’s our job. Mendelian randomization uses genetic variants to determine whether an observational association between a risk factor and an outcome is consistent with a causal effect.1 Mendelian randomization relies on the natural, random assortment of genetic variants during meiosis yielding a random distribution of genetic variants in a population.1 Individuals are naturally assigned at birth to inherit a genetic variant that affects a risk factor (eg, a gene variant that raises low-density lipoprotein [LDL] cholesterol levels) or not inherit such a variant. Individuals who carry the variant and those who do not are then followed up for the development of an outcome of interest. Because these genetic variants are typically unassociated with confounders, differences in the outcome between those who carry the variant and those who do not can be attributed to the difference in the risk factor. For example, a genetic variant associated with higher LDL cholesterol levels that also is associated with a higher risk of coronary heart disease would provide supportive evidence for a causal effect of LDL cholesterol on coronary heart disease. Basic principles of mendelian randomization can be understood through comparison with a randomized clinical trial. To answer the question of whether raising HDL cholesterol levels with a treatment will reduce the risk of coronary heart disease, individuals might be randomized to receive a treatment that raises HDL cholesterol levels and a placebo that does not have this effect. If there is a causal effect of HDL cholesterol on coronary heart disease, a drug that raises HDL cholesterol levels should eventually reduce the risk of coronary heart disease. However, randomized trials are costly, take a great deal of time, and may be impractical to carry out, or there may not be an intervention to test a certain hypothesis, limiting the number of clinical questions that can be answered by randomized trials. Mendelian randomization rests on 3 assumptions: (1) the genetic variant is associated with the risk factor; (2) the genetic variant is not associated with confounders; and (3) the genetic variant influences the outcome only through the risk factor. The second and third assumptions are collectively known as independence from pleiotropy. Pleiotropy refers to a genetic variant influencing the outcome through pathways independent of the risk factor. The first assumption can be evaluated directly by examining the strength of association of the genetic variant with the risk factor. The second and third assumptions, however, cannot be empirically proven and require both judgment by the investigators and the performance of various sensitivity analyses. In a previous report in JAMA, Frikke-Schmidt et al4 initially applied mendelian randomization to HDL cholesterol and coronary heart disease using gene variants in the ABCA1 gene. When compared with noncarriers, carriers of loss-of-function variants in the ABCA1 gene displayed a 17-mg/dL lower HDL cholesterol level but did not have an increased risk of coronary heart disease (odds ratio, 0.93; 95% CI, 0.53-1.62). The observed 17-mg/dL decrease in HDL cholesterol level is expected to increase coronary heart disease by 70% and this study had more than 80% power to detect such a difference; thus, the lack of a genetic association of ABCA1 gene variants and coronary heart disease was unlikely to be due to low statistical power. These data were among the first to cast doubt on the causal role of HDL cholesterol for coronary heart disease. In other mendelian randomization studies, genetic variants that raised HDL cholesterol levels were not associated with reduced risk of coronary heart disease, a result consistent with HDL cholesterol as a noncausal factor.5. Low HDL cholesterol levels track with high plasma triglyceride levels, and triglyceride levels reflect the concentration of triglyceride-rich lipoproteins in blood. Using multivariable mendelian randomization, Do et al3 examined the relationship among correlated risk factors such as HDL cholesterol and triglyceride levels. In an analysis of 185 polymorphisms that altered plasma lipids, a 1-SD increase in HDL cholesterol level (approximately 14 mg/dL) due to genetic variants was not associated with risk of coronary heart disease (odds ratio, 0.96; 95% CI, 0.89-1.03; Figure). In contrast, a 1-SD increase in triglyceride level (approximately 89 mg/dL) was associated with an elevated risk of coronary heart disease (odds ratio, 1.43; 95% CI, 1.28-1.60). LDL cholesterol and triglyceride-rich lipoprotein levels, but not HDL cholesterol level, may be the causal drivers of coronary heart disease risk as demonstrated by these mendelian randomization studies. The primary concern when evaluating mendelian randomization studies is whether genetic variants used in the study are likely to be pleiotropic. Variants in a single gene that affects an individual risk factor are most likely to affect the outcome only through the risk factor and not have pleiotropic effects. For example, variants in CRP, the gene encoding C-reactive protein, have been used in a mendelian randomization study to exclude a direct causal effect of C-reactive protein on coronary heart disease.6 However, variants in single genes that encode a risk factor of interest are often not available. In these cases, pleiotropy can be examined by testing whether the gene variants used are associated with known confounders such as diet, smoking, and lifestyle factors.7 More advanced statistical techniques, including median regression8 and use of population-specific instruments,7 have recently been proposed to protect against pleiotropic variants biasing results. Transparency and accountability are 2 recurring components of efforts to improve the quality and value of US health care. In the June 2017 issue of JAMA Internal Medicine, Blay et al1 compared the performance of US Department of Veterans Affairs (VA) hospitals with non-VA hospitals using Centers for Medicare & Medicaid Services (CMS) Hospital Compare data. VA hospitals (n = 129) were compared with both academic and community hospitals (n = 4010) on a range of clinical outcomes and patient experience measures. VA hospitals performed better than non-VA hospitals for most outcome measures—including lower rates of inpatient safety events such as pressure ulcer, iatrogenic pneumothorax, and central line–associated bloodstream infection—and lower 30-day mortality and readmission rates for acute myocardial infarction (MI), pneumonia, heart failure, and chronic obstructive pulmonary disease. At the same time, VA hospitals scored lower for patient experience and behavioral health. Given the unique mission and heightened visibility of VA as a public-sector institution, these findings raise 2 questions: how does VA ensure the best health care possible for veterans in all aspects of care, and what is the VA doing to address areas that may need further improvement?. What factors explain this better performance? One is likely the VA’s early investment in electronic health records (EHRs). Since the 1970s, the VA has collected system-wide data from EHRs to advance patient care, facilitate research, and enhance patient-physician communication. More recently, the VA has used EHR data to support performance measures and drive improvement. In fact, what began in the 1990s as a small set of measures to encourage improvement efforts by network leaders, today is a sophisticated matrix of 25 measures and measure sets used to assess each VA facility. This matrix—known as Strategic Analytics for Improvement and Learning (SAIL)—includes a comprehensive set of clinical performance metrics, including all veteran-relevant metrics reported to CMS and the National Committee for Quality Assurance, as well as assessments of factors thought to enable quality and safety improvement (eg, employee morale, nursing turnover, leadership vacancies, and selected utilization metrics). An overall improvement index is derived from SAIL to determine whether each facility’s performance has improved in the prior year. In 2016, 82% of 146 VA facilities demonstrated tangible improvements. The SAIL improvement index is now incorporated into performance plans of facility and network leaders. A second factor explaining the VA’s better performance on certain measures than the private sector relates to coordinated programs and clear organizational goals empowering improvement at the local level. Blay et al1 found that VA hospitals performed better than non-VA facilities on surgical patient safety indicators. The VA implemented the National Surgical Quality Improvement Program (NSQIP) in the 1980s. NSQIP collects clinical information about preoperative and postoperative surgical outcomes in the VA system. Local clinical leaders can compare risk-adjusted outcomes between facilities, enabling them to launch quality improvement efforts once they identify where they can improve relative to their peers. A third factor explaining VA’s high quality outcomes is an embedded research program. VA research helped develop the evidence behind national programs such as primary care–mental health integration, tele-mental health, and new models to care for women veterans. In an effort to accelerate evidence-based practice in critical clinical areas, in 1998 the VA launched its Quality Enhancement Research Initiative (QUERI), a program that brings together researchers and program leaders to accelerate evidence-based practices in critical clinical areas, including heart disease. The ischemic heart disease QUERI created a national data system for real-time tracking of every procedure performed at VA cardiac catheterization facilities, along with clinical outcomes. Other QUERI cardiac care initiatives have addressed improving hospitalization in chronic heart failure (CHF) and appropriate medication use post-MI in heart failure. The CHF QUERI helped implement an Institute for Healthcare Improvement initiative that reduced readmissions among patients with heart failure in VA hospitals from 21% in 2010 to 18% in 2014.6 A multifactorial intervention involving pharmacists, patient education, and automatic reminders increased the appropriate use of medications such as statins and clopidogrel from 74% to 89% in the year after an acute coronary event.7 An additional question implied by Blay et al is why these capabilities and expertise have not been translated into improvements across the board, including in important areas such as behavioral health and patient experience. One limitation when trying to compare inpatient behavioral health outcomes using Hospital Compare relates to its inability to account for integration of inpatient and outpatient services. Hospital Compare only assesses inpatient care. The VA has integrated primary and mental health care to facilitate the delivery of complex care in outpatient settings such as substance abuse treatment, specialty mental health care, and comprehensive screening for depression, tobacco, and alcohol abuse. Clinicians providing inpatient care at VA hospitals may not pursue alcohol and tobacco screening or continue care plan development because the shared EHR show these clinical problems were addressed in outpatient settings. Lower patient experience scores reported by veterans regarding VA care on the Hospital Consumer Assessment of Health Providers and Systems (HCAHPS) survey are more challenging to explain. One set of factors may arise from the VA’s greater visibility as a public health care system, coupled with its increasing transparency. The VA’s performance is frequently and directly compared with the performance of private sector health care, but private sector health care performance is rarely assessed in similar fashion. Another factor may be that the VA’s physical plant and capital infrastructure are significantly older and therefore lack many of the private sector’s amenities. In addition, patients with more physical or mental health problems generally provide lower scores in surveys of health care experience. Although CMS adjusts HCAHPS scores for patient mix, its statistical model was calibrated without the inclusion of veterans or veterans hospitals and therefore may not adequately adjust for these characteristics ,which are unique to the VA. An essential new lever for continuous improvement at VA is transparency. The VA recently launched a website10 that allows veterans to compare their local VA facility’s performance on Hospital Compare metrics to private-sector facilities in the same market area. Veterans can also use the site to obtain information on wait times and patient-reported experience for both primary and specialty care. Such openness, however, is not without trade-offs. Even as the VA has reduced excessive or unsafe use of opioids, its efforts often have uncovered problems that in turn triggered negative news stories. Nevertheless, the benefits of such transparency outweigh the risks. Such a policy serves to assure the majority of veterans that they are receiving the best care possible and exemplifies the department’s commitment to acknowledging and acting upon opportunities for improvement. EPI is a chronic disorder characterized by a deficiency of exocrine pancreatic enzymes, which results in malabsorption, steatorrhea, and weight loss. Irreversible EPI is commonly caused by chronic pancreatitis in adults and by cystic fibrosis in children. The standard treatment for EPI is pancreatic enzyme replacement therapy (Table). Treatment with pancreatic enzyme replacement products has reduced fat malabsorption and steatorrhea.1. The dosage for children and adults ≥4 years old with EPI is based on body weight, degree of steatorrhea present, and dietary fat content. The recommended dosage of all pancreatic enzyme replacement products is 500 lipase units/kg/meal (maximum 2500 units/kg/meal or ≤10 000 units/kg/day) or <4000 units/g of fat ingested/day. Older adults may need a lower dose because they ingest less fat per kilogram of body weight. Half the dose for a meal should be taken with each snack. The dosage for children <4 years old is based on Cystic Fibrosis Foundation Consensus Conference Guidelines.3. But Victor isn’t convinced that loneliness is as big a risk factor as, say, smoking and obesity. Much of the research into the health effects of loneliness has been imprecise because it failed to control for all potential confounders, Victor said. Plus, “a lot of these studies are cross-sectional. You don’t know which way the relationship is going.”. Loneliness might also be an early, relatively easy-to-detect marker for preclinical Alzheimer disease, suggests an article Donovan coauthored. She and her collaborators recently reported in JAMA Psychiatry that loneliness was associated with a higher cortical amyloid burden in 79 cognitively normal elderly adults. Cortical amyloid burden is being investigated as a potential biomarker for identifying asymptomatic adults with the greatest risk of Alzheimer disease. However, large-scale population screening for amyloid burden is unlikely to be practical. “You can be absolutely certain that loneliness messes up your quality of life,” Victor said. “It’s an unpleasant experience. It compromises well-being.” The problem, she and other researchers in the field say, is that there is a dearth of evidence to support what intuitively seems like good advice for lonely individuals—take a class, get a dog, do volunteer work. Nonprofits in the United Kingdom—where, according to Valtorta and Victor, loneliness is treated as public enemy number 1—send “befrienders” to people the organizations identify as likely to be lonely. “You’re lonely, therefore you need a friend,” the thinking goes, even though it’s very difficult to befriend a stranger, Valtorta said. Theeke and her collaborators recently published the results of a pilot study of LISTEN and are now seeking funding to conduct a larger study. The pilot included 27 cognitively normal adults (only 3 were men) who, to avoid confounding because of the grief reaction, had not lost a spouse in the previous 2 years. They were randomized to the LISTEN intervention or to a control group and met in groups of 3 to 5 with a facilitator for 5 weekly 2-hour sessions. Those in the control group received educational information about aging, while LISTEN participants talked about patterns of thought and behavior that were contributing to their perception of loneliness. Unlike obstructive sleep apnea, in which a partially or completely blocked airway disturbs sleep, central sleep apnea results when the brain fails to signal the diaphragm via the phrenic nerve to contract and make room for air to enter the lungs. Central sleep apnea is associated with an increased risk of hypertension, myocardial infarction, heart failure, stroke, obesity, and diabetes, according to the FDA. Among patients with chronic heart failure, an estimated 40% have central sleep apnea. Current treatments include medication, positive airway pressure devices, or surgery. In a multicenter trial involving 151 patients, investigators analyzed patients’ outcomes based on the apnea-hypopnea index (AHI), a measure of how many apnea or hypopnea episodes occur per hour of sleep. All patients received the implant. It was activated in half of the patients; the other half served as controls. After 6 months, AHI reduction was 50% or greater in 51% of patients whose device was activated compared with 11% in the control group. Adverse events included implant site infection, swelling, and local tissue damage or pocket erosion. All patients in the trial had their devices activated after the 6-month study period. Officials at the FDA approved the scanner through its 510(k) pathway, which requires a new device to be “substantially equivalent” to a predicate device already on the market. In addition to that equivalence, the agency’s approval was based on a safety review of the device’s radiofrequency subsystem through computational modeling, simulations, and rigorous experimental modeling. The manufacturer also provided data from a study that compared images from 35 healthy patients using the 7T device and a 3T scanner. Board-certified radiologists who reviewed the images confirmed that those derived from the 7T were as good or better than those from the 3T scanner. Points are estimated means from linear mixed models with error bars representing 95% CIs. Time 0 denotes the time of treatment intervention; sample size differs owing to missing data. A, Breathlessness score was measured with a visual analog scale, with a 100-mm line anchored with “no breathlessness” at 100 mm and “worst imaginable breathlessness” at 0 mm. B, Quality-of-life score was measured with a visual analog scale, with a 100-mm line anchored with “best quality of life” at 100 mm and “worst quality of life” at 0 mm. C, The EuroQol 5 Dimensions quality-of-life score consisted of 5 domains: mobility, self-care, usual activities, discomfort/pain, and anxiety/depression. Each domain was graded by the patient from 0 (worst imaginable) to 10 (best imaginable) and the total score out of a maximum 50 points was presented. This trial was conducted at 9 centers: Sir Charles Gairdner, Fiona Stanley, Swan District, Princess Alexandra, and St George & Sutherland hospitals in Australia; Wellington and Middlemore hospitals in New Zealand; National University Hospital Singapore; and Queen Mary Hospital Hong Kong. Ethics and governance approvals were obtained from the human research ethics committee at all sites, the primary committee being the Sir Charles Gairdner Group Human Research and Ethics Committee. Written informed consent was obtained from each study participant. The full trial protocol is available in Supplement 1. Patients randomized to the indwelling pleural catheter group had the catheter inserted as per the modified Seldinger technique with tunneling, followed by fluid removal, as a same-day or overnight-stay procedure unless there were other medical reasons necessitating continual hospitalization. Ambulatory fluid drainages were performed as guided by symptoms of individual patients by caregivers or nurses using either vacuum bottles or drainage bags. Indwelling pleural catheters were removed when clinically indicated, most commonly because of cessation of fluid accumulation. Participants randomized to talc pleurodesis underwent tube thoracostomy (12-18F), followed by instillation of talc slurry as per routine practice of the recruiting hospital. All participants received usual standard care including chemotherapy, radiotherapy, and palliative care as recommended by their attending clinicians. The primary outcome was the total number of days spent in hospital from trial intervention to death or up to the 12-month follow-up visit. Any hospital, including hospice, admission involving 1 or more days was included. One day referred to a hospital stay crossing midnight. Day-case procedures (eg, chemotherapy) were excluded. The duration of hospital admissions was decided independently by the treating physicians. Data on all hospital admissions were collected from participants (during follow-up visits), caregivers, general practitioners, electronic databases, and case records. An independent assessor (C.K.) reviewed the validity (justification and duration) of each hospital admission based on the discharge summary and full hospital record (if needed). Secondary outcomes included the following: first, the total number of days and episodes of hospitalization from pleural effusion–related causes, including admissions for management of pleural effusion, associated symptoms, related procedures, and/or their complications. Second, the need for further pleural drainage procedures. Third, breathlessness as measured using a visual analog scale (VAS) validated for patients with malignant pleural effusion.17 The VAS was a 100-mm line anchored with “no breathlessness” at 100 mm and “worst imaginable breathlessness” at 0 mm. Participants recorded their VAS scores daily in the initial 14 days and at follow-up visits at 1, 3, 6, 9, and 12 months. If patients were unable to attend follow-up, a research nurse (C.A.R.) would record the patient’s reported score by telephone. Fourth, QoL as quantified by (1) a modified EuroQol 5 Dimensions (EQ5D) questionnaire score18 for QoL and (2) a 100-mm VAS recorded at baseline (for both) and 8 and 14 days (for EQ5D) or daily for 14 days (for VAS) and 1, 3, 6, 9, and 12 months (for both). Fifth was survival and sixth was adverse and serious adverse events. Two-sided superiority analyses were conducted on an intention-to-treat basis for all outcomes. The primary end point was analyzed initially using a Mann-Whitney U test; the Hodges-Lehmann (HL) estimate of location shift between the groups and corresponding 95% CI is also presented. Subsequent supporting analyses were carried out using a negative binomial model with adjustments for actual length of follow-up (accounting for death and withdrawals), minimization variables, and random effect of center. The total effusion-related and non–effusion-related hospital bed days were similarly analyzed. The number of days spent in hospital, expressed as a percentage of the patients’ total days in the trial (from procedure to death or the 12-month follow-up), and the number of days in hospital for initial/subsequent hospitalizations, were compared using Mann-Whitney U tests as post-hoc analyses, with HL estimates of location shift between the groups and corresponding 95% CIs presented. Further post-hoc subgroup analyses by cancer type and by first or subsequent hospitalizations were separately performed using the same method described for the full group analyses. χ2 Tests were used to compare the proportions of deaths in the trial with differences in proportions and 95% CIs presented. Time to death was analyzed using both log-rank test and Cox proportional hazards models, the latter adjusting for minimization variables, with hazard ratios and 95% CIs presented. Frequencies of serious adverse events and further pleural intervention were described. The latter were analyzed with the Fisher exact test and differences in proportions and 95% CIs presented. No adjustment was made for multiple comparisons and thus the secondary outcomes should be considered exploratory, yielding hypothesis-generating findings. For all analyses, statistical significance was set at .05 and 2-sided tests were performed. All analyses were carried out using the R environment for statistical computing (R Foundation for Statistical Computing).19. Patients with a malignant pleural effusion (n = 146; 56.2% male) were recruited between July 2012 and October 2014 (Figure 1). The median age was 70.5 years (range, 38-92) and the most common underlying malignancies were lung cancer (n = 48), mesothelioma (n = 38), and breast carcinoma (n = 18). Both groups were well matched in their demographics, ratio of primary vs secondary pleural malignancies, effusion sizes, baseline symptom scores, and Eastern Cooperative Oncology Group status (Table 1). One patient from each group withdrew before presenting for the randomized intervention; they were excluded from all analyses. Pleurodesis failure was diagnosed when a further ipsilateral pleural procedure was needed for symptom relief. Talc pleurodesis failed in 16 patients (22.5%) and required further drainage interventions after a median of 32 days (IQR, 15.5-72.5). Most patients with pleurodesis failure (n = 10) were subsequently successfully treated with indwelling pleural catheter. Others received repeated therapeutic drainages (n = 3) and repeat talc slurry pleurodesis (n = 2); the latter failed again in 1 patient who then had video-assisted thoracoscopic surgery with talc poudrage. One underwent thoracotomy with partial pleurectomy and a pericardial window to control recurrent pleural and pericardial fluids. Three patients in the indwelling pleural catheter group required further pleural punctures. One had loculated effusion that prevented indwelling pleural catheter insertion and was treated with blunt dissection and large-bore chest drain insertion, followed by successful pleurodesis. One developed a pneumothorax with subcutaneous emphysema that required chest tube placement. The third patient had recurrence of effusion after successful removal of the initial indwelling pleural catheter and was treated with a second indwelling pleural catheter. Baseline VAS scores showed that the patients were breathless (indwelling pleural catheter group: mean, 50.8 mm, 95% CI, 39.9-61.6; pleurodesis group: mean, 52.8 mm, 95% CI, 42.0-63.5). The symptoms were significantly improved by day 1 after the procedure, with improvements in VAS score of 14.5 mm (95% CI, 8.4-20.7) for the indwelling pleural catheter group and 17.4 mm (95% CI, 11.1-23.7) for the pleurodesis group. The improvements were maintained in subsequent visits up to 12 months. No significant differences were found in the magnitude of symptom benefits derived from indwelling pleural catheter treatment or pleurodesis (Figure 3A). QoL measures quantified by VAS and by modified EQ5D both revealed a pattern similar to that of the breathlessness scores. The groups were balanced at their baseline QoL scores, which significantly increased from initial treatment with indwelling pleural catheter or pleurodesis. The improvement was maintained throughout the study follow-up in both groups. No significant differences were found in the magnitude of QoL improvement derived from indwelling pleural catheter treatment or pleurodesis (Figure 3B-C). The catheter was dislodged in 4 pleurodesis patients. Three were dislodged accidentally in hospital before talc instillation. In 2 cases, the fluid did not recur while in the third case, fluid recurred several months later and the patient had an indwelling pleural catheter inserted. In the fourth case, the chest drain was disconnected from the drainage bottle and was reconnected; pleurodesis was performed as planned. In 1 patient with indwelling pleural catheter, the catheter was removed accidentally and the participant declined a further indwelling pleural catheter insertion. Pleural infection (n = 2), cellulitis (n = 3), symptomatic fluid loculation (n = 1), and catheter blockage (n = 3) were reported with indwelling pleural catheter management. In this trial of patients with malignant pleural effusion, treatment with an indwelling pleural catheter vs talc pleurodesis resulted in fewer hospitalization days from treatment procedure to death (or 12-month follow-up), but the magnitude of the difference was of uncertain clinical importance. The data also showed that patients treated with indwelling pleural catheter over talc pleurodesis experienced fewer hospital days related to pleural effusion management and required fewer further invasive pleural drainages. Both indwelling pleural catheter and pleurodesis provided significant improvements in breathlessness and QoL; however, there were no significant differences between the 2 groups. These findings may inform patients and clinicians in deciding management. Indwelling pleural catheter and pleurodesis are 2 strategies with advantages and disadvantages22; existing literature suggests equipoise.8 To date, 3 RCTs9,10,23 have compared indwelling pleural catheter with talc pleurodesis (n = 106 and n = 57) or doxycycline pleurodesis (n = 144). Both indwelling pleural catheter and pleurodesis provided comparable symptomatic benefits in these studies; neither was found superior. This trial confirmed these findings and provided new data showing an advantage of indwelling pleural catheter in reducing hospitalization. This study, to our knowledge, was the first to measure total days spent in hospital in patients’ remaining lifespan as a principal outcome in malignant pleural effusion management. Malignant pleural effusions herald limited prognosis. The management goals are to relieve symptoms with minimal intervention and maximize time outside hospital. Post-hoc analysis showed that the use of indwelling pleural catheter significantly reduced the amount of time in trial patients spent in hospital (median, 6.2% vs 11.1% of their days in trial) over conventional pleurodesis. Indwelling pleural catheter provided several advantages over talc pleurodesis that would have contributed to the reduction in the total hospitalization days before death. First, patients randomized to indwelling pleural catheter treatment had shorter initial hospital admissions because indwelling pleural catheters were placed as day-case or overnight procedures whereas pleurodesis required chest tube insertion, complete evacuation of fluid, talc instillation, and hospitalization until fluid drainage ceased. Second, talc pleurodesis failed and necessitated further drainage interventions in 23% of patients in the trial, most of whom required admissions for further interventions. This failure rate is in keeping with other trials.6,24 Conversely, only 4% of indwelling pleural catheter–treated patients required further pleural drainages. The lower reintervention rate with indwelling pleural catheter is an important consideration and benefit for patients with advanced cancer. Further studies are under way to optimize the benefits of indwelling pleural catheter use. The AMPLE-2 trial is an RCT comparing aggressive (daily) indwelling pleural catheter drainages with symptom-guided “as-required” approach for dyspnea relief and likelihood of spontaneous pleurodesis28 (trial registration: ACTRN12615000963527). Whether talc instillation via an indwelling pleural catheter can improve pleurodesis rate is the subject of the recently completed IPC-PLUS trial29 (trial registration: ISRCTN73255764). Indwelling pleural catheter coated with a sclerosant (silver nitrate) has shown promise in animal studies30 and in a pilot clinical study.31. Talc can be administered as slurry via a chest tube or as dry powder via thoracoscopic poudrage. Previous RCTs showed that talc slurry and poudrage have similar failure rates6,7; whether the different delivery methods affect lifetime all-cause hospitalization has not yet been studied. Australia has one of the world’s highest incidences of mesothelioma.32 This study therefore included more patients with mesothelioma than would otherwise be expected in many countries. The study randomization did stratify patients by mesothelioma (vs other cancers) and subgroup analyses showed benefits in reducing hospitalization days. This study has several limitations. First, no health economic analyses were conducted. This decision was taken as the costs of hospital days, indwelling pleural catheter equipment and drainage kits, and pleurodesis vary vastly worldwide,33,34 including at the participating centers of this study. The investigators therefore made a decision at the start of the study not to proceed with cost analyses. However, based on the rough costs provided by the participating centers, there is a reasonable likelihood that savings from the reduction in the small number of hospitalization days found in this study will not meet the usual standards of cost-effectiveness. Clinicians need to translate the reduction of hospital days into local cost currencies to establish the health care savings in individual health systems. The Senhance System builds on traditional laparoscopic surgery with ergonomic improvements and technological advancements. New eye-sensing technology tracks surgeons’ pupils so they can pan throughout the surgical field using only eye movements. Surgeons also can zoom in or out by moving toward or away from the screen. Force feedback also helps surgeons feel whether the tissue they’re grasping with the robotic arm is stiff or flexible. Our health care system is waking up to the fact that the health of individuals and families does not depend solely on good coverage and good medical care; it also requires us to address social and other factors that are major contributors to a person’s physical and mental well-being. That’s why more and more clinics are screening incoming patients for challenges in areas ranging from housing conditions, nutrition, access to transportation, and even their ability to afford utilities. It’s why the American Academy of Pediatrics urged its members not only to screen all patients for food insecurity but to refer parents to appropriate agencies. It is also why some hospitals, to reduce readmissions, have brought organizations like Health Leads into their discharge planning to connect patients with social services. The evidence on social determinants of health is growing, but is still insufficient to convince many key decision makers. For instance, there is good research on the link between such housing problems as mold or substandard accommodations and health, and between family or social “toxic” stress and long-term mental health and other patterns. But purported linkages between health and other social conditions, such as general poverty, lack reliable evidence. Much more basic research is needed to understand the key determinants. There is always resistance to change. So jurisdictions and government budget committees, as well as private managers, need strong evidence to build the case that investments in social factors rather than just more medical services results in a good return on investment (ROI). But it is often lacking. In part that is due to the data collection challenges faced by innovative community organizations that are exploring social welfare strategies to improve health. Meanwhile, few government jurisdictions have well-developed analytical capabilities to measure the health ROI of addressing social determinants and procedures to incorporate that information into decision making. Fortunately, elements of a data infrastructure are emerging. For instance, the National Neighborhood Indicators Project, based at the Urban Institute, is helping communities and governments build and use better data systems. In addition, the Washington State Institute for Public Policy, created many years ago by the state’s legislature, conducts (in conjunction with state universities) cost-benefit analyses of programs and initiatives to inform legislative and agency decisions. But most communities and states still lack such tools, hampering the ability to make the case for a greater emphasis on social determinants. Within the health care industry itself, there needs to be new thinking about the business models of key institutions, such as hospitals, as well as the use of intermediaries to improve the cooperation of health care and other sectors, such as schools and housing. However, the range of potential models being seriously considered is constrained by such barriers as insurance reimbursement and the payment policies of Medicaid and Medicare, since these affect the financial viability of different approaches to improving health. Fortunately, Medicaid is slowly providing more payment and organizational flexibility for approaches that address social factors in health. Such flexibility is encouraged through the use of Medicaid Managed Care Organizations and through experiments made possible through Medicaid Section 1115 waivers, which allow states to experiment with different payment and organizational arrangements. Government at all levels is responsible for much of the funding of services and initiatives associated with social conditions affecting lower-income people. Thus, improving health through a greater emphasis on social determinants for these individuals depends on better coordination and planning between agencies, as well as greater flexibility in the use of funds. That requires strong leadership, but it also needs structures to make coordination and flexibility more routine. Children’s cabinets, established in more than half the states, are a possible model for how to coordinate medical and social services to improve health. These groups bring together senior agency officials of departments responsible for programs that provide services to young people to coordinate and jointly plan those services and budgets. Federal and state-level “health cabinets,” including departments dealing not just with medical care but also with housing, transportation, social services, and education, could be similar, valuable institutional tools. Breaking down agency budget silos is particularly challenging, but it is ultimately essential if the United States is ultimately to rebalance spending between medical and social programs to improve underlying health. As a step toward that goal, the federal government needs to widen the use of waivers to permit more experiments to test the effects of investments in social determinants on health. In the meantime, more states could adopt versions of Maryland’s use of local management boards. These are county-level bodies that have some discretion to blend budgeted money from different departments and private funds, to support innovative local organizations and programs. During 2015 and 2016, the researchers collected hair samples from 1044 women between 18 and 44 years in 37 locations, primarily from 1 or 2 areas in developing and transition countries, as well as the state of Alaska. Analysis of the samples showed that 42% of the women sampled had mercury levels that exceeded the 1-ppm threshold level. The primary sources of mercury exposure varied by country and included contaminated fish and sea mammals, artisanal and small-scale gold mining (ASGM), and proximity to coal-fired power plants. For example, high mercury levels in women in the Pacific islands likely resulted from a diet rich in seafood, whereas exposure to products heavily used in ASGM likely accounted for high levels of mercury in women in Indonesia, Kenya, and Myanmar. The study identified 52 252 deaths in neonates (<1 mo) and 42 057 deaths of children between 1 and 59 months. The analysis showed an average annual decline in mortality rates from 2000 to 2015 of 3.3% for neonates, with declines in deaths from infection, birth asphyxia or trauma, and tetanus. During the same period there was an even greater reduction of 5.4% in the death rate for children between 1 and 59 months, with mortality rates from pneumonia, diarrhea, tetanus, and measles decreasing. In contrast, mortality rates for prematurity or low birth weight rose somewhat in rural areas and poorer states from 2000 to 2015. Annual decline in child mortality accelerated from 2005 to 2015, with the steepest declines seen from 2010 to 2015. This acceleration in mortality declines in recent years helped India avoid about 1 million child deaths since 2005. The protection described in the preceding paragraph belongs to every medical officer and to the dependents named, as a matter of right; but if he desires to be further fortified against disaster, he may accomplish that result through the system of governmental insurance that the act establishes. Nominally such insurance is at the expense of the insured; but as the rates of insurance are computed merely on a peace basis, the government assuming all of the added risk arising out of the war, and as the government assumes all of the expenses incident to administration of the insurance system, the insured will obtain protection at a low rate. As all policies are convertible at the close of the war, without medical examination, into insurance policies of ordinary standard types, premiums paid for protection during the war will not be lost. Ordinarily application for insurance must be made within 120 days after entering the service; but men in the service, April 6, 1917, are by the terms of the act automatically insured up to and including Feb. 12, 1918, by which date they must have applied for such insurance as they desire, in the usual manner. Application can be made, however, before the latter date, in which event the insured will have the benefit of such insurance as he may select in lieu of the standard ad interim insurance named in the act. An allergen is something that triggers an allergic reaction. Currently, the only “treatment” for food allergies is strict avoidance of anything containing the allergen. In the case of accidental ingestion, medications can be used to treat allergy symptoms, which can range from mild to severe. Mild symptoms include rash (hives), itching, and swelling of the lips and tongue. More severe symptoms can include trouble breathing, wheezing, throat swelling, and nausea, vomiting, or diarrhea. A very severe allergic reaction is called anaphylaxis, which can be life threatening and requires immediate medical attention. It is important not to confuse a food allergy with food intolerance. A true food allergy results from an overactive immune system targeting an allergen found in food. Food intolerance, in contrast, is when a person has trouble digesting certain foods. This intolerance can come from a number of factors related to the gastrointestinal tract and the body, but it does not involve the immune system. Symptoms of food intolerance can include bloating, gas, abdominal pain, and diarrhea. An example is lactose intolerance. Immunotherapy uses a tiny amount of an allergen to trigger a mild immune response in the body without causing a full allergic reaction. Repeating these exposures over time (several years) allows the body to become used to the allergen and eventually produce less of an allergic reaction. This process is called desensitization. Immunotherapy can be delivered to the body as an injection under the skin (subcutaneous immunotherapy, commonly known as allergy shots) or as a pill that is swallowed (oral immunotherapy) or placed under the tongue (sublingual immunotherapy). Sources: Sampson HA, Shreffler WG, Yang WH, et al. Effect of varying doses of epicutaneous immunotherapy vs placebo on reaction to peanut protein exposure among patients with peanut sensitivity: a randomized clinical trial. JAMA. doi:10.1001/jama.2017.16591. Jones SM, Burks AW. Food allergy. N Engl J Med. 2017;377(12):1168-1176. The analysis of fee-for-service Medicare beneficiaries hospitalized with HF along with other conditions covered by the HRRP from 2008 to 2014 by Dr Dharmarajan and colleagues found that reductions in paired monthly hospital 30-day readmissions were weakly but statistically significantly correlated with reductions in 30-day mortality rates after discharge. This finding could indicate that 30-day postdischarge mortality was not increased by efforts to reduce rehospitalizations.3 However, the assessment for unintended consequences needs to consider the effect on all patients and all hospitals exposed, not just those from hospitals with declining readmissions. During the course of the study, after HRRP implementation, 30-day risk-adjusted postdischarge mortality in Medicare beneficiaries hospitalized with HF increased from 7.9% in 2008 to 9.2% in 2014, a 1.3% absolute increase and 16.5% relative increase. In 2014, because 385 222 Medicare beneficiaries were hospitalized with HF, a 1.3% absolute increase in mortality would represent a considerable number of excess HF patient deaths associated with HRRP implementation. Furthermore, in the decade prior to HRRP, 30-day risk-adjusted mortality rates in patients with HF had steadily declined by 16.4%,4 so the magnitude of this potential adverse consequence might be considerably greater. Rather than providing any measure of reassurance, we believe the question of harm remains. These findings suggest that the HRRP policies targeting readmissions after HF hospitalization may have been associated with the serious unintended consequence of higher mortality. If confirmed, these data represent the worst-case scenario regarding the potential effect of HRRP. Consideration needs to be given to more rigorous assessment of this question, and if proven to be harmful, action to mitigate those components that may have contributed to the increase in HF mortality should be implemented. Although we did find declining readmission rates and modestly increasing postdischarge mortality rates for HF across hospitals between 2008 and 2014, more importantly, these aggregated trends did not reflect readmission and mortality trends within individual hospitals. Instead, hospitals with declining readmission rates were more likely to have declining mortality rates. This relationship was present not only for HF but also for acute myocardial infarction and pneumonia. The strength of this association further increased when follow-up was extended from 30 to 90 days after hospital discharge. These findings indicate that improvements in readmission are not increasing mortality. hs-cTnI indicates high-sensitivity cardiac troponin I; NPV, negative predictive value. Data markers indicate the central estimate of NPV with size corresponding to the number of patients per cohort (large, >3000 patients; medium, ≥1000 patients; small, <1000 patients) and error bars indicating 95% CIs. Dotted line and shaded areas represent the central estimate and 95% CIs for the full analysis population. All 19 cohorts were included in analyses unless otherwise specified. hs-cTnI indicates high-sensitivity cardiac troponin I; NPV, negative predictive value. In all panels, performance of hs-cTnI thresholds are shown for all patients (dark blue) and when applied to patients with nonischemic electrocardiogram (ECG) findings at presentation (light blue). All estimates of NPV are derived from a binomial-normal random-effects model using individual patient-level data (available in 17 cohorts) for each hs-cTnI threshold (n = 18 601; eTable 5 in the Supplement).6-9,11,18-29 A, NPV across a range of hs-cTnI concentrations. Error bars indicate 95% CIs. Horizontal dotted line indicates prespecified target NPV of 99.5% and vertical dotted line indicates hs-cTnI concentration of less than 5 ng/L. B, Cumulative proportion of all patients with suspected acute coronary syndrome classified as low risk. Dotted vertical line indicates proportion of patients with hs-cTnI concentration of less than 5 ng/L. C, Number of false negatives per 1000 patients tested across a range of hs-cTnI thresholds. Electrocardiogram data were not available for 2929 patients (15.7%). Two investigators (A.R.C. and K.K.L.) performed the initial screening of titles and abstracts. Full-text reports of potentially relevant articles were obtained and assessed by both investigators using a prespecified protocol (PROSPERO register CRD42017059128). A third investigator (A.S.V.S.) adjudicated all disagreements. When there were multiple articles describing the same cohort, the article that included the largest number of participants was included. The corresponding authors of each eligible cohort were contacted with a request for anonymized data including cardiac troponin concentrations, adjudicated diagnosis, outcomes, and prespecified covariates (age, sex, chest pain, time from symptom onset to presentation sample, myocardial ischemia on electrocardiogram, cigarette smoking, diabetes mellitus, hypertension, hyperlipidemia, known angina, previous myocardial infarction, percutaneous coronary intervention, coronary artery bypass graft surgery, and stroke). All studies were prospective and conducted in accordance with the Declaration of Helsinki with approval from the regional ethics committee or institutional review board, and written consent was obtained where required. This approval permitted each contributor to share individual-level data or aggregate data for inclusion in this meta-analysis. Bias was assessed by 2 investigators independently, with consensus from a third, using the Quality Assessment of Diagnostic Accuracy Studies version 2 (QUADAS-2) framework (eAppendix 4 in the Supplement). The analysis population comprised patients with cardiac troponin concentrations at or below the 99th percentile at presentation (those above the 99th percentile have evidence of myocardial injury and are not eligible for risk stratification at presentation). Patients with ST-segment elevation myocardial infarction and those who presented in cardiac arrest were excluded from this analysis. The prespecified primary outcome was a composite of type 1 myocardial infarction or cardiac death at 30 days. The prespecified secondary outcomes were recurrent myocardial infarction and cardiac death at 1 year. In addition, we evaluated the performance of cardiac troponin thresholds for the diagnosis of type 1 or type 2 myocardial infarction on index presentation. The number of patients available for each analysis is shown in Figure 1. Baseline characteristics are summarized as mean (standard deviation) or median (interquartile range) as appropriate. The primary outcome measure was the NPV of a high-sensitivity cardiac troponin I concentration of less than 5 ng/L at presentation. All cardiac troponin concentrations were rounded to integer values in line with clinical standards for reporting. When individual patient-level data were available, this was checked for consistency and completeness, and cohort-level summary counts of patients with and without the primary outcome were derived for a high-sensitivity cardiac troponin I concentration of less than 5 ng/L at presentation. In cohorts in which raw data were not available, the corresponding authors were asked to provide these summaries. The NPV was calculated at a cohort level using a Bayesian approach, with a binomial likelihood and beta prior (a noninformative Jeffreys prior with both shape parameters equal to 0.5), as this produces confidence intervals with better coverage when proportions are close to 0 or 1.13 Heterogeneity is reported using the I2 statistic.14 Survival free from cardiac death at 30 days and at 1 year is reported for patients with cardiac troponin I concentrations of less than 5 ng/L, 5 ng/L to the 99th percentile, and greater than the 99th percentile at presentation. For the primary outcome, the NPV was evaluated in prespecified subgroups stratified by age (≤65 or >65 years), sex, history of ischemic heart disease, time since symptom onset (≤2 or >2 hours), and presence of myocardial ischemia on electrocardiogram. Most cohorts defined myocardial ischemia as at least 2-mm ST-segment depression in 2 consecutive leads or new T-wave inversion. To explore the clinical implications of differences in performance between subgroups, we undertook these subgroup analyses in patients without myocardial ischemia on electrocardiogram. Studies have demonstrated imperfect calibration between high-sensitivity cardiac troponin I and T assays, with up to 17.5% of patients greater than the 99th percentile on the T assay shown to be less than the 99th percentile on the I assay.15 Therefore, a further analysis evaluated whether the assay used to adjudicate the index diagnosis affected the performance of the risk stratification threshold. In addition, we determined whether the assessed risk of bias and site of patient recruitment affected the NPV. Meta-estimates of the NPV were derived in the analysis population for all primary and secondary outcomes by modeling cohort-level proportions (true negative/[true negative + false negative]) in a binomial-normal random-effects model, with an additional term when cohort-level characteristics (adjudication assay, assessment of bias or location of recruitment) were compared. We estimated odds ratios for the difference in NPV between prespecified subgroups, meta-analyzing this across cohorts to obtain the mean odds ratio and a P value for the null hypothesis of no association. For cohorts in which individual patient-level data were available, the cardiac troponin threshold that would identify the highest proportion of patients as at low risk for an NPV at or above 99.5% was determined. For this analysis, we prespecified an NPV of 99.5% as being clinically acceptable and equivalent to a miss rate of 5 per 1000 low-risk patients.16 To evaluate how the inclusion of a risk stratification threshold would affect the overall diagnosis in all patients with suspected acute coronary syndrome, meta-estimates of NPV, positive predictive value (PPV), and sensitivity were derived for risk stratification thresholds alone (2-16 ng/L) and in conjunction with a nonischemic electrocardiogram result at presentation. At each threshold, the proportion of the total population classified as low risk and the miss rate per 1000 patients was reported. All analyses were performed in R version 3.2.2, with the meta-analyses performed using the metafor package.17 The analysis code is available online (eAppendix 5 in the Supplement). All corresponding authors from the 19 individual cohorts identified in the systematic review agreed to provide data for the meta-analysis. Individual patient-level data were obtained from 17 cohorts6-9,11,18-29 and aggregate data from 2 cohorts,10,30 for a total study population of 22 457 patients with suspected acute coronary syndrome (mean age, 62 [SD, 16] years; 41.5% women) (Table 1, Table 2, and Table 3). In 11 cohorts, data were available for the prespecified primary outcome of type 1 myocardial infarction or cardiac death at 30 days (Table 4). In the remainder, the outcome was index type 1 myocardial infarction (n = 1) or non–ST-segment elevation myocardial infarction on index presentation (n = 5) or at 30 days (n = 2). The assessed risk of bias was high in 11 cohorts because of patient selection or use of a contemporary reference standard (eAppendixes 3 and 4 in the Supplement). Across all cohorts, the proportion with the primary outcome was 12.4% (range, 2.4%-24.0%). The analysis population comprised 18 248 of 22 457 patients in which high-sensitivity cardiac troponin I concentrations were below the 99th percentile at presentation, and the prevalence of the primary outcome was 3.5% (range, 0.6%-6.1%). High-sensitivity cardiac troponin I concentrations were less than 5 ng/L at presentation in 11 012 patients (49%), with an NPV of 99.5% (95% CI, 99.3%-99.6%) (Figure 2 and Table 2) for the primary outcome and a total of 60 missed index or 30-day events (59 index myocardial infarctions, 1 myocardial infarction at 30 days, and no cardiac deaths at 30 days) (eTable 1 in the Supplement). The NPV was similar across cohorts with varying prevalence of myocardial infarction. The estimate of heterogeneity (I2) was 31.9%. Cohort-level 2×2 summary tables are provided for the analysis population in eTable 2 in the Supplement. When data were available in the analysis population (n = 16 537 [90.6%]), we estimated the NPV for the secondary outcome of index non–ST-segment elevation myocardial infarction (type 1 or type 2 myocardial infarction). Cardiac troponin I concentrations were less than 5 ng/L at presentation in 9574 patients (48%), with an NPV of 99.4% (95% CI, 99.2%-99.6%) and a total of 58 missed events. Meta-estimates of NPV were obtained in a number of prespecified subgroups (Figure 3). The NPV was lower in those with (98.2%; 95% CI, 96.4%-99.1% [n = 2178]) compared with those without (99.7%; 95% CI, 99.4%-99.8% [n = 13 709]) myocardial ischemia on electrocardiogram (P < .001) and in those who presented within 2 hours of symptom onset (99.0% [95% CI, 97.7%-99.5%] [n = 2303] vs 99.6% [95% CI, 99.4%-99.8%] [n = 11 101]; P = .003). Differences in the NPV were also observed between patients older than 65 years (NPV, 99.1%; 95% CI, 98.5%-99.5% [n = 6818]) compared with those aged 65 years or younger (99.6%; 95% CI, 99.4%-99.8% [n = 11 430]; P = .02) and in those with (NPV, 98.8%; 95% CI, 98.1%-99.3% [n = 3990]) compared with those without (NPV, 99.6%; 95% CI, 99.4%-99.7% [n = 10 170]) a history of ischemic heart disease (P = .03). When this analysis was restricted to patients without myocardial ischemia on electrocardiogram, estimates of NPV were higher than 99% for all subgroups (eFigure 1 in the Supplement). Performance of the risk stratification threshold was similar regardless of the assay used for adjudication (high-sensitivity cardiac troponin I: NPV, 99.6% [95% CI, 99.3%-99.7%] [n = 7046]; contemporary cardiac troponin I or T: NPV, 99.6% [95% CI, 99.3%-99.7%] [n = 5907]; high-sensitivity cardiac troponin T: NPV, 99.2% [95% CI, 98.6%-99.6%] [n = 5295]; P = .27), the assessed risk of bias (high risk of bias: NPV, 99.5% [95% CI, 99.1%-99.7%] [n = 7043]; low risk of bias: NPV, 99.3% [95% CI, 99.3%-99.6%] [n = 11 205]; P = .37), and the site of patient recruitment (Europe: NPV, 99.5% [95% CI, 99.1%-99.7%] [n = 11 714]; North America: NPV, 99.5% [95% CI, 99.0%-99.8%] [n = 2999]; Asia-Pacific: NPV, 99.5% [95% CI, 99.1%-99.7%] [n = 3535]; P = .30) (eFigure 2 in the Supplement). Follow-up data for cardiac death at 30 days and at 1 year were available in 12 953 patients (57.7%) and 9271 patients (41.3%), respectively (eTables 3 and 4 in the Supplement). In patients with cardiac troponin concentrations of less than 5 ng/L at presentation (n = 6956), there were no cardiac deaths at 30 days (NPV, 100% [95% CI, 99.9%-100%]; sensitivity, 99.4% [95% CI, 97.7%-100%]) and 7 cardiac deaths (0.1%) at 1 year (NPV, 99.9% [95% CI, 99.7%-99.9%]; sensitivity, 96.1% [95% CI, 92.9%-98.3%]). In patients with cardiac troponin concentrations between 5 ng/L and the 99th percentile at presentation (n = 3817), there were 19 cardiac deaths at 30 days (0.5%) and 58 (2.1%) at 1 year. In comparison, in those with troponin concentrations above the 99th percentile (n = 2180), there were 62 cardiac deaths at 30 days (2.8%) and 125 (8.2%)  at 1 year. In patients with troponin concentrations of less than 5 ng/L at presentation and an index or 30-day myocardial infarction, there were no cardiac deaths at 30 days or at 1 year. Because the majority of studies did not adjudicate recurrent myocardial infarction events at 1 year, we were not able to conduct this prespecified analysis. In all patients with suspected acute coronary syndrome for whom individual patient-level data were available (n = 18 601 [82.8%]), we evaluated how different risk stratification thresholds would affect the NPV and sensitivity for the primary outcome. When used in isolation, a troponin I concentration of less than 5 ng/L gave an NPV of 99.5% (95% CI, 99.3%-99.7%) and a sensitivity of 98.0% (95% CI, 96.4%-98.9%), identifying 49.1% of patients as low risk with a miss rate of 5.4 (95% CI, 4.0-7.0) per 1000 patients. At a threshold of less than 2 ng/L, the NPV was 99.8% (95% CI, 99.0%-100%) and the sensitivity was 100% (95% CI, 98.9%-100%), but the proportion of patients identified as low risk was lower at 13.7%. Although the absolute number of missed cases was lower, the miss rate was similar at 4.1 (95% CI, 2.0-6.9) per 1000 patients (eTable 5 in the Supplement). There are a number of strengths to the analysis. This was a prespecified systematic review and meta-analysis that included individual patient-level data from all cohorts identified. The findings were consistent across a range of health care settings and geographic regions with considerable differences in the prevalence of myocardial infarction. Individual patient-level data were included from more than 22 000 patients, allowing a meaningful analysis of important subgroups. All studies were prospective, and in all studies the final diagnosis was adjudicated according to the universal definition of myocardial infarction. In clinical practice, cardiac troponin concentrations are interpreted in conjunction with the electrocardiogram and clinical assessment. When a risk stratification threshold of less than 5 ng/L was evaluated in the subgroup of patients without myocardial ischemia on electrocardiogram, the NPV and sensitivity were excellent. To ensure that safety estimates were conservative, performance was evaluated not just for an index diagnosis but for a composite end point that included events up to 30 days. Although there were 81 cardiac deaths at 30 days, none occurred in the 6956 patients with cardiac troponin I concentrations less than 5 ng/L. Furthermore, performance was similar for both spontaneous type 1 and secondary type 2 myocardial infarction. This is relevant because the diagnosis of type 2 myocardial infarction is more challenging and is associated with a worse prognosis.35-37. At a threshold of 5 ng/L, the analytical performance of the high-sensitivity cardiac troponin I assay is excellent.6,38 The use of lower thresholds did not improve diagnostic accuracy. A miss rate of 5 per 1000 patients was observed when applying less than 5 ng/L as the risk stratification threshold, with a miss rate of 4 per 1000 patients observed at a threshold of less than 2 ng/L. Although the true risk of missing an individual patient with myocardial infarction is the same at both thresholds, lower thresholds reduce the proportion of patients classified as at low risk; only 1 in 10 patients had a troponin I concentration of less than 2 ng/L compared with 5 in 10 patients a with concentration of less than 5 ng/L. Use of lower thresholds would result in more patients without myocardial infarction being admitted for serial testing and further investigation, with an increase in health care expenditures. This study has several limitations. First, not all cohorts used identical protocols, with differences both in the inclusion criteria and the diagnostic criteria used for adjudication (eAppendix 3 in the Supplement). However, no significant differences in NPV were observed when stratified by adjudicating assay, and the NPV was high across individual cohorts, suggesting that these findings are generalizable. Second, the percentage of patients who presented early after onset of symptoms was low at just 10% of the study population. Despite observing an NPV of 99% in this subgroup, inconsistencies in the documentation of symptom onset across cohorts may affect the analysis, and until further research is available, serial testing is recommended in patients presenting within 2 hours of symptom onset.5 The greatest number of false negatives was observed in the cohort with the shortest median symptom onset to sample time (179 [interquartile range, 119-349] minutes), which may explain the lower NPV and sensitivity reported at this threshold in a previous study.8 Third, while it is reassuring that patients with troponin I concentrations of less than 5 ng/L had a much lower rate of cardiac death at 1 year than did patients with concentrations between 5 ng/L and the 99th percentile, this observation needs to be verified in prospective studies in which patient care is guided by this approach. In Reply In response to the query by Dr Douma and colleagues regarding the HR of 0.83 reported in the multivariable analysis of overall survival, the clinical meaningfulness bar of an HR of 0.8 suggested by working groups of the ASCO Cancer Research Committee is intended for application to a different context, namely clinical trials of costly and toxic antineoplastic therapies.1 Although there is some cost associated with infrastructure and staffing for an online symptom-reporting intervention, the intervention itself is nontoxic and reduces rather than increases resource utilization (ie, leads to reduced emergency department visits). In this context, we think that the observed increase in overall survival is clinically meaningful, an assertion further supported by Sobrero and colleagues2 who noted that a benefit of the magnitude observed in our trial is clinically meaningful if cost and toxicity are moderate. Moreover, the intervention was associated with a clinically meaningful and significant gain in health-related quality of life,3 which we believe independently substantiates its value to patients. Based on these findings, criteria in the ASCO Value Framework would classify this intervention favorably.4. In response to the second query, overall survival results for the subgroups of patients with or without substantial prior computer experience are shown in the Figure. The unadjusted HR for each was 0.77, with a P value of .02 for the 539 patients with computer experience and 0.10 for the 227 without computer experience. Because the study was not designed with a sample size to assess overall survival in these 2 subgroups, it cannot be determined if the P value result in the inexperienced subgroup is attributable to underpowering or to lack of statistical significance. Therefore, future research would be necessary to address this question. For now, the results of our trial reflect overall survival benefits in a population of patients with various cancer types receiving systemic cancer therapy (chemotherapy, targeted therapy, or immunotherapy) who reported their symptoms online during chemotherapy, with automated email alerts to their nurses for severe or worsening symptoms. These findings are being further evaluated in a national multicenter trial (NCT03249090). They emphasized that administrative data—derived from insurance claims submitted to health plans—lack information on the specific brand and model of implant used. If claims were to include such device-identifying information, researchers could then use claims data to study safety more comprehensively than through voluntary reports (which are often underreported2); evaluate the frequency of device failures; and assess the total costs of care associated with different products. Researchers already use claims data in this way to monitor the safety of prescription drugs. The lack of device information in claims inhibits similar uses for medical implants.3. With the challenges inherent to creating a new claims form in mind, we find ourselves compelled to call for an approach leveraging the current existing claims form. At present, the claims form already has space for a procedure code (ie, the Current Procedural Terminology [CPT] codes or the International Classification of Disease [ICD] procedure codes) that is used routinely and required for billing. Legislation that would require each device to carry its own procedure code for placing and removing the device (as was incidentally the case for the gastric band) would effectively allow this already existing section of the claims form to also serve as a unique device identifier. Although device-specific procedure codes may not carry the same granularity as an individual unique device identifier, the example of the gastric band (in both CPT1 and ICD billing codes, as illustrated in our Viewpoint) demonstrates how even such a crude identifier could detect alarming trends in device safety and value. Moreover, focusing on device-specific procedure codes would allow researchers and regulators to take advantage of the robust methods that already exist for using CPT and ICD Clinical Modification coding algorithms within claims data. In the Viewpoint entitled “Vaccination Challenges in Confronting the Resurgent Threat From Yellow Fever” published online October 5, 2017,1 several errors were published. In the first paragraph, the name of the mosquito genus should be spelled Haemogogus. The second paragraph should end with the phrase “…owing to limited supply.” In the fourth paragraph the data for estimated risk of yellow fever in travelers to South America during epidemics should be 5 per 100 000. In the reference list, citations 9 and 10 should be reversed. This article was corrected online. Data markers indicate observed rates and solid colored lines indicate modeled rates. The error bars represent the 95% CIs of the observed rates. A, No significant trends for annual percentage change by age group were noted for males. B, Among females, the significant trends for annual percentage change by age group were 2009 to 2015 (18.8 [95% CI, 12.1-25.8]) for 10 to 14 years, 2008 to 2015 (7.2 [95% CI, 3.8-10.8]) for 15 to 19 years, and 2001 to 2015 (2.0 [95% CI, 0.8-3.1]) for 20 to 24 years. Self-inflicted injury ED visit rates were calculated from 2001 through 2015 by sex, age (10-14, 15-19, and 20-24 years), along with injury method (poisoning, sharp object, blunt object), and 95% CIs using US Census population estimates as denominators. Rates were weighted to obtain nationally representative estimates and age-adjusted to the 2000 US Census population. Trends in self-inflicted injury ED visit rates were assessed using joinpoint regression software (Surveillance Research Program, National Cancer Institute), version 4.3.1.0. The annual percentage change described the rate of change for each linear segment. From 2001 to 2015, NEISS-AIP captured 43 138 youth self-inflicted injury ED visits. The overall weighted age-adjusted rate for this group showed no statistically significant trend until 2008, increasing 5.7% (95% CI, 3.0%-8.4%) annually thereafter and reaching 303.7 per 100 000 population (95% CI, 254.1-353.3) in 2015 (Table). Age-adjusted trends for males overall and across age groups remained stable throughout 2001-2015 (Figure, Table). Overall age-adjusted rates for females demonstrated no statistically significant trend before 2009, yet increased 8.4% (95% CI, 5.6%-11.2%) yearly from 2009 to 2015. After 2009, rates among females aged 10 to 14 years increased 18.8% (95% CI, 12.1%-25.8%) per year—from 109.8 (95% CI, 69.9-149.7) in 2009 to 317.7 (95% CI, 230.3-405.1) per 100 000 population in 2015. Rates among females aged 15 to 19 years showed a 7.2% (95% CI, 3.8%-10.8%) increase per year during 2008-2015. Rates among females aged 20 to 24 years exhibited a 2.0% (95% CI, 0.8%-3.1%) increase per year throughout 2001-2015 (Figure, Table). Youth self-inflicted injury ED visit rates were relatively stable before 2008. However, rates among females significantly increased thereafter—particularly among females aged 10 to 14 years, who experienced an 18.8% annual increase from 2009 to 2015. This study only included ED cases; thus, rates were underestimated. Also, limited statistical power could have resulted in some trends not showing statistical significance. Findings are consistent with previously reported upward trends in youth suicide rates during 1999-2014, in which rates increased most notably after 2006 with females aged 10 to 14 years experiencing the greatest increase.4 Findings also coincide with increased reports of depression among youth, especially young girls.5 Other potential underlying reasons for the observed increasing trends, particularly among young females, warrant further study. Physicians are still taught to diagnose patients according to the 19th-century Oslerian blueprint. A physician takes a history, performs an examination, and matches each patient to the traditional taxonomy of medical conditions. Symptoms, signs, family history, and laboratory reports are interpreted in light of clinical experience and scholarly interpretation of the medical literature. However, diagnosis is evolving from art to data-driven science, whereby large populations contextualize each individual’s medical condition. Advances in artificial intelligence now bring insight from population-level data to individual care; a recent study sponsored by and including researchers from Google used data sets with more than 11 000 retinal fundus images to develop a deep learning algorithm that outperformed clinicians for detecting diabetic retinopathy.1. However, when clinicians make genetic diagnoses, they are practicing more like Osler than Google. The pathogenicity of a genetic variant is often determined from cohort studies of relatively small numbers of individuals. Even a simple comparison of a patient’s variant to a larger population of matched ancestry is generally not possible. Manrai et al2 illustrated the pitfalls of this approach, showing that monogenic variants considered diagnostic of hypertrophic cardiomyopathy, in fact, have a high frequency in unaffected individuals of African ancestry and, therefore, often apparently represent normal variants among black patients. This problem is more general and many studies implicate pathogenic variants, yet lack sufficient numbers of ancestrally diverse cases and controls. Furthermore, Van Driest et al3 reviewed electronic health record (EHR) data and electrocardiographs in a cohort of 2022 genotyped patients and found that the majority of participants (41 of 63) with a designated variant in either SCN5A or KCNH2—putatively associated with cardiac rhythm disturbances—had no identifiable pathological phenotype. Artificial intelligence will eventually help clinicians extract the maximum knowledge from large genetic reference data sets. But the first step is to simply be able to calculate the statistical genetics that reveal how often a variant is associated with pathology, and how outcomes compare in patients with and without the variant. To make a genetic diagnosis, a physician must evaluate a patient’s data against a larger and representative population. A high-functioning health care system not only needs the EHR databases produced as a by-product of care, but also must link EHRs to samples, sequence data, and myriad data sources needed to characterize medical care, lifestyle, and environment. Initiatives to develop genetic reference data at the population level could be grouped into 3 categories. First are well-known databases of genotype-phenotype relationships as observed and submitted by researchers (eg, Online Mendelian Inheritance in Man, ClinVar, and the National Human Genome Research Institute’s Genome-Wide Association Study [GWAS] Catalog). Second are databases, such as the Genome Aggregation Database (gnomAD),4 the next iteration of the Exome Aggregation Consortium (ExAC) database,5 and the 1000 Genomes Project,6 that aggregate sequences collected from other studies for secondary use. Third, patients and other study participants are invited to donate data to registries like GenomeConnect or enroll in cohorts like the National Institutes of Health All of Us initiative, which is recruiting 1 million patients to contribute biological samples and EHR data for research. All 3 types of databases rely on a research framework rather than a clinical framework for accrual of patients and data. This distinction is important. The populations are selected by researchers or are self-selected, and the data are either deidentified or acquired after a research consent. The bias inherent in these populations may distort the accuracy of data-driven genomic diagnosis. Furthermore, even though deidentification addresses the Health Insurance Portability and Accountability Act of 1996 (HIPAA) privacy concerns, it often precludes future linkage of myriad data sets needed to create a robust information commons. The architects of All of Us have instrumented enrollment centers to provide ongoing longitudinal phenotype data from EHRs. Still, relying on consented research participants is not population-based and success is limited by the willingness of individuals to participate and the expense and logistics of consenting. For clarity and transparency, the patient should be informed of the compact in writing. This could be accomplished through the consent-to-treat document signed in prelude to care or the notice of privacy practices. The compact requires a patient understanding that residual volumes from blood and tissue samples may be stored, linked to EHR data, and used in a CIC for a patient’s own medical care or the care of others. Patients could benefit from the compact by having a care system in which diagnosis may be more accurate. Hospitals could implement the requisite informatics and laboratory processes to capture residual biospecimens, including blood and pathology specimens (Figure). Additional testing beyond what is used clinically could be performed on these specimens and the results integrated. The CIC should be available to decision support systems in EHRs. In the process of diagnosis, the CIC could be queried in a dynamic fashion, such that an individual can be compared with a population based on her characteristics. The implementation of a CIC is sociologically, financially, and technically complex. Only larger institutions could implement the necessary biobanking. Importantly, the institutional CIC would be reflective of the patient population treated at that institution. Even though an institution-specific CIC is useful, joining deidentified data from CICs across institutions would produce even greater benefit. Unlike most cancer therapies that are identical from patient to patient, CAR-T therapies are made by removing the T cells of a patient, genetically modifying them to respond to certain targets expressed on the patient’s cancer cells, and then reinfusing the cells. When the T cells come into contact with the relevant target (for instance, CD19 in the case of ALL), they proliferate while secreting a number of programmed substances including inflammatory cytokines that destroy the cancerous cells. Targeted killing of tumor cells by lymphocytes was first suggested by the graft-vs-leukemia effect in bone marrow transplantation,2 but that effect and the infusion of donor T cells more generally has no effect on solid tumor malignancies or most hematologic cancers. The innovation underlying CAR-T involved exploiting the specificity of antibody-mediated recognition of tumor antigens, and then engineering CAR-T cells to have the relevant antibody fragment fused to the T-cell receptor. Thus, the “living drug” infused into the patient is the patient’s own T cells altered to express a receptor that is specific for the target antigen on the tumor. The impressive science aside, the treatment can have substantial adverse effects. Tisagenlecleucel carries a boxed warning regarding its tendency to cause cytokine release syndrome. The condition that can vary from just high fevers for a few days to hypotension, multiorgan failure, and even death, occurred in around half of patients in the pivotal approval study. Confusion, lethargy, stroke-like symptoms, and coma have also occurred. CAR-T therapies are effective at killing any cell, normal or cancerous, that expresses the target, so damage to normal cells and tissues and long-term and likely permanent toxic effects can be considerable. Also, the process of extracting the patient’s T cells from the body, modifying them, and returning the cells to the patient is technically challenging. Accordingly, for the foreseeable future this new type of treatment will (and should) only be available at institutions that have broad clinical capabilities and experience in hematopoietic cell transplantation. The Institute for Clinical and Economic Review (ICER) is planning to review CAR-T cost-effectiveness in early 2018, which will provide an estimate of the appropriateness of the price of this therapy relative to clinical benefits. But ICER’s review, just as in a recent hypothetical review of these products from the UK National Institute for Health and Care Excellence (NICE), depends substantially on the costs of what the treatment is replacing. That is because the incremental cost-effectiveness that both organizations focus on measuring is a product of both the change in health gain associated with the newer treatment (compared with the existing one) and the difference in the costs of the treatments. This unfortunate calculation means that a very expensive treatment can be incrementally cost-effective almost entirely because the existing treatment costs an enormous amount.4 In this case, one clear comparator treatment will be Amgen’s blinatumomab, which at $178 000 for a typical treatment course of 6 weeks is also priced outside the normal range of cancer treatments. Thus, comparison to high-cost alternatives can be misleading (this is the same mechanism that allows a BMW to look like a bargain when the only other car on the lot is a Ferrari). In addition, the $475 000 price substantially understates the total per-patient cost when expensive toxicities such as cytokine release syndrome are frequent, and extensive preinfusion and postinfusion treatments are regularly needed. An argument that this is for a limited population (childhood ALL is a rare disease) and so the overall effect on health care costs will be manageable may be accurate today but specious tomorrow given the large numbers of tumors for which CAR-Ts potentially might be  used. Studies are under way not only in lymphomas, but also in common solid tumors, inclusive of lung, prostate, pancreas, breast, and colon cancers, with variable degrees of preliminary success. If CAR-Ts are successful at improving the outcomes of patients with these other conditions, the health benefits could be substantial, but the financial cost to patients and the health care system could be consequential. Novartis has announced plans to enter into outcomes-based contracts for the drug’s first indication, a step toward value-based pricing, which has the potential to tie the price of a drug to the magnitude of the benefits it delivers.7 Under the outcomes-based contract, Novartis would only be paid for the drug if the patient were to have a response. While this is a welcome innovation in reimbursement, Novartis under this arrangement would still be expected to be paid for 83% of cases (if clinical experience parallels the trial). Alternative strategies could be used. For example, the company could only charge for patients who were disease free after 3 months, or who had no residual disease at 30 days (not those with minimal residual disease). While these end points are better surrogates of long-term benefit from this expensive therapy, the truth is that all near-term outcomes will be imperfect surrogates of benefit, and choosing proximal outcomes minimizes the complex administrative and financing exigencies intrinsic to them. Arguably a more important issue than choosing the right end point is starting at the right price in the first place.8. Novartis has also indicated that the company will price future indications differently—called “indication specific pricing”—meaning the same drug will have more than one price in the marketplace.9 This is another step toward having value-based pricing. The approach makes sense when the value of a therapy is vastly different across its uses. Under indication-specific pricing, the next approved indication of tisagenlecleucel (presumably for patients with relapsed or refractory diffuse large B-cell lymphoma) would likely have a lower price because efficacy will be lower and market size larger. Some of the administrative and marketplace challenges to indication-specific pricing are highlighted in reports10 as are concerns about how the approach may shift profits. Even as scientific and pricing challenges evolve, the hype around CAR-T could also undermine its rational use in clinical practice. Progress in oncology, as in most other conditions, is gradual and hard fought. Yet new therapies are too often characterized to the public as miraculous and awe inspiring. CAR-T treatments, because of the remarkable science, may be particularly likely to be described in terms that overstate the benefits while understating the harms. Health professionals should seek to consistently explain that this therapeutic approach has promise along with serious toxicity. When counseling patients, clinicians should explain the broad variation in outcomes, ranging from severe toxicities and occasionally death, to those that have durable remissions. The term “cure,” bandied about all too often in the media, should be excluded from any serious conversation about CAR-T, replaced with discussions that focus on what is known of their potential today. The reason is that the term cure will connote to some patients that a single infusion of this scientifically mystifying, genetically driven therapy will first vanquish their disease and then reset their life expectancy to normal. Perhaps one day data will support such a hope; as of yet the available data are insufficient to claim much more than CAR-T treatments have large promise at an enormous price. Affording health care has become a pressing national concern. According to a 2016 Gallup survey, 27% of individuals in the United States identified affordability as the country’s “most urgent health problem.”1 The level of concern is both severe and new. In the late 1990s, HIV/AIDS was the top health concern. In 2008, the leading health concern was accessing medical services. Today, 57% of individuals report that they worry “a great deal” about “the availability and affordability of healthcare.”1 More than 60% indicate that prescription drug prices should be a top health care priority. More than a quarter of individuals have postponed care owing to cost.2. When discussing high health care costs, academics and policymakers cite multiple measures: total national health expenditures, health care inflation, health care spending as a percentage of the US economy, and health care “waste.” Many individuals in the United States, and even many health care professionals, have difficulty grasping these macroeconomic indicators. How much is $3.2 trillion? Furthermore, these measures are deeply impersonal. They describe health care spending at the national level. Why should a family be concerned if health care is 17.5% of gross domestic product? Most important, these measures track expenditures, not affordability. They do not describe the ability to pay for health care. No index is perfect. Other policy experts might favor other measures of the affordability of health care. For instance, the denominator might be mean income per household with 1 or more workers. This Alternative Affordability Index would demonstrate the same trend, doubling since 1999 and plateauing after 2011, but with a lower 2016 value of 18.4% because of higher average earnings of working families (eFigure 1 in the Supplement). Others might use the median or mean total compensation rather than income to calculate the index. However, this would double-count employer contributions for health insurance. Moreover, the budgets of most families are based on income, not total compensation. Indeed, mean and median total compensation are neither regularly tabulated nor easily accessible. Employer-sponsored health insurance (ESI) is a mainstay for the US middle class. Fully 56% of individuals, approximately 178 million, receive health insurance through ESI. Given the mechanics of third-party payment, individuals with ESI experience health care costs through the purchase and use of health insurance products. Because employers contribute most health insurance premiums as pretax fringe benefits and insurers pay most health care expenses, most individuals find it difficult to grasp the link between health care costs, insurance premiums, and wages. A companion proposed indicator is the Comprehensive Affordability Index, which would include average out-of-pocket (OOP) expenses (co-pays, co-insurance, and deductibles) in addition to insurance premiums (eFigure 2 in the Supplement). The inclusion of OOP costs would generate a more comprehensive assessment of affordability, but also introduces definitional and comparability challenges. There are at least 3 different OOP measures: the Agency for Healthcare Research and Quality’s Medical Expenditure Panel Survey (MEPS),5 the Bureau of Labor Statistics (BLS) Consumer Expenditure Survey,6 and the Milliman Medical Index (MMI).7 In 2015, the Agency for Healthcare Research and Quality’s MEPS average OOP expenses for a family was $1589 (calculated)5; the BLS figure was $1365 for a 2.5-person consumer unit6; and MMI estimated OOP costs of $4065 for a family of 4 with an employer-sponsored PPO plan.7 Another complication is that OOP expenses are heavily skewed, with a per-family OOP mean of $1589 and median of $766 (calculated from 2015 MEPS data). This differential indicates that a few families have very high OOP spending, whereas most have little to no OOP medical costs. The proposed Affordability Index has several potential advantages. Its numerator (mean cost of insurance premiums) and denominator (median household income) are reliable, credible, and generated annually. This information is accessible online without complicated programming or calculations. Furthermore, the Affordability Index is widely understandable. It places health insurance costs within an accessible context: as a percentage of income. The index is personal, quantifying the increasing burden of health care costs for average individuals in the United States. This measure of health insurance costs as a percentage of wages reveals the direct relationship between health care costs and salaries. Higher health care costs depress incomes. As economists indicate, employers ultimately are concerned about total compensation (salary and benefits) per employee. Wages are simply a component of total compensation. Employer “contributions” to health insurance are part of total worker compensation. If the cost of health insurance were to decrease, a greater percentage of total compensation could convert to cash wages. The Affordability Index quantifies the downward pressure that increasing health care costs exert on wages. The Affordability Index also has potential disadvantages. The index is applicable for those with employer-sponsored insurance and may not reflect the financial burden of health care for those with Medicaid or Medicare coverage or the uninsured. In addition, premiums for employer-sponsored insurance do not capture all financial burdens of health care on individuals. It captures only direct costs and ignores indirect financial burdens such as the time and travel expenses associated with accessing specialists and other specific services, the time costs and potential lost wages of administering medical treatments, or caregiving at home for relatives. In 2016, the Affordability Index would have been 30.7%, reflecting a mean employer- sponsored family insurance premium of $18 1423 and median household income of $59 039.4That is, the mean family health insurance premium (combining both employer and employee contributions) represented 30.7% of median household income (Figure). The Comprehensive Affordability Index for 2015 (the last year for which OOP data are available) would have been 33.9%, again doubling from 1999 when it was 16.7% (eFigure 2 in the Supplement). The same trend is seen if the Comprehensive Affordability Index is based on mean income from households with 1 or more earners, doubling from 10.8% in 1999 to 20.4% in 2015 (eFigure 3 in the Supplement). To put it in context, in 2015, the average US household spent an estimated $7023 on all food purchases (including out-of-home purchases), so health care was nearly 2.5 times as expensive as food.6 This level of health care spending seems difficult to justify. Using longitudinal data, since 1999, the Affordability Index would have more than doubled from 14.2% to 30.7% (Figure). Between 1999 and 2016, the mean cost of a family health insurance plan increased 4.7 times faster than median household income (213% vs 45%). If the proposed Affordability Index had remained constant at 14.2%, median household income would have been $9741 greater ($68 780 vs $59 039) in 2016. Even if employers had shared only half of these savings with employees, US families would have seen a meaningful $4800 increase in household incomes. Potential applications of the Affordability Index are manifold. For example, a reasonable policy objective would be to maintain the Affordability Index at its current 30.7% level. Concomitantly, the Secretary of Health and Human Services could use the index and its component parts to identify industries with disproportionately increasing or decreasing health care costs. States could also use the index to pressure health care organizations and clinicians to reign in prices that exceed growth in household income before granting payment increases. Although the functions of an integrator are similar across communities, the entity that serves those functions will differ based on organizational capabilities and community needs. Awardees demonstrate the diversity of communities and integrators poised to execute this cross-sector approach to health improvement. The University of Kentucky is leveraging its trusted status as an academic center to reach communities across the state, including areas of Appalachia experiencing some of the highest rates of opioid addiction and related deaths. The Delta Health Alliance, a nonprofit network of community health centers, serves communities in southern Mississippi with some of the nation’s lowest life expectancies. The Health Collaborative applies its expertise as a nonprofit, data-driven health care improvement organization to lead a broad coalition of community partners focused on improving the health of communities in Cincinnati, Ohio, where life expectancies can vary by more than 20 years across neighborhoods.10. In addition to serving as integrators at the community level, these awardees will engage individual Medicare and Medicaid beneficiaries directly by screening for health-related social needs at participating clinical delivery sites and providing referral to organizations that provide resources in the community to help address unmet needs. Individuals at high-risk for poor health outcomes, demonstrated by visiting an emergency department 2 or more times in the previous 12 months, will be provided navigation services by awardees, aimed at tailoring referrals to an individual’s specific context, encouraging engagement of recommended community resources, and following up to determine whether the individual’s needs were met or more assistance is needed. This direct engagement with community members will provide awardees with important information that will help them to identify community resource gaps. The integrators will then be well positioned to convene and mobilize the key community stakeholders to address these gaps and expand the community’s ability to improve the health of the population. Ever since third grade, the octopus has been my least favorite animal. Not that any octopus ever did anything to me; on the contrary, I’ve only seen one in an aquarium, and I’m sure it took no special notice of me, nor I of it. In fact, given their keen intelligence, octopi should actually command some notion of respect within the animal kingdom. That said, my mind has chosen to overlook this fact and focus instead on the image of a pale, purple, cartoon octopus silently staring back at me from the ceiling of a postoperative recovery room. My story doesn’t begin in third grade; rather, it starts when I entered the world. Born with proximal femoral focal deficiency (PFFD), a birth defect characterized by a shortened femur and dysplastic acetabulum, I have lived my entire appreciable life—from preschool to medical school—as an amputee. Given the number of letters in the name of my condition, it’s safe to assume that it’s relatively rare. Consequently, the typical conservative response from many physicians was not to offer options, but to predict I’d never walk well. And then I was introduced to the aforementioned octopus. At the age of nine, I underwent the third in a series of rotationplasties that have since allowed me to walk. At some point in the postanesthesia care unit, the team severely underestimated my analgesic needs. With pain searing through my hip and everyone hastening to get things under control, all I could do was stare at the ceiling. On that ceiling, an octopus stared observantly back at me. As to why an octopus with eight “legs” adorned an orthopedic hospital for patients with a general lack thereof, I will never know. But, focusing on the octopus, a number of questions ran through my mind. Why was this happening to me? Would I ever be normal? Was all of this worth it?. These are the questions I reflect upon when I consider where I want to go, what I want to do, and who I want to be. They are, in part, what drove me toward a career in medicine, toward a career in helping others with problems like my own. But to simply view my disability as a catalyst for a career choice is to ignore the countless obstacles, pains, and failures that have accompanied it. These are experiences I wish I had never encountered, yet every cloud has a silver lining, and these experiences have affected my views regarding disability, patients, and how I wish to approach clinical practice in the future. Although I use it for the sake of communication, I do not, in and of itself, like the term disabled. I don’t wish to argue against its context because it would be a lie to claim that there was nothing I couldn’t do given my condition. The term emphasizes the entirely wrong aspect of a person with a disability, focusing on what he or she cannot do rather than on any other defining characteristic. This emphasis speaks volumes on the image of disability in society. To this day there is a certain stigma—sometimes obvious, sometimes not—regarding disability. Society, with its use of the term disabled, demonstrates a fondness for characterizing people by their medical conditions. Ray Charles, one of the most talented musicians of his time, is known as a wonderful blind musician. The image of Stephen Hawking, one of the most impactful physicists of this era, is intimately tied to that of his assistive devices. None of these descriptors are inherently wrong; however, it is as if the lives of those who live with disabilities are defined first by their disability and only subsequently by the rest of their person. What makes this emphasis negative is not the emphasis itself but the damaging stigma subconsciously tied to it. As a society, we often think that such a connotation does not exist, but the evidence says otherwise. During the last presidential election, controversy erupted surrounding the alleged public mocking of a reporter for his physical disability. In 2016, the movie Me Before You garnered significant backlash from various groups due to its portrayal of assisted suicide for someone living with a disability. Are these the types of statements that we want to make to the disabled community?. While this stigma may appear isolated to specific events, it has overarching effects on the people with disabilities. For all people of working age, able-bodied individuals have an employment rate approximately twice that of those who are not.1,2 Additionally, those with disabilities who are employed earn a significantly lower median salary than those without disabilities.1 Certainly it could be argued that many jobs require a definite level of physical skill that some individuals with disabilities may not have. However, according to the “Americans With Disabilities: 2010”3 census report, education—which forms the foundation upon which many people build their careers—follows the same trend, even when only considering those with “nonsevere” disabilities. This occurred in spite of the Individuals With Disabilities Education Act,4 which “[ensures] services to children with disabilities throughout the nation.”. But what do physicians and other health care professionals have to do with this? Admittedly, it is impossible for a group of people to change the subconscious outlook of a nation immediately. To be fair, health care professionals do more than most to help. We provide prosthetics to patients with amputated limbs, treat cerebral palsy with baclofen pumps, and administer disease-modifying therapies for multiple sclerosis. While these innovations have enormously improved the lives of many individuals, this is only half of the battle. Prosthetics don’t fix workplace discrimination. Baclofen pumps don’t absolve a lack of educational resources. Disease-modifying therapies, as useful as they are, don’t improve access to health insurance. Much of what we do, as a medical field, is targeted toward our patients’ medical conditions. In treating these conditions, we often forget to consider the rest of the patient’s life. Yet it is that aspect that often needs treatment the most. Addressing this stigma is difficult, and it is (or should be) the subject of further debates about approaching patients with disabilities. As we treat underlying medical conditions, we must help return patients to “normal” life. In children, this means not only securing access to equal educational resources but also inspiring the personal confidence that they can succeed in school. For adults, this means not only ensuring that they are capable of working but also instilling the desire to contribute to society and the confidence that they can make a difference. In all cases, this means ensuring that the image of disability is secondary to that of someone’s accomplishments, aptitudes, and character. My image is, despite my best wishes, intimately tied to my medical condition. Among strangers, I am not a medical student, or an engineer, a researcher, or a college graduate—I am just an amputee. But I suppose that, in my own mind, I feel somewhat the same way. I will always face challenges, both old and new. These challenges are central to what defines me as an individual, and they have shaped me both as a person and a professional. I am often reminded of the questions that ran through my head while staring at the octopus above me, just as I reached the age at which I could understand what lie ahead. I will never have an answer as to why I was the one afflicted with PFFD but, whether a simple genetic mutation or part of God’s larger plan, it doesn’t matter. I will never be “normal,” in the strictest sense of the word. But was it all worth it?. I am not thankful that I was born with a disability. I do not celebrate it, nor is it something that I would wish upon anyone else. That said, it is the proverbial hand I was dealt, and there’s no arguing with the dealer. All I can do is face it, move forward, and take lessons from my experiences that I would not have learned otherwise. These lessons have shown me the challenges that thousands of patients face each day. It is these challenges, and the stigma involved, that we must actively address as a medical community to continue to improve the lives of those living with disabilities. Then, and only then, can we truly say that we have provided the best possible care. The Shannon diversity index represents the relative abundance and evenness of bacterial species in a community. The index is highest in communities with large numbers of species with near equal representation, and lowest in communities with low numbers of species with skewered distribution. Shannon diversity index was significantly different between patients with recurrent C difficile infection in the capsule or colonoscopy groups and the donors before fecal microbiota transplantation (FMT) (P < .001) and between each group of patients before and after FMT at 1, 4, and 12 weeks (P < .01). There was no difference between the capsule and colonoscopy groups at any point. Some donors had provided 2 samples for the microbial composition analysis. Each patient is represented by a single point. The lines within boxes represent medians, while the edges of the boxes represent lower and upper quartiles. The whiskers represent the range of the data set. PCoA is a method to visualize similarities or dissimilarities in high-dimensional data. In this case, it assigns each patient’s microbial composition to a location in a 2-dimensional graph indicated by principal coordinate 1 (30.9%; x-axis) and principal coordinate 2 (8.4%; y-axis) where the distance between any 2 samples is a measure of their similarity (smaller distance for higher similarity). The microbial composition of patients with recurrent C difficile infection (red circles and triangles) was significantly different from donors (black squares) before fecal microbiota transplantation (FMT) but shifted toward the donor profiles 1 week after FMT (permutational multivariate analysis of variance P = .001). At 12 weeks, a few individuals appeared to revert back toward pre-FMT profiles (gold circles and triangles). Clostridium difficile infection (CDI) is the main cause of health care–associated infectious diarrhea. In a meta-analysis of primary studies published in 2002 and 2007, the annual cost of CDI in the United States adjusted to 2012 dollars was estimated at $1.5 billion.1 Following a course of antibiotic therapy for CDI, 10% to 30% of patients will experience a recurrence, with the risk approaching 60% after the third episode. Managing recurrent CDI (RCDI) remains a major challenge. Fidaxomicin can reduce the risk of recurrence; however, studies to date only included patients with primary or first recurrent infections.2 Bezlotoxumab, a monoclonal antibody against C difficile toxin B, is efficacious at reducing RCDI compared with placebo.3 However, the absolute risk reduction is modest, with the number needed to treat to prevent a recurrence being 10. Adult inpatients and outpatients, aged 18 to 90 years, with at least 3 documented episodes of CDI were recruited. In the absence of an alternative cause of diarrhea, each episode was defined as recurrence of diarrhea (>3 unformed bowel movements every 24 hours) within 8 weeks of completing a prior course of treatment, with either a positive C difficile toxin by glutamate dehydrogenase and C difficile toxins A/B (C diff QuikChek Complete; Techlab) or by detection of glutamate dehydrogenase and C difficile cytotoxin B gene (Cepheid), plus resolution of diarrhea for the current episode. Exclusion criteria included complicated CDI as defined by Surawicz and colleagues14; chronic diarrheal illness; inflammatory bowel disease (IBD), unless in clinical remission 3 or more months prior to enrollment; cancer undergoing therapy; subtotal colectomy, colostomy, or ileostomy; dysphagia; life expectancy of less than 3 months; pregnancy; breastfeeding; and conditions requiring antibiotic therapy. Written informed consent was obtained before screening. This study was approved by Health Canada (control No. 176567) and the ethics board of each participating center. The full trial protocol is available in Supplement 1. This randomized, noninferiority trial compared FMT delivered by capsule with colonoscopy. Potential participants were enrolled between October 2014 and September 2016, with follow-up to December 2016 at 3 academic centers in Edmonton and Calgary, Alberta, Canada. Eligible patients were randomized to FMT by capsule or colonoscopy at a 1:1 ratio by computer-generated random numbers in blocks of 4, stratified by age (≥65 vs <65 years) and immunosuppression. This study was not blinded owing to the practical barriers to masking. A data and safety monitoring board monitored the trial to completion. Following 10 or more days of vancomycin at 125 mg by mouth 4 times a day until symptom resolution, patients were treated with vancomycin, 125 mg, by mouth twice a day until 24 hours prior to FMT. No proton pump inhibitor (PPI) was given prior to FMT; patients taking a PPI discontinued it after screening. All patients received 4 L of GoLYTELY (polyethylene glycol) the night before FMT and remained fasting until the scheduled treatment. Patients randomized to the colonoscopy group received 360 mL of fecal slurry in the cecum. Those randomized to the capsule group swallowed 40 capsules under direct observation. All patients had clinic visits at weeks 1, 4, and 12 after FMT, with a telephone follow-up at week 2. In the event of diarrhea recurrence, C difficile testing was repeated. If test results were positive, patients were treated with vancomycin prior to the second FMT by the same delivery modality, with identical follow-up. Serial stool samples were collected and frozen at −80°C prior to FMT and at 1, 4, and 12 weeks after FMT. A subset of 23 capsule and 23 colonoscopy recipients was randomly chosen to proportionally represent the study cohort for microbiome profiles. Stool microbial DNA was extracted using the FastDNA Spin kit for Feces (MP Biomedicals) for whole-genome shotgun sequencing. Metagenome libraries were constructed using the Nextera XT (Illumina) protocol. Libraries were sequenced in an Illumina MiSeq using a paired-end 300-cycle protocol. Taxonomic classification of sequences was conducted with Kraken against a customized database that included full-genome sequences of bacteria, archaea, viruses, fungi, protozoa, and the human genome assembly GRCh38.15 Re-estimation of bacterial abundance was carried out with Bracken (eMethods 1 in Supplement 2). Participants filled in the 36-Item Short Form Survey (SF-36) questionnaire at the screening visit and 4 weeks after FMT. Responses were used to score the quality of life in 8 health domains on a scale of 0 (worst possible) to 100 (best possible), including physical functioning, bodily pain, role limitations due to physical health problems, role limitations due to personal or emotional problems, emotional well-being, social functioning, energy/fatigue, and general health perceptions. Participants also filled in at screening and 1 week after FMT a patient satisfaction and perception questionnaire (eMethods 2-4 in Supplement 2) the investigators developed. The questionnaire involved rating participants’ perceptions of FMT before the procedure and their experience with the procedure on a scale of 1 (not at all unpleasant) to 10 (extremely unpleasant). Seven healthy volunteer stool donors provided stool for all participants. Donor inclusion/exclusion criteria, screening, and testing followed recommendations proposed by Kelly and colleagues.16 Each fresh stool donation, weighing 80 g to 100 g, was received by laboratories within 12 hours of collection. Each collection was processed separately without pooling, by mixing in 200 cc of 0.9% normal saline and filtered using a stomacher bag to produce 180 cc of fecal slurry. The slurry was mixed with 20 cc of 100% glycerol and kept frozen at −70°C for up to 2 months. When required, the frozen slurry was thawed at 4°C overnight and reconstituted with 160 cc of 0.9% normal saline. For capsule manufacturing, the fecal slurry (approximately 200 cc) was mixed with 40 cc of 100% glycerol and centrifuged (Sorvall Legend RT+; Thermo Scientific) at room temperature at 400g for 20 minutes. After decanting the supernatant, it was centrifuged at 10 000g for 30 minutes at 4°C to 8°C using a high-speed centrifuge (Avanti J-30 I; Beckman Coulter). The supernatant was discarded and the final sediment (approximately 12 cc, estimated to contain 1013 microbes) was mixed to incorporate residual liquid to allow pipetting into capsules. Using either a microtiter template or by individual handheld half capsules, No. 1 gelatin capsules (1889-02; Medisca) were filled, then over encapsulated twice with No. 0 (2009-02; Medisca) and No. 00 (1109-02; Medisca) capsules, flash frozen at −55°C on dry ice and stored at −70°C for up to 2 months. Forty capsules were manufactured from 1 donation. To determine noninferiority of FMT administered by capsule compared with by colonoscopy, a sample size of 49 in each group was required to achieve 80% power to detect a noninferiority margin of −15% in success rates between the 2 groups at 5% significance level, assuming a 90% success rate for the colonoscopy group, as reported in the literature,10 and no difference in success rates between the capsule and colonoscopy groups. Data comparing upper gastrointestinal vs colonoscopy administration of FMT were limited. For example, one study reported a success rate of 74% (28/38) following 1 FMT administration by duodenal instillation17 compared with a 92% success rate with 1 FMT delivered by colonoscopy by Kelly and colleagues.10 The proposed −15% noninferiority margin was chosen based on investigators’ judgment that FMT administration by capsule was noninvasive and relatively inexpensive compared with colonoscopy, an invasive and costly procedure. Assuming a 15% attrition rate, a sample size of 58 patients in each group was needed. The per-protocol population consisted of all patients who strictly adhered to the study protocol and had completed the planned 12-week follow-up. The primary outcome was analyzed using the per-protocol approach based on the 2-sample binomial noninferiority test at the 5% significance level. The noninferiority was established when the 1-sided 95% CI for the difference in 12-week success rate was −15% or greater. Sensitivity analysis was performed to examine the noninferiority of the capsules considering the worst-case scenario, where all patients excluded from the primary analysis were assumed to have CDI recurrence if they were in the capsule group and no recurrence if they were in the colonoscopy group. For secondary outcome comparisons, between-group differences and 95% CIs were reported, along with 2-sided P values from the Wilcoxon rank sum test for changes in SF-36 scores between screening and week 4 and Fisher exact test for patient satisfaction using a 2-sided significance level of 5%. Because secondary end points were not adjusted for multiple comparisons, these findings should be considered exploratory. Statistical analyses were performed using SAS version 9.4 (SAS Institute). Of 213 patients assessed for eligibility, 97 patients were excluded. The remaining 116 underwent randomization (mean [SD] age, 58 [19] years; 79 women [68%]; 105 [91%] completed the trial): 57 in the capsule group and 59 in the colonoscopy group (Figure 1). Baseline characteristics were not significantly different between the groups (Table 1). Seventeen patients, 10 randomized to the colonoscopy and 7 to the capsule group, were immunosuppressed (eTable 1 in Supplement 2). In the end, 53 in the capsule group and 52 in the colonoscopy group were included in the per-protocol analysis for primary outcome; numbers of patients included for secondary analyses varied by outcomes (Figure 1). The primary outcome was not assessed for 11 patients: 4 did not receive the assigned treatment, 2 died, 3 withdrew from the study owing to IBD flare, and 2 were lost to follow-up before 12 weeks (eTable 2 in Supplement 2). Among the remaining patients, absence of RCDI was achieved in 96.2% of patients, both in the capsule group (51/53) and the colonoscopy group (50/52) after a single treatment as per-protocol analysis, with a rate difference of 0% (1-sided 95% CI, −6.1% to infinity; P < .001), resulting in rejection of the null hypothesis that FMT by capsule was less effective than FMT by colonoscopy by at least −15%. Sensitivity analysis assuming the worst-case scenario had a similar finding. With a success rate of 89.5% (51/57) for the capsule group and 96.6% (57/59) for the colonoscopy group, the rate difference was −7.1% with a 1-sided 95% CI of −14.9% to infinity (P = .048), demonstrating the noninferiority of the capsule group. Site differences in efficacy are presented in eTable 3 in Supplement 2. Two patients in each group developed RCDI and were successfully treated with a second FMT by the same modality. Quality of life before FMT and 4 weeks after FMT were not assessed for 13 patients: 4 did not receive the assigned treatment, 2 withdrew before 4 weeks, 1 died, and 6 did not complete the SF-36 questionnaire at 4 weeks. Among the remaining 103 patients, the domains of role limiting due to physical and emotional health problems scored lowest among all domains, both with a median of 0 before FMT. Four weeks after FMT, the domains of role physical and role emotional had improved significantly, with an increase of 25 and 33 in the median scores, respectively. Other domains also had significantly higher scores at 4 weeks compared with baseline. However, these changes were not significantly different between the 2 groups (Table 2). Post-FMT perception was not assessed in 6 patients, all of whom either did not receive the assigned treatment or withdrew early. Participants most frequently characterized FMT as “innovative treatment” (63% of patients), “natural remedy” (41%), and “unpleasant, gross, or disgusting” (30%). Factors reported most frequently to influence preference for FMT delivery method were effectiveness (91%), safety (62%), physician’s recommendation (45%), aesthetics (29%), and cost to health care system (18%). A total of 79% of participants reported the unpleasantness experienced to be the same or less than anticipated and 97% indicated they would undergo the assigned delivery method again if needed. A significantly greater proportion of participants receiving capsules rated their experience as “not at all unpleasant” (66% vs 44%; difference, 22% [95% CI, 3%-40%]; P = .01). Adverse events were assessed for all but 4 patients (n = 112) who did not receive the assigned treatment. One patient in each group died of underlying cardiopulmonary illness, unrelated to FMT, within 12 weeks of FMT. A woman in her mid-70s with underlying lupus taking low-dose prednisone, complete heart block requiring pacemaker, and congestive heart failure received FMT by colonoscopy after the fourth episode of CDI. She developed confusion between the time of screening and FMT, a new development not communicated to the study team. Although the colonoscopy was uneventful, the patient died of congestive heart failure 3 days later. A woman in her early 80s who had chronic obstructive pulmonary disease, peripheral vascular disease, and atrial fibrillation received FMT by capsule after the fifth episode of CDI. She did well after FMT but developed pneumonia and Staphylococcus epidermidis bacteremia 10 weeks after FMT and died of sepsis 1 week later. No infectious complication related to FMT was seen in any of the study participants. No colonic perforation occurred. Minor adverse events occurred in 5.4% of the capsule group and 12.5% in the colonoscopy group (eTable 4 in Supplement 2). Prior to FMT, patients with RCDI had decreased diversity as measured by the Shannon diversity index compared with donors (Figure 2). Following FMT, diversity increased significantly in both the capsule and colonoscopy groups to values not significantly different from donors. This increased diversity was maintained in both groups up to 12 weeks following the FMT. PCoA of the patients with RCDI showed a significantly different microbial community structure before and after FMT (permutational multivariate analysis of variance P = .001) (Figure 3). Patients with RCDI clustered apart from donors prior to FMT and moved toward the donor profiles and persisted to week 12 after FMT. Taxonomic composition of the patient and donor microbiota is shown in the eFigure in Supplement 2. Currently, most patients with RCDI are referred to gastroenterology or infectious diseases, and the method and route in which FMT is administered are specialty dependent. Although colonoscopy delivery is more invasive, resource intensive, costly, and inconvenient for patients, it has the advantage of identifying alternative diagnoses. Conversely, when FMT is given by oral capsules, it can be administered in an office setting, which could substantially reduce cost and wait time. Complete economic evaluations are needed to understand the value and efficiency of FMT by oral capsule. The theoretical need to prevent gastric acid from destroying the transferred microbes during FMT has led to the practice of using PPIs and/or acid-resistant capsules.8,25,26 However, in this study, PPIs or histamine antagonists were not used prior to FMT. If a patient was taking a PPI at the screening visit, PPI was discontinued because it is a known risk factor for CDI. Furthermore, the gelatin capsules used were not acid resistant. To counteract the lack of acid suppression, the microbial inoculum used in this study may have been sufficient to overcome the potential microbial loss in the stomach. Observing similar microbial composition in colonoscopy- and capsule-administered FMT supports the position that microbes alone can restore integrity to the intestinal ecosystem. This study has several strengths. First, only patients with a propensity for RCDI within a short period of treatment completion were included in this study (median of 4 CDI episodes over a 4-month period and median anti-CDI treatment duration of 2.4 months). Second, patient-reported outcomes were captured by SF-36 and satisfaction/preference questionnaires before and after FMT. Third, IBD and immunosuppressed patients were included, although the numbers were too small to determine the safety profile. Fourth, using a small number of stool donors minimized the risk of potential disease transmission, screening costs, and wait time. This study has several limitations. First, by not including a placebo group in this study, it was not possible to measure the magnitude of effect of FMT administered by each route. In a recent FMT vs placebo trial in the treatment of RCDI, the placebo response rate was 45%.27 In this study, patients knew that they received FMT regardless of randomization and this knowledge may have inflated the response rate. At the same time, the placebo effect would have existed in both groups. Assuming similar placebo effect, the result of this study still provides a valid comparison of the efficacy between the 2 treatment modalities. Second, there was no blinding. Although technically possible, blinding would have required patients randomized to the capsule group to undergo an invasive and expensive procedure. Furthermore, blinding would not have allowed for assessing a potential difference in patient preference or adverse events based on method of FMT delivery. Third, the generalizability of these findings was limited by the enrollment criteria as patients with severe and complicated CDI were excluded. Fourth, the cost comparison did not include donor screening, cost from a societal perspective, or FMT program infrastructure or liability. Additionally, the cost of colonoscopy is lower in Canada than in the United States. Therefore, the cost difference between FMT by capsule compared with colonoscopy may be larger in the United States. Fifth, strain typing was not performed in this study cohort, although the NAP1/ribotype 027 strain is estimated to be 20% in Calgary and 30% in Edmonton based on recent provincial surveillance data. Probabilities (95% CIs) models used restricted cubic splines adjusting for age, sex, year, income quintile, rurality, transfer from any health care institution, Deyo-Charlson score, history of frailty, diabetes, heart failure, chronic obstructive pulmonary disease, myocardial infarction, or hypertension, fracture and surgery type, Injury Severity Score, surgeon volume and experience, hospital volume and type, and surgery duration. Analysis conducted among 41 186 of 42 230 patients. C statistic was 0.756. Variance inflation factors were 4 or less for included variable included, indicating an absence of collinearity. Probabilities of the primary outcome according to wait times for surgery are presented for patients with average fracture, physician, and hospital system characteristics in the cohort. Probabilities (95% CIs) models used restricted cubic splines. Variables in the adusted models are listed in the “Outcomes” section of the Methods and in Figure 1. Analysis conducted among 41 186 of 42 230 patients. Probabilities of each outcome according to wait times for surgery are presented for the patient with average fracture, physician, and hospital system characteristics in the cohort. A population-based cohort study was conducted using health administrative databases in Ontario, Canada. Ontario residents have their medically necessary health care services, physician and hospital information, and demographic characteristics recorded in these databases. These data are held and linked at the Institute for Clinical Evaluative Sciences (ICES; http://www.ices.on.ca) and have been used previously to study patients with hip fractures (eAppendix A in the Supplement).2 The study protocol was approved by the Research Ethics Board at Sunnybrook Health Sciences Centre, Toronto, Ontario. Individual patient informed consent is not required for use of encoded administrative health data in Ontario. Index surgeon- and hospital-related factors were assigned at time of each patient’s operation. These included (1) years since each surgeon’s Canadian orthopedic certification (surgeon experience) and (2) the number of hip fracture procedures performed in the year preceding the index event (surgeon and hospital volume). Each hospital’s capacity for performing nonelective surgery was operationalized as the mean daily number of any nonelective (or urgent) procedures performed at the hospital, orthopedic or otherwise, in the year preceding the index event. Hospitals were also categorized as either academic or community based on their membership in the Council of Academic Hospitals of Ontario (http://www.cahohospitals.com). Community hospitals were further classified as large (≥400 beds) or small to medium (<400 beds).23. Risk-adjusted, restricted cubic splines with 4 knots26 were used to model the probability of complications according to the time elapsed from emergency department arrival to surgery. Any nonlinear relationship between surgical delay and each outcome could be assessed using spline regression, which makes no underlying assumptions about a functional form. Rather than arbitrarily dividing patients into early and delayed surgery groups,15 the association between surgical delay and mortality was graphically represented to visualize an inflection point (in hours), if one existed, when complications began to rise. At time thresholds around the area of inflection, the area under the receiver operating characteristic curve was calculated for adjusted logistic regression models relating surgical delay to 30-day mortality. The time (in hours) producing the maximum area under the curve increase was selected as the threshold to dichotomize time and classify patients as receiving either early or delayed surgery.27 To further evaluate the robustness of this definition, early and delayed patients were matched 1:1 without replacement28 on the logit of propensity scores.29. Two post hoc sensitivity analyses were also conducted. Prolonged wait times may indicate medical, rather than administrative, reasons for delay and even greater confounding. Therefore, an analysis restricted to patients’ receiving surgery within 36 hours was conducted to assess the association between wait times and complications in a subgroup with less potential for unmeasured confounding. As an indicator of residual confounding in the delayed group, the relationship between matching status (matched or unmatched) and mortality in delayed group patients was also explored. There were a total of 42 230 patients who met inclusion (Table 1) and were treated by 522 orthopedic surgeons at 72 Ontario hospitals. Their mean (SD) age was 80.1 (10.7) years, and 70.5% were women (Table 2). The mean (SD) time to surgery was 38.8 (28.8) hours. Adjusted splines modeled an area of inflection around 24 hours when the risk of developing complications began to increase, irrespective of the outcome or follow-up period assessed (Figure 1 and Figure 2). Surgical complications (dislocation, hardware removal) were not related to wait time (Figure 2). The maximum area under the curve increase occurred when using 24 hours to classify patients into early or delayed surgery groups compared with 6-, 12-, 18-, 30- and 36-hour thresholds (eAppendix D in the Supplement). Delayed hip fracture surgery was therefore defined as surgery occurring more than 24 hours after emergency department arrival. The relationships between wait times and complications were robust to stratification among different patient subgroups, including among patients without comorbidity and those receiving surgery within 36 hours, for whom confounding by indication should not play a role (eAppendices C and G in the Supplement). There was no significant difference in surgical complications (negative tracer outcomes) between groups (Table 3). Sensitivity analysis restricted to patients older than 65 years and adjusting for antiplatelet or anticoagulant medications also produced equivalent results (eAppendix E in the Supplement). Although unmatched patients in the delayed group were more medically complex than matched patients, adjusted regression found no significant association between matching status and 30-day mortality indicating no residual confounding in the matched delayed group (eAppendix F in the Supplement). This is the first study, to our knowledge, to analyze time as a continuous variable in hours and empirically identify a time-to-surgery threshold associated with increased complications after hip fracture. The primary finding pertains to increasing numbers of patients experiencing hip fractures and the different medical and surgical specialists treating them: a wait time of 24 hours may represent a threshold defining higher risk because complications increased when surgery was delayed after this time, irrespective of the complication, follow-up period, or patient subgroup assessed. Preoperative optimization is often required for patients with a hip fracture and can feasibly be performed within the proposed timeframe. Because wait times for hip fracture surgery are already used worldwide as quality indicators to assess hospital performance, the results of this study may inform existing hip fracture care guidelines and policies.6-8. Targeting surgery within 24 hours represents a significant change in practice because 66% of the patients in this study did not receive surgery within this timeframe. Hip fracture prioritization must also be balanced with the needs of patients on waiting lists for other surgical procedures. Because time to surgery rather than timing of surgery is associated with increased risk,2 continuing to conduct hip fracture operations during evenings and weekends may help reduce wait times for hip fracture surgery without conflicting with elective operations. However, other performance improvement efforts will be required, as well as future work that identifies where these efforts would be most successfully targeted. Although effect modification was not observed in subgroup analyses, given the practical challenges of further reducing existing thresholds, future work may continue trying to identify a subset most in need of urgent surgery. This study has several strengths. First, a population-based sample from Ontario’s diverse population of approximately 13.5 million was used. Findings are generalizable to other jurisdictions, including the United States11 and Europe,14 where operating room resources are limited and patients are required to wait for hip fracture surgery. Second and third, the study was conducted in a public health care system where patients can be followed up for complications even if they present elsewhere in the system and surgical delays are mostly related to administrative rather than patient factors.33. This study also has several limitations. First, since medically complex patients are predisposed to both complications and awaiting optimization prior to surgery, several analyses were performed to mitigate the influence of confounding. Comparisons between early and delayed surgery groups after matching were balanced across more than 30 covariates, which included detailed measures of preexisting medical comorbidity. The absence of effect modification in several important clinical scenarios was also demonstrated in subgroup analyses, including those restricted to patients without comorbidity and those receiving surgery within 36 hours for whom surgical delays should be administrative. However, the finding that increased wait time is associated with increased risk may still be influenced by unmeasured factors. This study adds to the growing body of evidence supporting the importance of early surgical stabilization of hip fractures. The question is how much evidence is necessary to drive change in the system. The timing of hip fracture surgery has been studied extensively but not conclusively. The bulk of evidence has been from observational cohort studies,2-4 which have been limited by confounding in that patients with hip fracture had greater comorbidities or severity of illness (ie, the ones most likely to die or experience complications) are also the most likely to have surgery delayed for medical optimization. Pincus et al provide analyses that convincingly address this issue, such as adjusting for numerous variables in their restricted cubic spline, and propensity score models, as well as demonstrating the robustness of their findings with several sensitivity and post hoc analyses. However, the authors also acknowledge potential limitations of unmeasured factors, potential misclassification, and failure to include patients with hip fractures who did not undergo surgery. Other cohort studies with far fewer patients with hip fracture (n = 2660) but with more rigorously collected prospective data, such as by Moran et al,5 have failed to demonstrate an association between surgical delays and increased mortality except in patients delayed more than 4 days. Given the relatively small absolute increase in 30-day mortality noted by Pincus et al, the study by Moran et al may have been underpowered to detect the difference. Orosz et al6 found that early surgery (ie, <24 hours) was not associated with reduced mortality in a cohort of 1178 patients with hip fracture, but the investigators also reported that patients who underwent early surgery had fewer days of severe pain  and fewer major complications. Even with the absence of definitive proof of harm, delaying surgery for no good reason is suboptimal care. Several factors affect the timing of hip fracture surgery. Many patients who sustain hip fracture are older and have multiple comorbidities that require medical evaluation to ensure that their conditions are stable. A smaller subgroup of patients with hip fracture have decompensation of their health status and require medical treatment prior to surgery. This process may involve evaluations by multiple consultants, performance of diagnostic testing, and ongoing medical management. The logistics of this process and the lack of consensus on what is required prior to surgery frequently add days to the patient’s preoperative course. Even when the patient is stabilized and the health conditions are optimized, operating rooms and surgeons are not consistently available, leading to further delays. In an evaluation of the reasons for delay in 571 patients with hip fracture, Orosz et al7 found that 123 had surgery delayed beyond 48 hours, of which 43 (35%) required medical stabilization; however, 78 (63%) were still awaiting completion of multiple medical evaluation and 55 (44%) were delayed because of operating room or surgeon availability. Although risk stratification of patients undergoing elective surgery has been extensively studied, few guidelines exist to help physicians with the specific preoperative preparation for patients with hip fractures. Even when guidelines do exist, they are often not followed. For example, the 2014 ACC/AHA Guideline on Perioperative Cardiovascular Evaluation and Management of Patients Undergoing Noncardiac Surgery8 indicates that the routine evaluation of left ventricular function is not recommended except for some patients with new or worsening heart failure and that stress testing is only recommended if it will lead to some intervention that will change the management of the patient’s disease. Despite this guideline, echocardiography, pharmacological stress testing, or both are often part of the routine preoperative evaluation. For example, Ricci et al9 examined a cohort of 235 patients undergoing operations for hip fracture and found that 35 patients (15%) had cardiac testing prior to surgery. No patient had cardiac surgery or coronary angioplasty resulting from the testing. For 17 patients (48%) cardiac testing did not change the medical management. For 18 patients (52%), recommendations were made only for medical management of a previously known condition. To address the complex medical needs and better coordinate the care of older patients with hip fractures, several centers have developed comanagement teams. Unlike the common situation in which patients with hip fracture are admitted by the orthopadic surgeon and whose care is managed by the primary care physician and multiple consultants, these comanagement models involve a consistent team that includes orthopedic surgeons and geriatric or medical specialists. This approach has led to decreased complication rates, delays, and lengths of stay. Friedman et al10 compared hip fracture outcomes at 2 hospitals staffed by the same orthopedic and anesthesia departments. At one hospital, 193 patients older than 60 years with hip fractures were admitted to an orthopedic-geriatric comanagement service whereas 121 patients at the other hospital continued to receive usual care. Patients admitted for comanagement were older, had more comorbidities and dementia, and were less likely to dwell in the community. Despite this, the patients in the comanaged group were operated on sooner (24.1 vs 37.4 hours), had fewer infections (2.3% vs 19.8%), fewer overall complications (30.6% vs 46.3%), and shorter lengths of stay. Given the increased up-front cost involved with establishing a comanagement model, Swart et al11 performed a breakeven analysis and found that a dedicated comanagement approach would be cost-effective for hospitals that treated more than 54 patients with fractures a year. Despite these encouraging results, traditional fragmented care persists at many hospitals. Economic incentives maintain the status quo. In-hospital consultations are a source of revenue for physicians, and hospitals prefer to keep their operating rooms filled with elective cases. Hospitals would benefit from decreased length of stay, but few are willing to jeopardize alienating surgeons by forcing them to operate off hours and weekends. This is changing with the move to value-based care and bundled payments but risk and gainsharing arrangements must be created. The planned extension of mandatory bundles by the Centers for Medicare & Medicaid Services to include hip fractures has been put on indefinite hold.12 Nevertheless, the study by Pincus et al and other reports emphasize the importance of acting now to improve care. How can these pharmaceutical agents be widely available to the public without FDA approval, prescription, or manufacturing oversight? The increasing use of these agents illustrates the complex interplay of rapid developments in information technology, consumerism, medicine, and public policy. The origins may be traced back to the 1994 Dietary Supplement Health and Education Act, which exempted products considered “dietary supplements” from rigorous studies to demonstrate safety and efficacy as required for FDA approval of drugs prior to marketing. The result was a flood of unregulated dietary supplements with clever advertising schemes encouraging the public to pursue an easy solution to deeper medical problems. Androgens and androgen precursors as chemical entities were aggressively marketed as supplements, even though these compounds are drugs and not food. Androstenedione was specifically excluded as a dietary supplement when the consequences of its widespread use became known; however, many of these so-called dietary supplements still contain hormones, drugs, and known toxins often not listed on the label. Whose responsibility is it to police the illicit sale of androgens and selective androgen receptor modulators or similar violations of federal laws and public safety? The medical profession has no authority in this area. Consumers are unlikely to become involved because nonmedical androgen users, like other individuals with substance use disorders, often conceal and deny their use. The FDA is responsible for taking action against any adulterated or misbranded dietary supplement product after it reaches the market. However, unlike drugs, the vendor (not the FDA) is responsible for evaluating the safety and labeling of its products and there is little incentive to do so. Flagrant violations of these statutes abound, and the FDA does not have the resources to address all of these cases in enough detail to take corrective legal action. The US Drug Enforcement Administration (DEA) shares responsibility because androgens are schedule III drugs. However, the DEA is overwhelmed with the opioid epidemic, and industry-sponsored legislation last year seriously impaired efforts of the DEA to thwart complicit narcotic distributors. It is unrealistic to think that the DEA will turn its attention to androgens when opioid addiction remains a major concern. Without major changes in the laws to regulate putative dietary supplements and internet sales (along with substantial resources for enforcement), the unrestricted abuse of androgens and related drugs among the general public will continue. GRADE indicates Grading of Recommendations Assessment, Development, and Evaluation. GRADE levels of evidence: A, high; B, moderate; C, low; D, very low. In principle, all patients should be offered general measures, medical treatment, and surgery in parallel and not sequentially. Mild but widespread disease generally does not provide a target for lesion-directed treatment; therefore, it is less suitable for surgery as the only category. Disease severity is traditionally classified according to the Hurley classification,11 which defines stage I as transient nonscarring inflammatory lesions; stage II as separate lesions consisting of recurrent abscesses with tunnel formation and scarring, and single or multiple lesions separated by normal-looking skin; and stage III as coalescent lesions with tunnel formation, scarring, and inflammation (Figure 1). Hurley classification is, however, not suited for dynamic assessment of HS and a large number of scores have been devised but not validated. The modified Sartorius Score (mSS) in which involved anatomical predetermined regions are counted and, in addition, inflammatory and noninflammatory lesions are counted, classified, and weighted according to type. Additional points are given for the longest distance between 2 lesions within each affected anatomical region and for any regions containing Hurley stage III. The points are added for an overall severity score.12 The HS-Physician’s Global Assessment (HS-PGA; an objective total count of HS lesions) is also used. It is an anchored 6-point PGA based on lesion counts in predilection areas.13 Currently, an international effort is under way to identify and validate core outcomes for HS.14. A literature search was conducted using PubMed, MEDLINE (Medical Subject Headings [MeSH]), and EMBASE from September 1, 2011, to May 1, 2017. Reviews, guidelines, conference abstracts, and articles reporting the results from less than 10 patients were excluded. Grading of Recommendations Assessment, Development, and Evaluation (GRADE; levels of evidence: A, high level; B, moderate; C, low; D, very low) was used to assess the overall quality of the evidence. An internet search for guidelines was conducted and further details about the PRISMA flow diagrams and literature search are presented in Supplement 1 and in the eAppendix in Supplement 2, respectively. Injection of triamcinolone acetonide into HS lesions has been used, but evidence for this approach is limited. In a prospective case series of 36 patients, triamcinolone (10 mg/mL) was injected into HS lesions. Pain was significantly reduced on the day following the injection (from a score of 5.5 to 2.3 on a 0-10 point numerical rating scale [0 indicating no pain and 10 indicating the worst pain imaginable], P < .005). After 1 week of injection therapy, there was a reduction in erythema (median score from 2-1 on a 5-point anchored rating scale [0-4: 0 indicating normal-appearing skin in all aspects and 4 indicating dark red erythema], P < .001), edema (median score from 2-1, P < .001), suppuration (median score from 2-1, P < .001), and lesion size (median score from 3-1, P < .001).22 The long-term efficacy of this approach remains to be established. Phase 3 adalimumab studies include the PIONEER I (n = 307) and PIONEER II (n = 326) studies. In both studies, patients were randomly assigned in a 1:1 ratio to receive 12 weeks of adalimumab or placebo in period 1. In the second study period (weeks 13-24), patients who received placebo in PIONEER I were reassigned to receive adalimumab weekly or they continued to receive placebo in PIONEER II in a blinded fashion. In both PIONEER I and II, patients who received adalimumab in the first treatment period were reassigned to receive adalimumab weekly, every other week, or placebo. The outcome for these studies was the HiSCR-50 with no increase in abscess or draining fistula count relative to baseline.42. Adverse events were comparable with other indications for adalimumab, and rates of serious adverse events were not statistically significant. During period 1 in PIONEER I, the rate of any adverse event was 50.3% in the adalimumab group and 58.6% in the placebo group. Serious adverse events rate was 1.3% in the adalimumab group and 1.3% in the placebo group. During period 1 in PIONEER II, the rate of any adverse event was 57.1% in the adalimumab group and 63.2% in the placebo group. The rate of serious adverse events was 1.8% in the adalimumab group and 3.7% in the placebo group. A prospective, uncontrolled, open-label study in HS among 17 patients receiving weight-based ustekinumab treatment (≤100 kg: 45 mg; >100 kg: 90 mg) at baseline and weeks 4, 16, and 28 with follow-up at week 40 has been conducted. Only 12 of 17 patients (70%) completed the study, 8 of 17 patients (47.0% [95% CI, 23.0%-72.2%]) achieved HiSCR-50, and 14 of 17 patients (82.0% [95% CI, 56.6%-96.2%]) had a moderate to marked improvement of the mSS at week 40. The mean mSS was significantly reduced from 112.12 at baseline to 60.18 at week 40 (46.33% improvement; P < .01 [95% CIs were not available]). The most common mild and temporary adverse events were headache, fatigue, and upper respiratory tract infections. One patient dropped out due to urticaria.30. Local excision or tissue-saving methods like deroofing and skin tissue–saving excision with electrosurgical peeling (STEEP) have recently been assessed. In deroofing, a probe is used to explore tunnels (sinus tracts) and only the “roof” is excised, leaving the epithelialized floor of the sinus tract intact. In STEEP, diseased tissues are removed by stepwise tangential excisions, preserving unaffected tissue. In both methods, the wounds are left open to heal by secondary intention.46,67 Postsurgical morbidity and the risk of scar contractures are reduced but recurrence rates are higher than for wide excision procedures (Table 3). Wide excision is defined as an excision of a lesion including a lateral margin of disease-free tissue, sometimes encompassing the entire anatomical region (eg, all axillary skin). It is associated with lower recurrence rates but greater postoperative morbidity (such as infection, bleeding, and contractures) (Table 3). Large wounds resulting from wide excision procedures are generally closed using split-thickness skin grafts or flaps, but are sometimes left to heal by secondary intention.51,53,54,56,57,59,62 This results in a prolonged recovery and scar formation. Extensive surgery is necessary when the HS is complicated by the presence of cancer (eg, Marjolin ulcers) or amyloidosis secondary to the chronic inflammation (when amyloid [an acute phase protein] is deposited in, for example, the kidneys causing life-threatening nephrotic syndrome).69-71. The mean patient score was 2.8 after infliximab monotherapy and 3.3 after adding surgery (P < .001; 95% CIs were not available). Six patients (20%) were treated with infliximab only (surgery was not necessary in 4 patients and 2 patients declined surgery). At the end of the follow-up period (mean, 50 months [maximum, 127 months]), 10 of 30 patients (33.0% [95% CI, 17.3%-52.8%]) were free of lesions, 13 of 30 patients (43.0% [95% CI, 25.5%-62.6%]) were improved, 4 of 30 patients (13.0% [95% CI, 3.8%-30.7%]) moderately improved, and 3 of 30 patients (10.0% [95% CI, 2.1%-26.5%]) had severe HS. Adverse events (not specified in the article) due to infliximab treatment were seen in 12 of 30 patients (40.0% [95% CI, 22.7%-59.4%]), resulting in treatment discontinuation in 9 of 30 patients (30.0% [95% CI, 14.7%-49.4%]). No surgical complications were observed.64. Quiz Ref IDHS is a multifocal disease requiring systemic therapy, the choice of therapy is guided by disease severity. Based on the available evidence, 2 medical therapies can be recommended for mild disease: topical clindamycin (GRADE B; supported by a small RCT) and resorcinol (GRADE C). There is no available data to guide the choice between the 2, but the irritant effect of resorcinol makes it more suitable for use in smaller areas (eg, when only a few lesions are present). For mild or moderate disease unresponsive to topical treatment, tetracycline (500 mg twice daily; GRADE B) or doxycycline/minocycline (50-100 mg twice daily; GRADE D) can be administered. For moderate or severe disease, rifampicin (300 mg twice daily) and clindamycin (300 mg twice daily; GRADE B) may induce temporary remission. Initial treatments are usually begun with tetracycline-type drugs because they are less susceptible to developing resistant organisms and have more limited use as antibiotics.37 Patients with moderate to severe disease can also be treated with TNF antibody therapy, especially if they have required long-term antibiotic treatments for disease control, flared rapidly when antibiotic treatments were stopped, or have moderate to severe disease without secondary bacterial infections. Adalimumab was approved for the treatment of HS by the US Food and Drug Administration in 2016 (GRADE A). Infliximab may be used as an alternative anti-TNF therapy (GRADE B). In case of treatment failure, third-line treatment targeting IL-1 (anakinra; GRADE B), p40 (ustekinumab, GRADE C), dapsone (GRADE C), or acitretin (GRADE C) may be tried. Mild cases of HS can often be managed by intralesional administration of 3 mg to 5 mg of triamcinolone (GRADE C), but once chronic lesions form, surgery is necessary. For mild to moderate HS, carbon dioxide laser evaporation of lesions or deroofing or STEEP may be performed. Deroofing or STEEP may be tried when tunnels or cysts are present. Wide excision has a better cure rate than these operations, but may also be associated with a higher risk of complications, so it is reasonable to consider a stepwise approach escalating from least-invasive to more-invasive surgical options. A postmenopausal woman with abnormal breast imaging had 2 palpable masses within her right breast, 1 of which had ductal carcinoma in situ (DCIS) on biopsy. The other mass appeared benign on further imaging and did not require treatment. Partial mastectomy for the DCIS was performed. No intraoperative imaging was used because the mass was palpable and the examination note documented which of the 2 masses contained DCIS. The excised breast mass was labeled “right breast mass, 9:00 position” and sent to the pathology department and the patient was discharged home from the recovery area. The final pathology report noted “benign breast tissue, no evidence of biopsy site changes or biopsy clip.”. This is an example of wrong-site surgery, considered a sentinel event by The Joint Commission.1 Sentinel events require a thorough and credible root cause analysis followed by a corrective action. Wrong-site surgery is in the general category of adverse events that also include wrong-procedure and wrong-patient surgery events (WSPEs). The reported rate of these events is low. Any individual hospital can expect to have less than 1 nonspinal WSPE per 5- to 10-year period, but spinal and ophthalmic procedures may have higher rates.2,3 Although this estimate is quite low (perhaps due to underreporting), the harm to a patient can be substantial and the effect on the physician profound. It is imperative that clinicians be prepared to address this problem should they encounter it. Quiz Ref IDFormal review of wrong-site surgery is required. The Joint Commission requires that health care organizations perform a root cause analysis for sentinel events. A root cause analysis is a structured, systematic multidisciplinary approach for understanding the processes involved in the event. In the current case, representatives from several hospital departments participated. These included surgical oncology, radiology, pathology, perioperative services, hospital quality improvement, and patient safety (Audio). Quiz Ref IDThe Joint Commission’s Universal Protocol is designed to decrease WSPEs by outlining safe site-marking and verification practices. Hospitals differ in their protocol for site marking, but the general requirement is that any procedure involving laterality or level must be marked by a clinician involved in performing the procedure. In this case, the correct laterality was marked but the specific lesion was not. This case highlights a limitation of the Universal Protocol guideline in preventing WSPEs. Previous studies suggest insufficient progress in reducing the rate of wrong-site surgery events since the adoption of the Universal Protocol.3 More detailed site marking may be warranted for other procedures for which specifying laterality may not be sufficient to prevent errors. Examples include skin lesion excision, in which many lesions are present, or surgery for a specific digit. Confirming the site with the patient is also required and can be a helpful adjunct to safety. The second area identified for improvement was the use of perioperative radiographs. There was no radiology protocol for breast surgery at the institution where this event occurred at the time of the event. Some hospitals require display of preoperative images in the operating room. Surgeons could be required to review the images before operating as a routine practice. But such a requirement is subject to variation or omission and does not ensure that errors will be avoided. Quiz Ref IDA better alternative is using intraoperative imaging of the surgical specimen to ensure that the biopsy clip and/or suspected lesion is contained within the resected specimen. This may reduce the risk of not including the suspected lesion in the resected specimen. Including intraoperative radiograph review as part of the sign-in, sign-out, or time-out process ensures that several team members are engaged in this process, minimizing the risk of an error that might occur if any 1 individual is responsible for this task. The guideline was developed and funded by the AASM,4 which commissioned a task force of board-certified sleep medicine specialists with expertise in diagnosis and management of OSA in adults (Table). The task force was required to disclose any potential conflicts of interest according to AASM policy, which were classified as level 1 or level 2. Task force members with a level 1 conflict were not allowed to participate. Those with a level 2 conflict were required to recuse themselves from any related discussion or writing responsibilities. A draft of the guidelines was posted for 2 weeks for public comment on the AASM website before publication. The task force considered all comments and made appropriate revisions. The final recommendations were also approved by the AASM Board of Directors. The task force performed a systematic review of the scientific literature to identify articles that addressed at least 1 of the questions of interest. In addition to articles included in prior AASM clinical practice guidelines,5,6 literature searches were performed for articles published between January 2005 and June 2016. The analysis was limited to randomized trials and observational studies. A total of 98 articles were included in the evidence base for recommendations. Each recommendation statement was assigned a strength (strong or weak). Strong recommendations are those that clinicians should under most circumstances follow. Weak recommendations are associated with a lower degree of certainty and, thus, clinicians should use their clinical knowledge and experience and consider a patient’s values and preferences to determine the best course of action. Dr Zarate:Although we have a number of medications, they work exclusively on the monoamine neurotransmitter system—that’s serotonin, norepinephrine, and dopamine. Unfortunately, these medications do not benefit many. We see very low response and remission rates. It’s estimated that only about one-third remit, [and remission] usually takes approximately 10 to 12 weeks. Dr Zarate:The theory goes back several decades. Basic scientists back then wanted to figure out better ways of treating our patients, so in preclinical studies they examined not only the effects of monoaminergic antidepressants, but what other systems were also implicated. These earlier investigators noted that there were subtle effects [in models of depression] on NMDA [N-methyl-d-aspartate] and AMPA [α-amino-3-hydroxy-5-methyl-4-isoxazole propionic acid] receptors, which are glutamate receptors.The glutamate system is very important in learning, in memory, in plasticity, and other important functions of the brain. It’s believed that glutamate is dysregulated [in] the circuits and synapses that are implicated in depression.When glutamate is released, it activates the NMDA receptor complex. Ketamine blocks this NMDA receptor. It is hypothesized that this blocking of the NMDA receptor on GABA [γ-aminobutyric acid] interneurons leads to an increased release of glutamate from pyramidal cells. That increased release of glutamate—what we refer to now as the glutamate burst—preferentially [activates] AMPA receptors, [which] leads to an increase in BDNF, or brain-derived neurotrophic factor, a protein in the brain that’s been implicated in the response to many different antidepressants. BDNF restores or produces an increase in synaptogenesis and plasticity and other mechanisms that are an important part [in maintaining] homeostatic regulation [of key brain functions]. Dr Zarate:There have now been many published clinical trials, and most of them have found that it is effective in treatment-resistant depression. The response and remission rates within a very short period of time are pretty prominent. It is efficacious even in treatment-resistant depression, meaning patients that have failed multiple antidepressants, and in many cases electroconvulsive therapy, which is one of the most effective treatments. And ketamine is efficacious in treatment-resistant bipolar depression, for which, unfortunately, we have very few treatment options. Further clinical research suggests that ketamine does have—although this is preliminary—very rapid antisuicidal efficacy and seems to have effects on anhedonia. Dr Zarate:Ketamine, unfortunately, has certain limitations. When ketamine is administered intravenously, we see dose-related side effects, such as psychotomimetic and dissociative side effects. For example, one might experience a distortion of time, one might see trails of light or hear muffled sounds. There are also effects that result in transient elevations in blood pressure and heart rate, temporary impairments of cognition. There is the risk, with continued indiscriminate use, of hepatotoxicity and also reports that it might lead to a cystitis. In terms of oral administration, it’s poorly absorbed. So although it will likely have an important clinical use in people with treatment-resistant depression, it would be very important to come up with drugs that are better tolerated [that] one can even give in people who do not have treatment-resistant depression. Newer treatments would also not have the risk of abuse potential that occurs with ketamine. Dr Zarate:The original preclinical evidence linking the NMDA receptor blocking or antagonism has led to a decade of preclinical and clinical studies with NMDA receptor antagonists, with the hopes of achieving the rapid antidepressant effects of ketamine, but without the side effects or risk of abuse. [But] the efforts to develop better or alternate versions of ketamine have been fraught with many difficulties from the start. Several broad and subunit selective receptor antagonists either failed or did not demonstrate the efficacy in treatment-resistant depression, or showed minimal efficacy. This has led to questions about whether preclinical studies [that] demonstrated the promise of NMDA receptor antagonism will actually lead to better treatments. So with that in mind, additional targets have been looked at in terms of its mechanism of action and some postulate that enhanced AMPA receptor throughput might be implicated in rapid antidepressant-acting agents.For instance, there is a drug called rapastinel, an NMDA receptor modulator, [that] seems to have rapid antidepressant effects with a better side effect profile [than ketamine] that is currently being developed in treatment-resistant depression. Preclinical studies suggest that it also activates AMPA receptors.There is another [NMDA receptor modulator] called AV-101. It seems to block the glycine site that is outside of the NMDA receptor channel. AV-101 is currently in the clinic and is being tested for treatment-resistant depression. Animal studies suggest it would have acute, rapid antidepressant effects, and without the side effects of ketamine.Building on this work, our group in collaboration with other laboratories began to explore the cellular and molecular effects of ketamine, and we have found that the effects appear to be largely NMDA-receptor independent. Blocking an NMDA receptor appears to be linked to the side effects of dissociation and to the increases of blood pressure, and potentially to the abuse potential. Dr Zarate:The current form of ketamine is what we call racemic ketamine. That means it’s formed by 2 isomers; enantiomers R [arketamine] and S [esketamine]. We have known for many years now that esketamine is much more potent as an anesthetic analgesic agent than arketamine. [Esketamine] is now being developed for intranasal use. You could have fewer side effects. It’s currently in phase 3 studies. The phase 2 studies have been encouraging, showing that intranasal esketamine produces rapid antidepressant [effects], and suggests antisuicidal ideation effects, as well. Dr Zarate:That’s a very good question. Because of its side effect profile and its risk for abuse, it would be very hard to bring ketamine earlier into the decision tree—that is, when somebody’s experiencing their first or second episode of depression. But imagine if you were to have a ketamine without the abuse potential or the side effects. Very early, within the first episode of depression, or even [the] second, one could probably intervene. Some people have dozens of major depressive episodes that wipe away years of optimal function and ability to work, to contribute to society, have a family. Then imagine you’re intervening very early in depression. You will eliminate all that time ill. Dr Zarate:We are carefully following the wonderful work by our colleagues around the world. Ayahuasca, for example, is being reported to have rapid antidepressant effects and to be useful for treatment-resistant depression. Psilocybin [is] out there as well. LSD [lysergic acid diethylamide] for either PTSD [posttraumatic stress disorder] or anxiety. MDMA or ecstasy is another one. In a controlled research setting, we may be able to study patients or individuals who are exposed to these [agents] using brain imaging or other techniques to see what precisely they do in the brain. So I’m in favor of trying to learn as much [as we can], but as long as it’s ethically and safely done. Dr Zarate:Some have suggested that [they] turn on or off certain circuits at the precise and important time. Some have used the lay term of rebooting the brain. And you can imagine that maybe certain circuits have been stuck and functioning in the wrong manner, but treatments such as hallucinogens, electroconvulsive therapy, scopolamine—which is another treatment [that] seems to have effects in depression—ketamine, might temporarily reboot certain aspects of the brain function at specific circuits. It seems that plasticity and connectivity might be very important aspects of how these drugs might work. Whether they do it in a similar way or dissimilar way, we don’t know yet. The product contents were subjected to both targeted and untargeted analysis. In the targeted analysis, the mass-spectrometric precursor to the fragment transitions specific to the compounds of interest were monitored at the known retention times for the compounds to provide the highest degree of sensitivity in detecting their presence. In the untargeted analysis, full mass-spectrometric scans were acquired with and without molecular fragmentation across the full chromatogram to allow detection of all ionizing species (ie, without predefining which molecular masses to identify). The mass-spectrometric data were acquired with high mass accuracy (mass error of <10 ppm) and at high resolution (35 000 Da) to enable determination of the candidate molecular formulas for the detected masses. Internet searches yielded 210 products from 51 supplier sites offering selective androgen receptor modulators and other muscle-building compounds (Figure 1). Of these, 45 products could not be purchased due to insufficient information on the websites, and 121 products could not be obtained because they were out of stock, discontinued, or had other restrictions. Forty-four products purchased from 21 suppliers were analyzed (Figure 2 and Table). The number of products purchased from various suppliers ranged from 1 to 5. Figure 2 shows the most common compounds detected in the 44 products. Selective androgen receptor modulators, peroxisome proliferator-activated receptor-δ (PPAR-δ) agonists, and growth hormone secretagogues were the 3 most frequently found classes of compounds among those tested. Selective androgen receptor modulators accounted for 23 of the 44 tested compounds (52%). Ostarine and LGD-4033 were found in more than 80% of selective androgen receptor modulator products. The PPAR-δ agonist GW501516 was the second most frequently found compound, and the growth hormone secretagogue ibutamoren was the third most common. In only 18 of the 44 products (41%), the amount of active compound in the product matched that listed on the label. In 11 products (25%), the listed compound was detected, but in an amount that differed from the label (Table and Figure 1). In 8 products (18%), the compound listed on the label was not found; however, different compounds not listed on the label were found. Three products (7%) contained the amount listed on the label but also contained an additional compound not listed. Four products contained minimal amounts of the selective estrogen receptor modulator tamoxifen. In another 4 products (9%), no active compounds were detected. The amount of the compounds listed on the label differed substantially from that found by analysis in 26 of 44 products (59%). Seventeen products (39%) contained another unapproved drug, including the growth hormone secretagogue ibutamoren, the peroxisome proliferator-activated receptor-δ agonist GW501516, and the Rev-ErbA agonist SR9009. None of the 5 products purchased from 1 supplier contained the compounds listed on the label (Table). In 2 of 5 products obtained from this supplier, the analyses detected banned substances not listed on the label. Of the 44 products, 20 (45%) were sold as dietary supplements and contained Supplement Fact panels on their labels. No product label included a Drug Fact panel. There were 24 products (55%) labeled as being for research use only, not for human consumption, or both (Table). The retail cost, product form (capsules or solution), and contents (quantity or solution volume) appear in the Table. Quiz Ref IDThis limited investigation found that products containing investigational selective androgen receptor modulators and other performance-enhancing drugs not approved by the FDA can be purchased from internet sites without a prescription. Although the tested products were advertised as selective androgen receptor modulators, some products contained growth hormone secretagogues,10 PPAR-δ agonists,11 and Rev-ErbA agonists,12 all of which are recognized by WADA as banned substances.13 Neither the efficacy nor the safety of these investigational drugs has been demonstrated. For many products, the ingredients and their amounts found in these analyses did not match the label information. These findings suggest the need for greater regulatory oversight of products sold on the internet. Most tested products were investigational drugs that have not received FDA approval. The FDA requires all drugs to have an approved application for continued marketing.14,15 Such approval from the FDA requires demonstration of safety and efficacy.15 Some compounds such as Ostarine,16 Andarine, LGD4033,6 and ibutamoren10 have undergone some human studies, but have not received FDA approval. The development of GW50151617 was halted because of safety concerns. Preclinical studies have occurred for SR9009, but no human trials. Quiz Ref IDInternet sales of drugs that are not approved by the FDA and whose benefits and safety have not been demonstrated raise public safety concerns. Individuals who use these products generally do not disclose the use of these products to their physicians; furthermore, clinicians do not have ready access to information about the contents and safety of such unapproved products, further increasing the health risk to the user. This investigation has several strengths. Rigorous testing procedures, which are similar to those used to detect banned substances in the WADA-certified testing program, were used for the analyses of these compounds. The chain of custody was recorded and products were stored in secure locations. The sensitivity of these procedures enabled detection of even small quantities of compounds. This study has several limitations. Because all possible internet sites were not searched, this search should not be viewed as exhaustive. It is possible that additional appearance- and performance-enhancing substances are being sold on the internet as selective androgen receptor modulators. The results apply only to the products purchased. Therefore, these findings should not be viewed as representative of all products sold through internet sites because the sample is limited to a single point in purchase time. Because of the rapidly changing nature of such internet sites, the results of similar searches will vary. The CGM group had a small but significant reduction in hemoglobin A1c levels at 34 weeks’ gestation compared to the control group. Pregnant CGM users also spent more time in target glycemic control range (68% vs 61%) and less time in the hyperglycemic range (27% vs 32%) than did pregnant control participants. Neonatal health outcomes were significantly improved in the CGM group, with a lower proportion of infants who were large for their gestational age (53% vs 69%), fewer neonatal intensive care admissions lasting more than 24 hours (27% vs 43%), less neonatal hypoglycemia (15% vs 28%), and shorter length of hospital stay. Continuous glucose monitoring offered no benefit to women planning pregnancy. Clinical pathological conferences may be conducted in a variety of ways. The objective of the pathologist must be to present those structural changes which are most important for the understanding of the clinical manifestations of the case under discussion. The morphology and pathogenesis of the disease state should be correlated and integrated with its clinical symptomatology and pathological physiology. When appropriate, generalizations which demonstrate the practical importance of certain findings can be made. Modern therapeutics (chemotherapy and antibiotics, hormones, anticoagulants, cytostatics, thyreostatics, radiation therapy, and radioisotopes) have changed not only the clinical symptomatology and natural history but also the pathological anatomy of disease. The clinical pathological conference is the place to discuss the “pathology of therapy,” a new branch of pathology, which includes not only new types of morphologic alterations of diseased tissue but, not infrequently, pathological changes secondary to the therapeutic agents used. With the progress of rational scientific therapy, progress in social conditions, change of environment, and increased longevity, morphologic pathology changes just as much as does clinical medicine. The discussion of these points at the clinical pathological conference will contribute to the development of a medical ecology which forms the principal component of what is referred to as “natural history.”. The question of correlation is a very delicate one; it depends to some extent on the interest and experience of the pathologist and the type of case under discussion. In the final analysis, it should not be the exclusive job of either the clinician or the pathologist, but rather should represent the integrated endeavor of both. The morphologic pathologist has to be conservative in the dynamic approach of clinical pathological correlation. The conclusions reached by him are based on the available and time-honored methods of careful observation, dissection, and examination of tissue sections with the light microscope, although by using newer methods in the future his horizon will widen. With this in mind, he will attempt to cross the delicate bridge connecting morphologic pathology with pathological physiology. It must be stressed that this bridge is an artificial one, for while at the molecular level structure and function unite, at the level of practical medicine there is a wide gap which cannot be easily or one-sidedly crossed. It is better to build the bridge from both sides, as a team, than to decide arbitrarily, either as a clinician or as a pathologist. The clinical pathological conference is not a stage for prima donnas, nor should it degenerate to a contest between clinician and pathologist. Moreover, the clinician should be in error when he states, “If I saw this patient on the ward, I would make a diagnosis of myocardial infarction, but at a clinical pathological conference I have to make the diagnosis of aneurysm of the right ventricle because otherwise it would not be a case for a conference.” The clinician should rather place himself in the position of the physicians who cared for the patient and expose to view the thought processes and clinical associations utilized in his approach to the problem of the individual patient. The pathologist then presents a rational exposition of the problem based on the morphologic findings and formulates a clinical pathological correlation. By doing so, the pathologist will prove to the younger generation that morphologic pathology is as important to medicine in the age of paper chromatography and atomic science as it was 100 years ago. He will try to deserve for the autopsy room the name given to it in an editorial by Averill Liebow entitled “The Autopsy Room as a Hall of Learning.”1. If you are a woman, harmful mutations in the BRCA1 or BRCA2 genes can greatly increase your risk of developing cancer of the breast or ovaries. Such mutations also can increase your risk of developing cancer of the fallopian tubes, peritoneum (lining of the abdominal cavity), or pancreas. If you are a man, such mutations can increase your risk of developing cancer of the breast, prostate, or pancreas. Some BRCA1/2 gene mutations, if inherited from both parents, are also associated with the development of certain forms of anemia in both women and men. A negative test result means that you most likely do not have a harmful BRCA1/2 mutation—but the implications of that information depend on factors such as your personal and family medical histories. For example, if your personal or family histories suggest that you might be at risk of having a harmful BRCA1/2 mutation but no close relatives have been identified as having such a mutation, you might still have a BRCA1/2 mutation that was not identified by the test and that has not yet been identified as harmful. Your negative test result is considered true negative only if you also have a close relative identified as having a harmful BRCA1/2 mutation. However, a true-negative result does not mean that you definitely will not develop cancer. You still have an average overall risk of developing a cancer unrelated to BRCA1/2 mutations. We performed a systematic evaluation of a randomly selected subset of peer-reviewed articles published using the NIS from January 1, 2015, through December 31, 2016, using a checklist of major methodological considerations relevant to the database. Using data from a public repository of publications from the NIS9 and supplemented with data from bibliographic repositories, we identified 1082 unique studies (eAppendix 1 in the Supplement). From these, we selected all 25 studies that were published in journals with a Journal Citation Reports impact factor (2015) of 10 or greater and a simple random sample of 100 additional studies that were published in journals with an impact factor of less than 10 (Figure 1). The sampling of studies was performed using the SURVEYSELECT procedure in SAS 9.4; all studies (n = 1057) in journals with an impact factor of less than 10 were assigned a random number, and 100 studies were selected with each study having an equal probability of being selected in the sample (sampling-probability = 100 ÷ 1057). The representativeness of the sample was assessed against the NIS universe for (a) distribution of studies across the spectrum of journal impact factors, (b) the nature of the source journal (medical or surgical), and (c) the clinical field of the journals (medicine and medical subspecialties, surgery and surgical subspecialties, pediatrics, obstetrics and gynecology, or mental and behavioral health) in which the articles were published. All selected studies were evaluated for 7 research practices in the major domains of data interpretation, research design, and data analysis. These research practices were compiled based on the publicly accessible recommendations by AHRQ for the use of the NIS.3,5-7,10-14 The design of the NIS and required research practices for use of the data are described in eAppendices 2 and 3 in the Supplement. Adherence to these research practices is essential for drawing appropriate conclusions using data from the NIS and is therefore required of all studies using these data. The 7 research practices (Table 1) are described briefly below. The 7 practices were assessed by an objective set of criteria for grading each study (eAppendix 4 in the Supplement). Five of the practices are applicable in all settings, thus they applied to all the studies. The remaining 2 practices (3 and 7) were applicable to fewer studies. Practice 3 required that studies performing hospital-volume assessments be limited to data before 2012, and practice 7 required that studies performing trend analyses spanning transitions in the NIS make required modifications to their analyses. Before study evaluation, all investigators involved in data abstraction (S.A., T.C., J.W.W., and R.K.) reviewed a standard summary of the methodological design of the NIS, compiled by all investigators, and reviewed the official data documentation reflecting the 2 different sampling designs (before 2011; 2012 and later). Each study was evaluated independently by 2 of 3 investigators (S.A., T.C, and J.W.W.), and results were collated and confirmed by a fourth abstracter (R.K.). There was good interrater reliability (κ statistic, 0.88) and disagreements were resolved with mutual agreement, discussion with the senior author (H.M.K.), or both. All study outcomes are reported as the percentage of eligible studies that did not adhere to a research practice. To demonstrate the practical implications of these errors, we present an example based on our own analyses. We used the NIS data from the years 2010 through 2013 to simulate errors in the assessment of hospitalization-level trends in the use of coronary artery bypass grafting (CABG) in the United States, emphasizing the need for using a survey-specific methodology and accounting for major changes in data structure over time (practices 6 and 7). In this example, hospitalizations with CABG procedures were identified using the clinical classification software procedure code 44. We examined temporal trends in CABG procedures during 2010-2013, using a set of modified discharge weights for the years 2010-2011 (AHRQ-recommended weights) that accounted for changes in the NIS data structure for subsequent years. We then simulated these trends using discharge weights that did not account for changes in data structure over time (incorrect weights). Differences in time trends with these 2 approaches were assessed using analysis of covariance. The percentage of studies that did not adhere to individual required practices varied considerably (Table 3). Denominators varied by each of the evaluated research practices. Of the 120 studies, 79 did not account for the complex survey design of NIS in their analyses, corresponding to 68.3% (95% CI, 59.3%-77.3%) of the studies in the universe of 1082 NIS studies, 62 (54.4%, 95% CI, 44.7%-64.0%) used nonspecific secondary diagnosis codes to infer complications, 45 (40.4%, 95% CI, 30.9%-50.0%) reported results to suggest that NIS included individual patients rather than hospitalizations (without addressing this in the interpretation of their results), 10 (7.1%, 95% CI, 2.1%-12.1%) improperly performed state-level analyses, and 3 (2.9%, 95% CI, 0.0%-6.2%) improperly performed physician volume estimates. Seventeen studies performed an assessment of diagnosis and/or procedure volumes at the hospital, corresponding to an estimated 141 (95% CI, 71-212) overall. Of these, 2 studies in the sample (8.2%, 95% CI, 0.0%-22.5%) included data from 2012 when such estimates were unreliable. In addition, although 27 studies (weighted, 218 studies [95% CI, 134-303]) had periods of major data redesign in the NIS, the analyses in 21 (79.7%, 95% CI, 62.5%-97.0%) of these did not account for the changes. Studies published in journals with an impact factor of 10 or greater frequently did not adhere to required research practices (Table 2). Of the 24 publications in journals with high impact factors, 16 (67%) did not adhere to at least 1, and 9 (38%) did not adhere to 2 or more required research practices. These rates were higher among the 96 studies sampled from publications in journals with an impact factor of less than 10, in which nearly 90% (86 of 96 studies) had at least 1 or more instances of nonadherence to required research practices (absolute difference, 23% [95% CI, 0%-45%]; P = .01); two-thirds of all studies (65 studies) had 2 or more practices that were not appropriate for data from the NIS (absolute difference, 30% [95% CI, 7%-52%]; P = .009). Moreover, compared with studies published in journals with an impact factor of 10 or greater, those published in journals with an impact factor of less than 10 had more instances of nonadherence to required research practices per study (median, 2 [interquartile range {IQR}, 1-3] vs 1 [IQR, 0-2] for journals with an impact factor of ≥10 [P = .006]) (Table 2). The nature of the nonadherences followed a similar pattern in studies with an impact factor of  less than 10 vs 10 or greater (Table 3). Studies using data from the NIS were cited a median of 4 times (IQR, 0-9) during a median follow-up period of 16 months since their publication (Table 4). More instances of nonadherence were associated with fewer citations among studies with zero or 1 nonadherence (median, 6.5 citations [IQR, 2-12]) compared with a median of 2 citations (IQR, 0-7) for studies with 2 or more instances of nonadherence to required practices (P = .01). Further, among studies published in journals with an impact factor of less than 10, although the median number of citations was higher in studies with zero to 1 instance of nonadherence (median, 4 [IQR, 0-7]) compared with studies with 2 or more citations (median, 2 [IQR, 0-6]), these differences were not statistically significant (P = .49). For studies published in journals with an impact factor of 10 or greater, there was no significant difference in the median number of citations among studies with zero to 1 instance of nonadherence compared with those with 2 or more (Table 4). In this overview of a random sample of 120 published studies drawn from 1082 unique studies published using data from the NIS during the years 2015-2016, 85% of studies did not adhere to 1 or more required research practices. Most studies did not account for the complex design of the sample in their analyses and therefore did not address the effects of sampling error, clustering, and stratification of data on the interpretation of their results. Similarly, 80% of the studies did not account for major changes in the data structure of the NIS over time and were thus likely to ascribe effects of data changes to temporal changes in the disease condition of interest. Investigations using data from the NIS also frequently misinterpreted the NIS as a patient-level data set rather than a record of hospitalization, thereby inflating prevalence estimates. Furthermore, 52% of the studies extrapolated information from the available data to infer in-hospital events using nonspecific secondary diagnosis codes. Several studies performed state-, hospital- and physician-level analyses in conditions for which such analyses would not be considered appropriate. The quality issues identified were pervasive in the literature based on the NIS, even among articles published in journals with high impact factors. In addition, despite limited follow-up, publications based on the NIS have been frequently cited, regardless of the number of required research practices not followed. Within the NIS, the limited agreement between robust official recommendations and actual practice raises questions about the inferences that have been made from many published investigations. Further, it raises questions about the reasons for investigations’ nonadherence to the research practices required by AHRQ. First, the data can be obtained by anyone with access to a computer, and there is no requirement for statistical training or analytic support for individuals using the database to conduct investigations. Although the NIS has robust documentation and tutorials, the resources may not be known to researchers. Second, even experienced investigators may incorrectly design studies or misinterpret data from the NIS. In particular, the sampling strategy ensures representativeness but requires an understanding of more advanced survey-analysis procedures to appropriately account for the stratification and clustering of data from the NIS. The NIS has a data structure that is similar to other common administrative data sets, such as Medicare, in which each observation represents a discrete health care encounter and includes a set of administrative diagnosis and procedure codes that correspond with that encounter. However, the NIS data include several additional variables that identify the sampling strata and clusters for each observation, which are necessary for its appropriate use. Further, features such as the inability to track patients longitudinally, or obtain estimates for states or physicians require that researchers invest the time to understand the nuances of data analysis using the NIS as opposed to transposing methodology from analyses of more conventional administrative data sets such as Medicare. Therefore, a careful review of the required practices is essential to ensure appropriate use of NIS data. In addition, it is critical that investigators ensure that the NIS represents the most appropriate database for their research question and not predicate their decision on its easy accessibility compared with other data sources. This study has several limitations. First, the study only includes an evaluation of studies from a recent 2-year period, and the quality of investigations using the NIS in preceding years may be different. However, given the forward-feeding nature of science and limited familiarity with the NIS that was observed in the studies examined, superior quality in earlier years would not be expected. Second, the present study performed a limited evaluation of study quality focused on 7 NIS-specific practices but did not evaluate other aspects of quality. Therefore, the study does not suggest that investigations that followed all the NIS practices that were examined are of the highest quality, given the potential for additional limitations and incorrect research practices (eg, inflating the generalizability of the study’s population or its outcomes to those outside of an inpatient clinical setting or both). Third, the present study did not independently examine the direct implications of the identified research practices in the context of their specific field. However, it would be prudent to confirm the results of studies using data from the NIS that did not adhere to research practices, particularly those that are of major importance to a research field. Fourth, this study was not designed to compare quality in different types of studies or in other publicly available databases, and an independent assessment of studies published using other data sources is needed. Fifth, while the present study followed objective criteria and performed multiple independent evaluations of the studies, there is a potential for misclassifying studies if the authors did not report the methods clearly. Sixth, the present study uses study citations as a marker of the association between a publication using the NIS database and subsequent investigations in the field; however, it does not specifically address the nature of these study citations. In Reply We agree with Drs Plante and Hoyt that using mean adherence to PAP among the relatively few patients remaining on treatment at 6-year follow-up in the trial by Peker et al overestimated mean adherence in that study.1 In response to Plante and Hoyt, we obtained adherence data from the trial investigators (personal written communication, Yüksel Peker, MD, PhD, August 23, 2017), which estimates mean adherence to be 2.8 hours per day rather than 6.6 hours per day and recalculated the meta-regression. The corresponding revised meta-regression for the outcome of major adverse cardiovascular events (MACE) is shown in the Figure and remains nonsignificant for the association between adherence to PAP and risk (P = .17). The associations between adherence to PAP and risk for the MACE plus hospitalization for unstable angina outcome (P = .56) and the cardiovascular death outcome (P = .98) also remained nonsignificant. Revising the adherence subgroup analyses by placing the trial by Peker et al in the subgroup with adherence less than 4 hours per day did not substantively change the conclusions—there was still no definitive evidence of heterogeneity of effects of PAP on the MACE outcome in the trials with 4 or more hours per day adherence (relative risk, 0.43 [95% CI, 0.22 to 0.85]) compared with the trials with less than 4 hours per day adherence (relative risk, 1.01 [95% CI, 0.82 to 1.25]) (P value for homogeneity = .06). The P values for the comparisons between subgroups for other outcomes also remained nonsignificant (all P > .26). In regard to the comments by Drs Javaheri and Campos-Rodriguez, we persist with our view that the evidence base is currently insufficient to support the use of PAP with the aim of reducing cardiovascular risk.3 We acknowledge that the available data are imperfect, but unless new studies show benefit, there is no reasonable basis for indicating to patients that treatment with PAP will prevent cardiovascular events. Observational analyses of the associations of sleep apnea with cardiovascular risks and cardiovascular outcomes are prone to confounding and an inadequate basis upon which to assure safety or efficacy of PAP for the prevention of cardiovascular events. Likewise, posthoc analyses of selected intermediate outcomes are a weak basis for defining benefit or harm for clinical outcomes. There are many examples of diseases for which observational findings have been shown to be inconsistent with trial results, even when clear benefits on intermediate risk markers have been demonstrated for interventions. In diabetes, for example, the withdrawal from the market of agents found initially to produce favorable effects on glycemia but ultimately to cause harmful effects on cardiovascular outcomes has resulted in an entirely new regulatory strategy for the approval of new therapies for diabetes.3 So, although the hypothesis laid out linking observational findings to intermediate outcomes and clinical events is interesting, it remains a hypothesis until sufficient evidence from adequately powered, well-conducted trials is generated to support it or refute it. Those data are currently unavailable and patient expectations should be set accordingly. In Reply Mr Rudman and colleagues question whether mandatory review of PDMP data reduces opioid overdose death rates. They observe that the 4 states with mandated PDMP policies examined in one study1 each experienced increases in heroin-related and overall opioid-related overdose death rates from 2011 to 2015. In fact, nearly all states experienced increases in these rates since 1999, including from 2011 to 2015. Therefore, an evaluation of any state-level policy during this period would seem to imply that the policy was associated with increased overdose deaths. To differentiate background trends and other variables across states, Dowell et al1 used a difference-in-difference regression analysis and compared changes in overdose death rates associated with policies vs changes in rates absent policy intervention. The analysis found that specific policies (eg, mandated PDMP use and pain clinic laws) were associated with reduced prescription opioid–related overdose deaths and overall opioid–related overdose deaths (including those from prescription opioids and heroin). Using this method, these policies were associated with reduced heroin deaths, although this association was not statistically significant.1. These findings suggest that existence of a PDMP may be insufficient and emphasize the need to ensure effective use. Clinician consultation of PDMPs is infrequent when they are available but not mandated, whereas use increases significantly with mandates.1 Timelier data are more clinically useful and might provide greater incentive for clinicians to review them. Clinician burden can be reduced by creating a single sign-on for clinician access to electronic health records (EHRs) and PDMPs, integrating PDMP data into existing workflows, and/or allowing clinician delegates to access PDMP data. Integration of PDMP information into EHRs has been associated with substantial increases in PDMP use.6. Opioid overdose deaths are increasingly driven by nonprescribed opioids, particularly illicitly manufactured fentanyl. Although findings to date are reassuring that strong PDMP policies do not appear to be associated with increased heroin overdose deaths,1,2 it is unclear whether they can reduce overdose deaths related to illicit opioids. Unintended consequences of PDMP use, such as dismissal from care, may occur in individual cases. However, consistent use of PDMP information by clinicians can improve patient safety by identifying dangerous medication combinations, high cumulative dosages, or controlled substances obtained from multiple clinicians. Best practices include discussing concerns with patients nonjudgmentally, coordinating care with other clinicians, assessing for opioid use disorder, and offering or arranging medication-assisted treatment if indicated. In Reply Mr Lifton, Dr Faulkner, and Dr Nora correctly identified several errors in our study. Most importantly, the data were from tax year 2014 rather than FY 2013, and the 10-year baseline began in tax year 2004, not FY 2003. Lifton and Nora are correct that 1 board (American Board of Nuclear Medicine) was inadvertently left out. In Table 2, there were minor discrepancies in 5 data elements, which led to differences in the totals; however, these discrepancies represent a mean change of less than 0.5% in the cumulative data elements and the conclusions of the study are unchanged. In addition, the data were not a sample and so confidence intervals should not have been reported. These errors have been corrected online and a correction notice accompanies this letter. We apologize to readers. The subject of deferred revenues raised by the letters is puzzling. Only 13 of the 24 ABMS member boards use this accounting method, in which fees collected and saved to pay for future examination administration are reported as liabilities. For many boards, this deferred revenue “liability” has increased steadily for years, making their net worth appear smaller or even negative (eg, American Board of Internal Medicine). Because only some member boards (generally those with the largest total assets) use this unusual accounting method, deferred revenue was excluded from both the assets and the liabilities to standardize our reporting. This was clearly denoted in our Research Letter, but it is unclear why some of these nonprofit organizations utilize deferred revenue accounting and why these assets reported as liabilities continue to increase. As researchers, we were limited to publicly available data (ie, tax records) to understand board finances. Although Lifton claims that excluding minor line item revenue and expenditures (<10% of totals) affects the overall results, he provides no evidence that his supposition is correct. Similarly, Nora notes the IRS Form 990 provides only some financial information but provides no additional insight about the finances of ABMS member boards. Instead, she explains the disparity between revenue and expenditures through “detailed operational costs.” Having more complete financial reports available could enable physicians and the public to better understand the true costs of certification and make informed value judgments. Instead of providing any data or evidence to support their arguments, the letters from Lifton and Faulkner comment on other issues, such as investment income. For example, Faulkner claims that the member boards saw $229 million in asset growth “likely due to return on investment and not on physician fees.” He assumes an annual growth rate of 7% on investments for the entire net balance of all member boards. This scenario, which would involve investing the entire net worth of all member boards in the open market, is implausible. Furthermore, analysis of the tax records shows that physician examination fees (Form 990; Part VIII, lines 2a-g) vastly exceeded return on investment for every member board (Form 990; Part VII, lines 3-4). Hence, our claim that physician fees primarily drive revenue is based firmly on data. Nora and Faulkner suggest that the costs are appropriate, but justification of high costs and physician-derived revenue was not provided. They also claim that nonprofit organizations have no fiduciary responsibility to match revenue and expenditures. Yet the National Council of Nonprofits suggests otherwise.1 Fundamentally, the ABMS and member boards are nonprofit organizations funded by physicians. They are responsible for maintaining reasonable expenditures to meet their mission, minimizing fees to physicians and completely disclosing their finances. Toward these goals, we hope our study can better align the medical boards with their physician constituents. In the Original Investigation entitled “Association of Spinal Manipulative Therapy With Clinical Benefit and Harm for Acute Low Back Pain: Systematic Review and Meta-analysis”1 published in the April 11, 2017, issue of JAMA, there were data errors in the text. The text says the N for Figure 2 is 1335, but it should be 1421 (as stated in Figure 2), and the text says the N for Figure 3 is 975, but it should be 1049 (as stated in Figure 3). Also, the N for the 15 randomized clinical trials should have been 1699, not 1711. This article was corrected online. This article was also corrected for errors in Figures 2 and 3 on June 6, 2017.2. In the Research Letter entitled “Fees for Certification and Finances of Medical Specialty Boards”1 published in the August 1, 2017, issue of JAMA, some incorrect data were reported in the text and Tables. The dates of the study were incorrect (fiscal year [FY] 2013 should have been tax year [TY] 2014, and the 10-year baseline FY 2003 should have been TY 2004); 1 board (American Board of Nuclear Medicine) was left out of both Tables; a Deferred Rate column was added to Table 2; there were minor discrepancies in 5 data elements and the totals in Table 2; and CIs should not have been reported. However, the conclusions did not change. This article was corrected online. The University of Pennsylvania institutional review board waived review of this study. We used the Provider Utilization Files, containing all Part B Medicare fee-for-service billings, to identify generalist physicians (internal medicine, geriatrics, general practice, or family medicine) and advanced practitioners (nurse practitioners and physician assistants) who provided nursing home–based care (Health Common Procedure Coding System codes, 99304-99310, 99315-99316, or 99318) from 2012 through 2015. We aggregated episodes of care by clinician to measure the proportion of episodes that were nursing home–based. We then defined clinicians who billed at least 90% of episodes from the nursing home as nursing home specialists (a definition analogous to the one used to identify hospitalists4). We also subcategorized nursing home–based episodes into post–acute care vs long-term care episodes using place of service codes. Linear regression was used to model the number of clinicians (or clinicians per 1000 occupied nursing home beds) in each HRR as a function of year. Two-sided P values less than .05 were considered significant. Analyses were conducted using Stata (StataCorp), version 13.1. Among 319 264 unique generalist clinicians, 50 227 (15.7%) billed for nursing home care from 2012 through 2015. The number of clinicians billing for nursing home care remained relatively stable (−0.4%; from 33 218 to 33 087; P for trend, .97) (Table), whereas the total number of generalists increased (15.6%; from 224 358 to 259 373; P for trend, <.001). The number of nursing home specialists increased from 5127 to 6857 (P for trend, <.001), a relative increase of 33.7%. The mean number of nursing home specialists per 1000 beds increased from 3.35 (95% CI, 3.06 to 3.64) in 2012 to 4.58 (95% CI, 4.19 to 4.96) in 2015 (P for trend, <.001). Clinicians who specialized in post–acute care (2.00 clinicians/1000 beds [95% CI, 1.75 to 2.26]) and advanced practitioners (3.21 clinicians/1000 beds [95% CI, 2.90 to 3.51]) were the most prevalent types of nursing home specialists. It is standard practice for the WMA to circulate its policy papers for review every 10 years to reevaluate the accuracy, essentiality, and relevance of the documents. The Declaration of Geneva is no exception. In 2016 (10 years following the most recent editorial revision of the Declaration), the WMA established an international workgroup to assess the Declaration of Geneva’s content, structure, audience, and implementation and to determine whether any amendments were necessary. Given the crucial nature of this document, the assigned workgroup charted a generous timeline of nearly 2 years to allow ample opportunity to gather feedback and suggestions not only from member national medical associations, but also from external experts. The goal in doing so was to ensure that the revision was as transparent and collaborative an effort as possible. The central feature of insomnia disorder is dissatisfaction with sleep quantity or quality, associated with difficulty falling asleep, maintaining sleep, or early morning awakening.1 Insomnia disorder causes clinically significant distress or impairment in important areas of functioning. Sleep difficulties occur at least 3 nights per week for at least 3 months, and are not better explained by use of substances, medications, or by another disorder. Insomnia is diagnosed only when an individual has adequate opportunity for sleep; this distinguishes insomnia from sleep deprivation, which has different causes and consequences. Insomnia disorder is often comorbid with other sleep-wake, mental, or medical disorders that require separate management. Increased neural, physiological, and psychological arousal, together with perpetuating behavioral factors (such as excessive time in bed) are thought to underlie most cases of chronic insomnia. Acute insomnia, which meets all diagnostic criteria as chronic insomnia except in duration, may have different causes and specific treatment implications. Individuals with insomnia disorder typically experience multiple sleep symptoms over time. Nevertheless, specific sleep symptoms may aid differential diagnosis. Difficulty falling asleep may signal delayed sleep phase syndrome, restless legs syndrome, or anxiety. Difficulty maintaining sleep can result from sleep apnea, nocturia, or pain. Early morning awakening is associated with advanced sleep phase syndrome and depression. The diagnosis of insomnia relies on patient history from both the patient and bed partner. Self-report questionnaires and sleep diaries are often useful to assess insomnia severity, identify behaviors contributing to persistent insomnia, and monitor treatment effects. The Insomnia Severity Index2 and Consensus Sleep Diary3 are examples of clinically practical, sensitive outcome measures. Both questionnaires can be completed quickly, within 2 to 3 minutes, at home or in the clinician’s office and provide useful self-report and behavioral information facilitating clinical management. CBT-I is a multimodal treatment that combines several behavioral and cognitive interventions. Specific components include education (eg, healthy sleep practices and expectations), stimulus control instructions, time-in-bed restriction, and relaxation training. CBT-I produces reliable, durable benefits in 70% to 80% of patients and may reduce the use of sedatives.5 CBT-I can be delivered by trained therapists and by self-guided, fully automated online programs (eg, SHUTi, Sleepio, and others). Overall CBT-I shows moderate to large effect sizes on outcomes of interest, including time to fall asleep, continuity, restfulness, and duration of sleep.5. US Food and Drug Administration (FDA)–approved prescription medications for insomnia include benzodiazepines and benzodiazepine receptor agonists (BzRAs), the melatonin receptor agonist ramelteon, the tricyclic drug doxepin, and the orexin receptor antagonist suvorexant. Even though the margin of safety for benzodiazepines and BzRAs is relatively wide, adverse effects may include anterograde amnesia, complex sleep-related behaviors, falls, cognitive impairment, respiratory depression, and rebound insomnia. Agents with short elimination half-lives are preferred to longer-acting drugs to avoid daytime sedation. Intermittent dosing 3 to 4 times per week may reduce exposure and long-term use. Doxepin (3-6 mg) is appropriate for sleep maintenance insomnia and may be particularly helpful in patients with contraindications to benzodiazepine and BzRA drugs, such as substance use disorders. Suvorexant improves sleep maintenance insomnia symptoms with little evidence of tolerance and has a distinct mechanism of action. Ramelteon is most appropriate for sleep onset insomnia symptoms. Although antihistamines (eg, diphenhydramine, doxylamine) are FDA-approved for insomnia, evidence regarding their efficacy and safety is not robust. A variety of other drugs commonly used to treat insomnia have not been rigorously evaluated for efficacy and safety and are not FDA-approved for this indication. These include low doses of sedating antidepressant drugs (eg, trazodone, mirtazapine). Sedating antipsychotic drugs (eg, olanzapine, quetiapine) are recommended only for patients with appropriate psychiatric diagnoses because of their potential metabolic, neurologic, and cardiovascular effects. Complementary and alternative agents, including melatonin and valerian, also lack sufficiently rigorous efficacy and safety data to recommend their use. Insomnia disorder is frequently presented in general medical practice. Evaluation involves a careful history with the patient and bed partner, if available, and use of brief instruments to gauge severity and behaviors that destroy sleep. Use of cognitive behavioral therapy for insomnia is recommended as the first-line treatment. Pharmacotherapy of insomnia disorder, if used, should be on a short-term basis, and in shared decision making with the patient. The prevalence of obesity in the United States has been increasing for almost 50 years. Currently, more than two-thirds of adults and almost one-third of children and adolescents are overweight or obese. Youths who are obese are more likely to be obese as adults, compounding their risk for health consequences such as cardiovascular disease, diabetes, and cancer. Trends in many of the health consequences of overweight and obesity (such as type 2 diabetes and coronary heart disease) also are increasing, coinciding with prior trends in rates of obesity. Furthermore, the sequelae of these diseases are related to the severity of obesity in a dose-response fashion.2 It is therefore not surprising that obesity accounts for a significant portion of health care costs. A report released on October 3, 2017, by the US Centers for Disease Control and Prevention assessed the incidence of the 13 cancers associated with overweight and obesity in 2014 and the trends in these cancers over the 10-year period from 2005 to 2014.3 In 2014, more than 630 000 people were diagnosed as having a cancer associated with overweight and obesity, comprising more than 55% of all cancers diagnosed among women and 24% of cancers among men. Most notable was the finding that cancers related to overweight and obesity were increasingly diagnosed among younger people. From 2005 to 2014, there was a 1.4% annual increase in cancers related to overweight and obesity among individuals aged 20 to 49 years and a 0.4% increase in these cancers among individuals aged 50 to 64 years. For example, if cancer rates had stayed the same in 2014 as they were in 2005, there would have been 43 000 fewer cases of colorectal cancer but 33 000 more cases of other cancers related to overweight and obesity. Nearly half of all cancers in people younger than 65 years were associated with overweight and obesity. Overweight and obesity among younger people may exact a toll on individuals’ health earlier in their lifetimes.2 Given the time lag between exposure to cancer risk factors and cancer diagnosis, the high prevalence of overweight and obesity among adults, children, and adolescents may forecast additional increases in the incidence of cancers related to overweight and obesity. Since the release of the landmark 1964 surgeon general’s report on the health consequences of smoking, clinicians have counseled their patients to avoid tobacco and on methods to quit and provided referrals to effective programs to reduce their risk of chronic diseases including cancer. These efforts, coupled with comprehensive public health and policy approaches to reduce tobacco use, have been effective—cigarette smoking is at an all-time low. Similar efforts are warranted to prevent excessive weight gain and treat children, adolescents, and adults who are overweight or obese. Clinician referral to intense, multicomponent behavioral intervention programs to help patients with obesity lose weight can be an important starting point in improving a patient’s health and preventing diseases associated with obesity. The benefits of maintaining a healthy weight throughout life include improvements in a wide variety of health outcomes, including cancer. There is emerging but very preliminary data that some of these cancer benefits may be achieved following weight loss among people with overweight or obesity.4. The US Preventive Services Task Force (USPSTF) recommends screening for obesity and intensive behavioral interventions delivered over 12 to 16 visits for adults and 26 or more visits for children and adolescents with obesity.5,6 Measuring patients’ weight, height, and body mass index (BMI), consistent with USPSTF recommendations, and counseling patients about maintaining a healthy weight can establish a foundation for preventive care in clinical care settings. Scientific data continue to emerge about the negative health effects of weight gain, including an increased risk of cancer.1 Tracking patients’ weight over time can identify those who could benefit from counseling and referral early and help them avoid additional weight gain. Yet less than half of primary care physicians regularly assess the BMI of their adult, child, and adolescent patients. Encouraging discussions about weight management in multiple health care settings, including physicians’ offices, clinics, emergency departments, and hospitals, can provide multiple opportunities for patients and reinforce messages across contexts and care environments. Implementation of clinical interventions, including screening, counseling, and referral, has major challenges. Since 2011, Medicare has covered behavioral counseling sessions for weight loss in primary care settings. However, the benefit has not been widely utilized.7 Whether the lack of utilization is a consequence of lack of clinician or patient knowledge or for other reasons remains uncertain. Few medical schools and residency programs provide adequate training in prevention and management of obesity or in understanding how to make referrals to such services. Obesity is a highly stigmatized condition; many clinicians find it difficult to initiate a conversation about obesity with patients, and some may inadvertently use alienating language when they do. Studies indicate that patients with obesity prefer the use of terms such as unhealthy weight or increased BMI rather than overweight or obesity and improved nutrition and physical activity rather than diet and exercise.8 However, it is unknown if switching to these terms will lead to more effective behavioral counseling. Effective clinical decision support tools to measure BMI and guide physicians through referral and counseling interventions can provide clinicians needed support within the patient-clinician encounter. Inclusion of recently developed competencies for prevention and management of obesity into the curricula of health care professionals may improve their ability to deliver effective care. Because few primary care clinicians are trained in behavior change strategies like cognitive behavioral therapy or motivational interviewing, other trained health care professionals, such as nurses, pharmacists, psychologists, and dietitians could assist by providing counseling and appropriate referrals and help people manage their own health. Achieving sustainable weight loss requires comprehensive strategies that support patients’ efforts to make significant lifestyle changes. The availability of clinical and community programs and services to which to refer patients is critically important. Although such programs are available in some communities, there are gaps in availability. Furthermore, even when these programs are available, enhancing linkages between clinical and community care could improve patients’ access. Linking community obesity prevention, weight management, and physical activity programs with clinical services can connect people to valuable prevention and intervention resources in the communities where they live, work, and play. Such linkages can give individuals the encouragement they need for the lifestyle changes that maintain or improve their health. Tumors are my passion. I love dealing with all aspects of tumors, from the basic science research to the clinical treatments, complex surgeries, and even the challenge of discussing poor prognoses with families. But I recall one day, as a fourth-year medical student on the orthopedic oncology service, when I lacked the usual enthusiasm. Instead, my stomach hurt. I tried hard to focus on the tasks at hand. The more I could focus on the tumors before me, I thought, the less time I had to worry about the tumors that might be elsewhere, like back home in South Carolina, like in my dad. My father (“Pops”) had called me the night before because he was jaundiced. “I don’t have any pain,” he had said. A cold sweat had dripped down my neck. Even as a novice clinician, I knew what was at the top of the differential for painless jaundice: pancreatic cancer. For the rest of the night I alternated between pacing and sitting nervously. “Not my family,” I kept thinking. “Not my dad.” He was only 61 and very healthy. The next day, I continued my tumor rotation. We removed another mass and wheeled the patient to the postanesthesia care unit. As I left the operating room, my sister called in tears and broke the news—Pops had stage IV pancreatic cancer. I remember crumbling to the floor crying, sobbing. Staff members had to pick me up so patients could get by. That night I went home to South Carolina, reaching his hospital room around 3 am. Over the next few days, my siblings and I couldn’t leave our father’s side, seemingly afraid that if we withdrew our touches of love and comfort, he might die right then. Our extended family arrived at the hospital and squeezed into his room. I have heard doctors complain about large groups like this before, with comments such as, “They think it’s a hotel in there.” I probably have made similar comments myself because this can make caring for a patient more challenging. But we did it because Pops needed it. We called friends and family. We told jokes. We brought family photo albums. It was a time of love. Many different medical teams and doctors came in to visit. As a physician and family member, the variety of bedside manners I observed made me realize that I did not learn my bedside manner in medical school. I had learned it from Pops. In contrast, when I was a child, Pops would come to each of our bedsides at bedtime, kneel, and ask us about our day. If he had work to return to or other obligations, I never knew it. No matter what was on his mind, Pops was always present. I do not remember what we talked about, but I remember it was my special time to ask or share whatever I wanted. When my father was at my bedside, I knew I had his undivided attention. Do my patients feel the same way when I am at their bedside?. Although my dad was an electrical engineer by training, he epitomized the traits we seek in our physicians but rarely find. No matter where your life intersected with his, you met the same wonderful man: humble, loving, hardworking, great listener, and slow to anger, and he always had a hug. Perhaps he worked hard at this or perhaps it just came naturally. Either way, whether Pops was kneeling at my bedside at night or meeting someone new during the day, his attention was always focused on the person before him. I witnessed this type of behavior from Pops my whole life. Do I treat patients the same way?. In the middle of this confusion, an attending surgeon entered the room, sat down, and listened to my father. He looked my father in the eye. Then he asked questions much like my father would on meeting someone new—What do you do for work? What do you do for fun? He noted a religious book my father had on the table. They talked about it. He also reviewed the diagnosis and the pertinent imaging, talking about treatment options that would come up. The surgeon was kind and blunt. Life was going to be different now, he explained, but there was nothing that could be done to change that. Instead he encouraged my father to focus on making reasonable decisions in regards to treatment, creating a bucket list, and being with family. From that moment on, my father’s outlook changed. He started thinking about the risk and reward of treatments. He started making a bucket list. His mood improved. The attending?—a general surgeon who knew my father’s cancer was inoperable. Yet his attention was on my father the person, not my father the “nonoperative patient.” Do I deliver the same level of care toward my nonoperative patients?. About a year after his diagnosis, my father began to decline rapidly. I found myself at his bedside, our roles now reversed. I no longer was just a son but instead became my father’s doctor and nurse, sleeping on the floor next to his bed, giving him morphine and lorazepam around the clock. I wondered if Pops knew I learned that attentiveness from him, not from medical school. My father fought bravely for 14 months after his diagnosis, even speaking at a pancreatic cancer vigil only a month before his death, giving of himself even in his final days. Do I give of myself to others like he did?. Toward the end, Pops was bedridden, with eyes closed. He did not move much, although he would to indicate that he could hear us. Pops had great difficulty moving his bowels, but on this day there happened to be a moment of very audible flatus. In an effort to provide some humor, I started clapping. My dad, eyes closed, mustered the strength to stretch his arm high into the air and say with a smirk, “Thank you! Thank you!” We all burst out laughing. Even in his final moments Pops could provide humor when we needed it most. Can I provide humor when my patients need it, even on my worst days?. My father was a man of science and faith. He was a calculating, precise, brilliant engineer yet also a man who pondered things beyond this life. He taught me that being a scientist did not mean giving up all philosophical pursuits and questions. On the night of his death, I sat at his bedside and read aloud his favorite texts to him, the book of Psalms, 121:7-8:. As I read the last sentence he drew in and out one final breath and then was still. Was there a divine intervention at that second to relieve his suffering, or was he simply comforted by his favorite verse? I do not know. What I do know is that my dad lived a life of faith, and in his final moments it was not the opiates or benzodiazepines that ultimately brought him comfort; rather, it was the scriptures that he had lived his life by. As a physician-scientist and as his son, this lesson was not lost on me. How many of my patients are men or women of faith? Do I recognize the role their faith plays in both their life and death?. Today, four years later, I think of my father each day and try to follow his example when talking to patients. Over the years countless friends of mine commented on his remarkable personality and his attention to them during conversation. “Salt of the earth,” one friend said of Pops after meet